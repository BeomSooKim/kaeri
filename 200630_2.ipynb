{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- normalized input을 channel-wise concat\n",
    "- 학습 속도 및 성능을 봤을 때 3x1, 4x1, 5x1이 가장 적당한듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)\n",
    "        #return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (3, 1))\n",
    "        fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        model = cnn_model(conv, fc)\n",
    "        #model = get_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.384248, val loss drop 10000000.0000 to 1.6434\n",
      "[2] : train loss 0.725203, val loss drop 1.6434 to 0.3786\n",
      "[3] : train loss 0.176617, val loss drop 0.3786 to 0.1619\n",
      "[4] : train loss 0.059922, val loss drop 0.1619 to 0.0715\n",
      "[5] : train loss 0.028553, val loss drop 0.0715 to 0.0426\n",
      "[6] : train loss 0.017678, val loss drop 0.0426 to 0.0237\n",
      "[7] : train loss 0.010899, val loss drop 0.0237 to 0.0081\n",
      "[9] : train loss 0.004155, val loss drop 0.0081 to 0.0063\n",
      "[10] : train loss 0.003109, val loss drop 0.0063 to 0.0047\n",
      "[12] : train loss 0.001906, val loss drop 0.0047 to 0.0029\n",
      "[13] : train loss 0.001685, val loss drop 0.0029 to 0.0024\n",
      "[15] : train loss 0.001304, val loss drop 0.0024 to 0.0018\n",
      "[20] : train loss 0.000722, val loss drop 0.0018 to 0.0015\n",
      "[27] : train loss 0.000578, val loss drop 0.0015 to 0.0011\n",
      "[41] : train loss 0.000644, val loss drop 0.0011 to 0.0010\n",
      "[47] : train loss 0.000700, val loss drop 0.0010 to 0.0009\n",
      "[51] : train loss 0.000592, val loss drop 0.0009 to 0.0005\n",
      "fold 2\n",
      "[1] : train loss 2.696067, val loss drop 10000000.0000 to 1.5327\n",
      "[2] : train loss 0.806341, val loss drop 1.5327 to 0.3143\n",
      "[3] : train loss 0.170892, val loss drop 0.3143 to 0.2552\n",
      "[4] : train loss 0.054452, val loss drop 0.2552 to 0.0489\n",
      "[5] : train loss 0.023946, val loss drop 0.0489 to 0.0191\n",
      "[6] : train loss 0.012202, val loss drop 0.0191 to 0.0118\n",
      "[7] : train loss 0.008525, val loss drop 0.0118 to 0.0065\n",
      "[8] : train loss 0.005261, val loss drop 0.0065 to 0.0050\n",
      "[10] : train loss 0.003346, val loss drop 0.0050 to 0.0033\n",
      "[13] : train loss 0.001908, val loss drop 0.0033 to 0.0022\n",
      "[15] : train loss 0.001743, val loss drop 0.0022 to 0.0016\n",
      "[19] : train loss 0.001185, val loss drop 0.0016 to 0.0010\n",
      "[33] : train loss 0.001224, val loss drop 0.0010 to 0.0010\n",
      "[36] : train loss 0.000600, val loss drop 0.0010 to 0.0007\n",
      "[38] : train loss 0.000739, val loss drop 0.0007 to 0.0007\n",
      "[51] : train loss 0.000710, val loss drop 0.0007 to 0.0005\n",
      "[72] : train loss 0.000463, val loss drop 0.0005 to 0.0004\n",
      "[76] : train loss 0.000997, val loss drop 0.0004 to 0.0004\n",
      "fold 3\n",
      "[1] : train loss 2.659479, val loss drop 10000000.0000 to 1.7789\n",
      "[2] : train loss 1.118418, val loss drop 1.7789 to 0.5036\n",
      "[3] : train loss 0.319895, val loss drop 0.5036 to 0.1487\n",
      "[4] : train loss 0.083490, val loss drop 0.1487 to 0.0616\n",
      "[5] : train loss 0.029205, val loss drop 0.0616 to 0.0341\n",
      "[7] : train loss 0.007979, val loss drop 0.0341 to 0.0138\n",
      "[8] : train loss 0.005271, val loss drop 0.0138 to 0.0061\n",
      "[9] : train loss 0.003719, val loss drop 0.0061 to 0.0040\n",
      "[14] : train loss 0.001714, val loss drop 0.0040 to 0.0039\n",
      "[15] : train loss 0.001651, val loss drop 0.0039 to 0.0020\n",
      "[17] : train loss 0.001357, val loss drop 0.0020 to 0.0019\n",
      "[18] : train loss 0.001096, val loss drop 0.0019 to 0.0018\n",
      "[21] : train loss 0.001152, val loss drop 0.0018 to 0.0011\n",
      "[30] : train loss 0.001004, val loss drop 0.0011 to 0.0009\n",
      "[35] : train loss 0.000899, val loss drop 0.0009 to 0.0008\n",
      "[56] : train loss 0.000780, val loss drop 0.0008 to 0.0007\n",
      "[62] : train loss 0.000391, val loss drop 0.0007 to 0.0006\n",
      "[63] : train loss 0.000252, val loss drop 0.0006 to 0.0005\n",
      "[80] : train loss 0.000281, val loss drop 0.0005 to 0.0004\n",
      "fold 4\n",
      "[1] : train loss 2.664503, val loss drop 10000000.0000 to 1.2551\n",
      "[2] : train loss 0.874673, val loss drop 1.2551 to 0.3904\n",
      "[3] : train loss 0.202836, val loss drop 0.3904 to 0.1261\n",
      "[4] : train loss 0.059849, val loss drop 0.1261 to 0.0405\n",
      "[6] : train loss 0.011638, val loss drop 0.0405 to 0.0307\n",
      "[7] : train loss 0.007346, val loss drop 0.0307 to 0.0058\n",
      "[10] : train loss 0.003453, val loss drop 0.0058 to 0.0052\n",
      "[11] : train loss 0.004201, val loss drop 0.0052 to 0.0035\n",
      "[17] : train loss 0.001397, val loss drop 0.0035 to 0.0017\n",
      "[21] : train loss 0.000921, val loss drop 0.0017 to 0.0012\n",
      "[40] : train loss 0.000955, val loss drop 0.0012 to 0.0007\n",
      "[52] : train loss 0.000697, val loss drop 0.0007 to 0.0004\n",
      "[75] : train loss 0.000317, val loss drop 0.0004 to 0.0003\n",
      "[86] : train loss 0.000287, val loss drop 0.0003 to 0.0003\n",
      "fold 5\n",
      "[1] : train loss 2.564623, val loss drop 10000000.0000 to 1.2020\n",
      "[2] : train loss 0.689150, val loss drop 1.2020 to 0.2404\n",
      "[3] : train loss 0.130814, val loss drop 0.2404 to 0.2252\n",
      "[4] : train loss 0.041837, val loss drop 0.2252 to 0.0207\n",
      "[6] : train loss 0.009187, val loss drop 0.0207 to 0.0087\n",
      "[7] : train loss 0.005690, val loss drop 0.0087 to 0.0053\n",
      "[11] : train loss 0.002818, val loss drop 0.0053 to 0.0041\n",
      "[13] : train loss 0.002304, val loss drop 0.0041 to 0.0041\n",
      "[15] : train loss 0.002216, val loss drop 0.0041 to 0.0024\n",
      "[16] : train loss 0.001277, val loss drop 0.0024 to 0.0021\n",
      "[18] : train loss 0.001658, val loss drop 0.0021 to 0.0010\n",
      "[28] : train loss 0.001293, val loss drop 0.0010 to 0.0009\n",
      "[36] : train loss 0.000537, val loss drop 0.0009 to 0.0008\n",
      "[40] : train loss 0.000613, val loss drop 0.0008 to 0.0006\n",
      "[50] : train loss 0.000734, val loss drop 0.0006 to 0.0005\n",
      "[75] : train loss 0.000543, val loss drop 0.0005 to 0.0004\n",
      "[90] : train loss 0.000421, val loss drop 0.0004 to 0.0004\n",
      "fold 6\n",
      "[1] : train loss 2.782387, val loss drop 10000000.0000 to 1.9205\n",
      "[2] : train loss 0.981220, val loss drop 1.9205 to 0.3019\n",
      "[3] : train loss 0.179639, val loss drop 0.3019 to 0.1225\n",
      "[4] : train loss 0.062579, val loss drop 0.1225 to 0.0581\n",
      "[5] : train loss 0.025091, val loss drop 0.0581 to 0.0215\n",
      "[6] : train loss 0.013122, val loss drop 0.0215 to 0.0139\n",
      "[8] : train loss 0.004473, val loss drop 0.0139 to 0.0105\n",
      "[9] : train loss 0.003150, val loss drop 0.0105 to 0.0043\n",
      "[11] : train loss 0.001949, val loss drop 0.0043 to 0.0041\n",
      "[12] : train loss 0.002315, val loss drop 0.0041 to 0.0028\n",
      "[17] : train loss 0.001662, val loss drop 0.0028 to 0.0018\n",
      "[19] : train loss 0.001229, val loss drop 0.0018 to 0.0012\n",
      "[25] : train loss 0.000608, val loss drop 0.0012 to 0.0010\n",
      "[31] : train loss 0.000598, val loss drop 0.0010 to 0.0008\n",
      "[45] : train loss 0.000568, val loss drop 0.0008 to 0.0006\n",
      "[80] : train loss 0.000277, val loss drop 0.0006 to 0.0005\n",
      "fold 7\n",
      "[1] : train loss 2.630356, val loss drop 10000000.0000 to 1.4812\n",
      "[2] : train loss 0.793458, val loss drop 1.4812 to 0.3095\n",
      "[3] : train loss 0.185511, val loss drop 0.3095 to 0.2075\n",
      "[4] : train loss 0.054794, val loss drop 0.2075 to 0.0616\n",
      "[5] : train loss 0.024703, val loss drop 0.0616 to 0.0421\n",
      "[6] : train loss 0.012378, val loss drop 0.0421 to 0.0111\n",
      "[9] : train loss 0.004131, val loss drop 0.0111 to 0.0059\n",
      "[12] : train loss 0.002415, val loss drop 0.0059 to 0.0026\n",
      "[15] : train loss 0.001674, val loss drop 0.0026 to 0.0021\n",
      "[17] : train loss 0.001048, val loss drop 0.0021 to 0.0017\n",
      "[24] : train loss 0.001578, val loss drop 0.0017 to 0.0010\n",
      "[31] : train loss 0.001075, val loss drop 0.0010 to 0.0009\n",
      "[46] : train loss 0.000504, val loss drop 0.0009 to 0.0008\n",
      "[57] : train loss 0.001080, val loss drop 0.0008 to 0.0007\n",
      "[58] : train loss 0.000749, val loss drop 0.0007 to 0.0006\n",
      "[60] : train loss 0.000493, val loss drop 0.0006 to 0.0003\n",
      "fold 8\n",
      "[1] : train loss 2.813620, val loss drop 10000000.0000 to 1.9675\n",
      "[2] : train loss 1.249591, val loss drop 1.9675 to 0.7484\n",
      "[3] : train loss 0.394343, val loss drop 0.7484 to 0.3208\n",
      "[4] : train loss 0.090998, val loss drop 0.3208 to 0.0746\n",
      "[5] : train loss 0.031147, val loss drop 0.0746 to 0.0690\n",
      "[6] : train loss 0.013038, val loss drop 0.0690 to 0.0133\n",
      "[8] : train loss 0.004856, val loss drop 0.0133 to 0.0051\n",
      "[9] : train loss 0.003473, val loss drop 0.0051 to 0.0051\n",
      "[10] : train loss 0.003014, val loss drop 0.0051 to 0.0025\n",
      "[12] : train loss 0.002002, val loss drop 0.0025 to 0.0019\n",
      "[18] : train loss 0.000968, val loss drop 0.0019 to 0.0014\n",
      "[24] : train loss 0.000728, val loss drop 0.0014 to 0.0011\n",
      "[25] : train loss 0.000515, val loss drop 0.0011 to 0.0007\n",
      "[53] : train loss 0.000565, val loss drop 0.0007 to 0.0006\n",
      "[54] : train loss 0.000461, val loss drop 0.0006 to 0.0004\n",
      "[78] : train loss 0.000440, val loss drop 0.0004 to 0.0004\n",
      "[84] : train loss 0.000387, val loss drop 0.0004 to 0.0003\n",
      "fold 9\n",
      "[1] : train loss 2.553688, val loss drop 10000000.0000 to 1.2374\n",
      "[2] : train loss 0.594171, val loss drop 1.2374 to 0.2359\n",
      "[3] : train loss 0.120502, val loss drop 0.2359 to 0.0728\n",
      "[4] : train loss 0.039414, val loss drop 0.0728 to 0.0378\n",
      "[5] : train loss 0.021091, val loss drop 0.0378 to 0.0117\n",
      "[7] : train loss 0.005401, val loss drop 0.0117 to 0.0058\n",
      "[8] : train loss 0.004266, val loss drop 0.0058 to 0.0050\n",
      "[9] : train loss 0.002763, val loss drop 0.0050 to 0.0024\n",
      "[10] : train loss 0.002236, val loss drop 0.0024 to 0.0021\n",
      "[12] : train loss 0.001445, val loss drop 0.0021 to 0.0019\n",
      "[13] : train loss 0.001311, val loss drop 0.0019 to 0.0018\n",
      "[16] : train loss 0.001759, val loss drop 0.0018 to 0.0016\n",
      "[17] : train loss 0.002679, val loss drop 0.0016 to 0.0014\n",
      "[19] : train loss 0.001232, val loss drop 0.0014 to 0.0011\n",
      "[20] : train loss 0.001029, val loss drop 0.0011 to 0.0011\n",
      "[23] : train loss 0.001037, val loss drop 0.0011 to 0.0010\n",
      "[32] : train loss 0.000832, val loss drop 0.0010 to 0.0009\n",
      "[38] : train loss 0.000748, val loss drop 0.0009 to 0.0008\n",
      "[51] : train loss 0.001510, val loss drop 0.0008 to 0.0008\n",
      "[59] : train loss 0.000582, val loss drop 0.0008 to 0.0006\n",
      "[65] : train loss 0.000901, val loss drop 0.0006 to 0.0005\n",
      "[83] : train loss 0.000472, val loss drop 0.0005 to 0.0004\n",
      "[89] : train loss 0.000276, val loss drop 0.0004 to 0.0003\n",
      "[91] : train loss 0.000169, val loss drop 0.0003 to 0.0002\n",
      "fold 10\n",
      "[1] : train loss 2.475843, val loss drop 10000000.0000 to 1.0547\n",
      "[2] : train loss 0.489024, val loss drop 1.0547 to 0.5217\n",
      "[3] : train loss 0.129154, val loss drop 0.5217 to 0.2203\n",
      "[4] : train loss 0.047188, val loss drop 0.2203 to 0.0362\n",
      "[6] : train loss 0.013057, val loss drop 0.0362 to 0.0181\n",
      "[7] : train loss 0.007902, val loss drop 0.0181 to 0.0098\n",
      "[9] : train loss 0.003650, val loss drop 0.0098 to 0.0047\n",
      "[11] : train loss 0.002261, val loss drop 0.0047 to 0.0047\n",
      "[13] : train loss 0.002241, val loss drop 0.0047 to 0.0029\n",
      "[16] : train loss 0.001614, val loss drop 0.0029 to 0.0024\n",
      "[17] : train loss 0.001347, val loss drop 0.0024 to 0.0022\n",
      "[19] : train loss 0.001100, val loss drop 0.0022 to 0.0020\n",
      "[22] : train loss 0.000946, val loss drop 0.0020 to 0.0013\n",
      "[23] : train loss 0.000729, val loss drop 0.0013 to 0.0011\n",
      "[32] : train loss 0.000578, val loss drop 0.0011 to 0.0010\n",
      "[34] : train loss 0.000461, val loss drop 0.0010 to 0.0009\n",
      "[49] : train loss 0.000515, val loss drop 0.0009 to 0.0009\n",
      "[51] : train loss 0.000364, val loss drop 0.0009 to 0.0008\n",
      "[59] : train loss 0.000304, val loss drop 0.0008 to 0.0007\n",
      "[68] : train loss 0.000394, val loss drop 0.0007 to 0.0005\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 3.275704, val loss drop 10000000.0000 to 1.0651\n",
      "[2] : train loss 0.154422, val loss drop 1.0651 to 0.2299\n",
      "[3] : train loss 0.067388, val loss drop 0.2299 to 0.0558\n",
      "[4] : train loss 0.029866, val loss drop 0.0558 to 0.0173\n",
      "[5] : train loss 0.014387, val loss drop 0.0173 to 0.0064\n",
      "[6] : train loss 0.007466, val loss drop 0.0064 to 0.0057\n",
      "[8] : train loss 0.006544, val loss drop 0.0057 to 0.0042\n",
      "[9] : train loss 0.006422, val loss drop 0.0042 to 0.0039\n",
      "[10] : train loss 0.004574, val loss drop 0.0039 to 0.0036\n",
      "[12] : train loss 0.002915, val loss drop 0.0036 to 0.0022\n",
      "[24] : train loss 0.006671, val loss drop 0.0022 to 0.0020\n",
      "[29] : train loss 0.002912, val loss drop 0.0020 to 0.0015\n",
      "[47] : train loss 0.001680, val loss drop 0.0015 to 0.0012\n",
      "[57] : train loss 0.004354, val loss drop 0.0012 to 0.0011\n",
      "[59] : train loss 0.005476, val loss drop 0.0011 to 0.0009\n",
      "[74] : train loss 0.003411, val loss drop 0.0009 to 0.0006\n",
      "[87] : train loss 0.000759, val loss drop 0.0006 to 0.0006\n",
      "fold 2\n",
      "[1] : train loss 9.502544, val loss drop 10000000.0000 to 1.8499\n",
      "[2] : train loss 0.387037, val loss drop 1.8499 to 0.0958\n",
      "[4] : train loss 0.061495, val loss drop 0.0958 to 0.0579\n",
      "[5] : train loss 0.038950, val loss drop 0.0579 to 0.0366\n",
      "[6] : train loss 0.023345, val loss drop 0.0366 to 0.0139\n",
      "[7] : train loss 0.010996, val loss drop 0.0139 to 0.0127\n",
      "[8] : train loss 0.007925, val loss drop 0.0127 to 0.0105\n",
      "[9] : train loss 0.007748, val loss drop 0.0105 to 0.0079\n",
      "[18] : train loss 0.014848, val loss drop 0.0079 to 0.0077\n",
      "[22] : train loss 0.005595, val loss drop 0.0077 to 0.0057\n",
      "[26] : train loss 0.003932, val loss drop 0.0057 to 0.0053\n",
      "[28] : train loss 0.009591, val loss drop 0.0053 to 0.0050\n",
      "[30] : train loss 0.003716, val loss drop 0.0050 to 0.0040\n",
      "[33] : train loss 0.010099, val loss drop 0.0040 to 0.0032\n",
      "[47] : train loss 0.009479, val loss drop 0.0032 to 0.0026\n",
      "[59] : train loss 0.006719, val loss drop 0.0026 to 0.0023\n",
      "[63] : train loss 0.001962, val loss drop 0.0023 to 0.0016\n",
      "fold 3\n",
      "[1] : train loss 45.814012, val loss drop 10000000.0000 to 0.2089\n",
      "[2] : train loss 0.214710, val loss drop 0.2089 to 0.1546\n",
      "[3] : train loss 0.103133, val loss drop 0.1546 to 0.0886\n",
      "[4] : train loss 0.053828, val loss drop 0.0886 to 0.0364\n",
      "[6] : train loss 0.057005, val loss drop 0.0364 to 0.0361\n",
      "[7] : train loss 0.029228, val loss drop 0.0361 to 0.0229\n",
      "[8] : train loss 0.024563, val loss drop 0.0229 to 0.0182\n",
      "[12] : train loss 0.026451, val loss drop 0.0182 to 0.0154\n",
      "[15] : train loss 0.020744, val loss drop 0.0154 to 0.0115\n",
      "[21] : train loss 0.010895, val loss drop 0.0115 to 0.0111\n",
      "[22] : train loss 0.005295, val loss drop 0.0111 to 0.0097\n",
      "[24] : train loss 0.013897, val loss drop 0.0097 to 0.0079\n",
      "[25] : train loss 0.008535, val loss drop 0.0079 to 0.0050\n",
      "[45] : train loss 0.013572, val loss drop 0.0050 to 0.0047\n",
      "[51] : train loss 0.002910, val loss drop 0.0047 to 0.0043\n",
      "[55] : train loss 0.003994, val loss drop 0.0043 to 0.0042\n",
      "[59] : train loss 0.005335, val loss drop 0.0042 to 0.0039\n",
      "[62] : train loss 0.005131, val loss drop 0.0039 to 0.0036\n",
      "[63] : train loss 0.001630, val loss drop 0.0036 to 0.0034\n",
      "[67] : train loss 0.002953, val loss drop 0.0034 to 0.0033\n",
      "[73] : train loss 0.003831, val loss drop 0.0033 to 0.0027\n",
      "[83] : train loss 0.002002, val loss drop 0.0027 to 0.0021\n",
      "[85] : train loss 0.002806, val loss drop 0.0021 to 0.0019\n",
      "[97] : train loss 0.004709, val loss drop 0.0019 to 0.0016\n",
      "fold 4\n",
      "[1] : train loss 38.014008, val loss drop 10000000.0000 to 10.8280\n",
      "[2] : train loss 2.197454, val loss drop 10.8280 to 1.3831\n",
      "[3] : train loss 0.662767, val loss drop 1.3831 to 0.8708\n",
      "[4] : train loss 0.292391, val loss drop 0.8708 to 0.2519\n",
      "[5] : train loss 0.148899, val loss drop 0.2519 to 0.0982\n",
      "[7] : train loss 0.049875, val loss drop 0.0982 to 0.0480\n",
      "[9] : train loss 0.031842, val loss drop 0.0480 to 0.0313\n",
      "[11] : train loss 0.023509, val loss drop 0.0313 to 0.0311\n",
      "[12] : train loss 0.022687, val loss drop 0.0311 to 0.0285\n",
      "[13] : train loss 0.021424, val loss drop 0.0285 to 0.0235\n",
      "[15] : train loss 0.015264, val loss drop 0.0235 to 0.0199\n",
      "[17] : train loss 0.012845, val loss drop 0.0199 to 0.0185\n",
      "[19] : train loss 0.012402, val loss drop 0.0185 to 0.0184\n",
      "[22] : train loss 0.009922, val loss drop 0.0184 to 0.0156\n",
      "[27] : train loss 0.007853, val loss drop 0.0156 to 0.0141\n",
      "[31] : train loss 0.006742, val loss drop 0.0141 to 0.0110\n",
      "[32] : train loss 0.009204, val loss drop 0.0110 to 0.0101\n",
      "[37] : train loss 0.009147, val loss drop 0.0101 to 0.0084\n",
      "[59] : train loss 0.005526, val loss drop 0.0084 to 0.0074\n",
      "[86] : train loss 0.004432, val loss drop 0.0074 to 0.0070\n",
      "[93] : train loss 0.002069, val loss drop 0.0070 to 0.0070\n",
      "fold 5\n",
      "[1] : train loss 39.021785, val loss drop 10000000.0000 to 27.5684\n",
      "[2] : train loss 1.491677, val loss drop 27.5684 to 2.0438\n",
      "[3] : train loss 0.631698, val loss drop 2.0438 to 0.3327\n",
      "[4] : train loss 0.287645, val loss drop 0.3327 to 0.1900\n",
      "[5] : train loss 0.130490, val loss drop 0.1900 to 0.0787\n",
      "[6] : train loss 0.063432, val loss drop 0.0787 to 0.0525\n",
      "[8] : train loss 0.047824, val loss drop 0.0525 to 0.0284\n",
      "[12] : train loss 0.028172, val loss drop 0.0284 to 0.0271\n",
      "[16] : train loss 0.021832, val loss drop 0.0271 to 0.0229\n",
      "[17] : train loss 0.017397, val loss drop 0.0229 to 0.0206\n",
      "[27] : train loss 0.037286, val loss drop 0.0206 to 0.0111\n",
      "[45] : train loss 0.031408, val loss drop 0.0111 to 0.0094\n",
      "[51] : train loss 0.007546, val loss drop 0.0094 to 0.0086\n",
      "[52] : train loss 0.010386, val loss drop 0.0086 to 0.0084\n",
      "[54] : train loss 0.032371, val loss drop 0.0084 to 0.0070\n",
      "[60] : train loss 0.015480, val loss drop 0.0070 to 0.0064\n",
      "[61] : train loss 0.011943, val loss drop 0.0064 to 0.0062\n",
      "[70] : train loss 0.008342, val loss drop 0.0062 to 0.0053\n",
      "[83] : train loss 0.008462, val loss drop 0.0053 to 0.0048\n",
      "[100] : train loss 0.004091, val loss drop 0.0048 to 0.0038\n",
      "fold 6\n",
      "[1] : train loss 78.840773, val loss drop 10000000.0000 to 0.8282\n",
      "[2] : train loss 0.617854, val loss drop 0.8282 to 0.3140\n",
      "[3] : train loss 0.239696, val loss drop 0.3140 to 0.1914\n",
      "[4] : train loss 0.114911, val loss drop 0.1914 to 0.1364\n",
      "[5] : train loss 0.085636, val loss drop 0.1364 to 0.0568\n",
      "[7] : train loss 0.044547, val loss drop 0.0568 to 0.0391\n",
      "[8] : train loss 0.042371, val loss drop 0.0391 to 0.0346\n",
      "[9] : train loss 0.028316, val loss drop 0.0346 to 0.0326\n",
      "[11] : train loss 0.026107, val loss drop 0.0326 to 0.0280\n",
      "[12] : train loss 0.017489, val loss drop 0.0280 to 0.0217\n",
      "[16] : train loss 0.022284, val loss drop 0.0217 to 0.0176\n",
      "[19] : train loss 0.016955, val loss drop 0.0176 to 0.0173\n",
      "[20] : train loss 0.011986, val loss drop 0.0173 to 0.0135\n",
      "[38] : train loss 0.013945, val loss drop 0.0135 to 0.0131\n",
      "[39] : train loss 0.015222, val loss drop 0.0131 to 0.0119\n",
      "[51] : train loss 0.007926, val loss drop 0.0119 to 0.0118\n",
      "[52] : train loss 0.007582, val loss drop 0.0118 to 0.0113\n",
      "[56] : train loss 0.030776, val loss drop 0.0113 to 0.0092\n",
      "[60] : train loss 0.006913, val loss drop 0.0092 to 0.0091\n",
      "[76] : train loss 0.010175, val loss drop 0.0091 to 0.0065\n",
      "[84] : train loss 0.014765, val loss drop 0.0065 to 0.0064\n",
      "fold 7\n",
      "[1] : train loss 38.186907, val loss drop 10000000.0000 to 3.5093\n",
      "[2] : train loss 1.031197, val loss drop 3.5093 to 1.8816\n",
      "[3] : train loss 0.346815, val loss drop 1.8816 to 0.1516\n",
      "[4] : train loss 0.157846, val loss drop 0.1516 to 0.1344\n",
      "[5] : train loss 0.082955, val loss drop 0.1344 to 0.0719\n",
      "[9] : train loss 0.049074, val loss drop 0.0719 to 0.0553\n",
      "[10] : train loss 0.052777, val loss drop 0.0553 to 0.0364\n",
      "[12] : train loss 0.023726, val loss drop 0.0364 to 0.0344\n",
      "[13] : train loss 0.018097, val loss drop 0.0344 to 0.0250\n",
      "[16] : train loss 0.021064, val loss drop 0.0250 to 0.0168\n",
      "[29] : train loss 0.008976, val loss drop 0.0168 to 0.0121\n",
      "[40] : train loss 0.007574, val loss drop 0.0121 to 0.0111\n",
      "[44] : train loss 0.008805, val loss drop 0.0111 to 0.0097\n",
      "[55] : train loss 0.004181, val loss drop 0.0097 to 0.0077\n",
      "[83] : train loss 0.009592, val loss drop 0.0077 to 0.0065\n",
      "[97] : train loss 0.002011, val loss drop 0.0065 to 0.0065\n",
      "[99] : train loss 0.001780, val loss drop 0.0065 to 0.0059\n",
      "fold 8\n",
      "[1] : train loss 69.918316, val loss drop 10000000.0000 to 0.7748\n",
      "[2] : train loss 0.257347, val loss drop 0.7748 to 0.2839\n",
      "[3] : train loss 0.097502, val loss drop 0.2839 to 0.1153\n",
      "[4] : train loss 0.058784, val loss drop 0.1153 to 0.0735\n",
      "[5] : train loss 0.043670, val loss drop 0.0735 to 0.0382\n",
      "[6] : train loss 0.032567, val loss drop 0.0382 to 0.0290\n",
      "[7] : train loss 0.019758, val loss drop 0.0290 to 0.0206\n",
      "[10] : train loss 0.018228, val loss drop 0.0206 to 0.0161\n",
      "[11] : train loss 0.013061, val loss drop 0.0161 to 0.0158\n",
      "[17] : train loss 0.018288, val loss drop 0.0158 to 0.0148\n",
      "[18] : train loss 0.011775, val loss drop 0.0148 to 0.0128\n",
      "[25] : train loss 0.009842, val loss drop 0.0128 to 0.0115\n",
      "[26] : train loss 0.005053, val loss drop 0.0115 to 0.0095\n",
      "[29] : train loss 0.006367, val loss drop 0.0095 to 0.0083\n",
      "[38] : train loss 0.009337, val loss drop 0.0083 to 0.0078\n",
      "[40] : train loss 0.011319, val loss drop 0.0078 to 0.0067\n",
      "[41] : train loss 0.005844, val loss drop 0.0067 to 0.0060\n",
      "[46] : train loss 0.008752, val loss drop 0.0060 to 0.0059\n",
      "[48] : train loss 0.004657, val loss drop 0.0059 to 0.0059\n",
      "[49] : train loss 0.002899, val loss drop 0.0059 to 0.0056\n",
      "[57] : train loss 0.004384, val loss drop 0.0056 to 0.0046\n",
      "[84] : train loss 0.003486, val loss drop 0.0046 to 0.0045\n",
      "[99] : train loss 0.003011, val loss drop 0.0045 to 0.0038\n",
      "fold 9\n",
      "[1] : train loss 28.933457, val loss drop 10000000.0000 to 1.0907\n",
      "[2] : train loss 0.107413, val loss drop 1.0907 to 0.0874\n",
      "[3] : train loss 0.035474, val loss drop 0.0874 to 0.0230\n",
      "[4] : train loss 0.016901, val loss drop 0.0230 to 0.0154\n",
      "[5] : train loss 0.012425, val loss drop 0.0154 to 0.0123\n",
      "[6] : train loss 0.009403, val loss drop 0.0123 to 0.0083\n",
      "[7] : train loss 0.007350, val loss drop 0.0083 to 0.0071\n",
      "[8] : train loss 0.005677, val loss drop 0.0071 to 0.0065\n",
      "[9] : train loss 0.008240, val loss drop 0.0065 to 0.0060\n",
      "[13] : train loss 0.005098, val loss drop 0.0060 to 0.0051\n",
      "[15] : train loss 0.005374, val loss drop 0.0051 to 0.0037\n",
      "[18] : train loss 0.003309, val loss drop 0.0037 to 0.0033\n",
      "[21] : train loss 0.003133, val loss drop 0.0033 to 0.0030\n",
      "[33] : train loss 0.003936, val loss drop 0.0030 to 0.0028\n",
      "[35] : train loss 0.001715, val loss drop 0.0028 to 0.0018\n",
      "[54] : train loss 0.002627, val loss drop 0.0018 to 0.0015\n",
      "[59] : train loss 0.002334, val loss drop 0.0015 to 0.0014\n",
      "[63] : train loss 0.004580, val loss drop 0.0014 to 0.0014\n",
      "[64] : train loss 0.000917, val loss drop 0.0014 to 0.0012\n",
      "[81] : train loss 0.001984, val loss drop 0.0012 to 0.0011\n",
      "[82] : train loss 0.000838, val loss drop 0.0011 to 0.0009\n",
      "fold 10\n",
      "[1] : train loss 20.632328, val loss drop 10000000.0000 to 2.0686\n",
      "[2] : train loss 0.825919, val loss drop 2.0686 to 0.2626\n",
      "[4] : train loss 0.092506, val loss drop 0.2626 to 0.0479\n",
      "[5] : train loss 0.043190, val loss drop 0.0479 to 0.0237\n",
      "[7] : train loss 0.017957, val loss drop 0.0237 to 0.0223\n",
      "[8] : train loss 0.017458, val loss drop 0.0223 to 0.0201\n",
      "[10] : train loss 0.009864, val loss drop 0.0201 to 0.0111\n",
      "[12] : train loss 0.011274, val loss drop 0.0111 to 0.0107\n",
      "[21] : train loss 0.017339, val loss drop 0.0107 to 0.0092\n",
      "[23] : train loss 0.009692, val loss drop 0.0092 to 0.0084\n",
      "[24] : train loss 0.004816, val loss drop 0.0084 to 0.0070\n",
      "[31] : train loss 0.006036, val loss drop 0.0070 to 0.0054\n",
      "[43] : train loss 0.005890, val loss drop 0.0054 to 0.0053\n",
      "[48] : train loss 0.002785, val loss drop 0.0053 to 0.0047\n",
      "[61] : train loss 0.003603, val loss drop 0.0047 to 0.0042\n",
      "[66] : train loss 0.002150, val loss drop 0.0042 to 0.0039\n",
      "[84] : train loss 0.003548, val loss drop 0.0039 to 0.0038\n",
      "[89] : train loss 0.004965, val loss drop 0.0038 to 0.0037\n",
      "[92] : train loss 0.002355, val loss drop 0.0037 to 0.0027\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 21.458011, val loss drop 10000000.0000 to 4.8632\n",
      "[3] : train loss 1.477867, val loss drop 4.8632 to 4.1290\n",
      "[4] : train loss 0.577519, val loss drop 4.1290 to 0.4095\n",
      "[5] : train loss 0.390730, val loss drop 0.4095 to 0.1907\n",
      "[6] : train loss 0.370310, val loss drop 0.1907 to 0.1540\n",
      "[12] : train loss 0.609362, val loss drop 0.1540 to 0.1284\n",
      "[14] : train loss 0.269357, val loss drop 0.1284 to 0.0726\n",
      "[16] : train loss 0.338370, val loss drop 0.0726 to 0.0658\n",
      "[18] : train loss 0.141550, val loss drop 0.0658 to 0.0526\n",
      "[26] : train loss 0.221415, val loss drop 0.0526 to 0.0421\n",
      "[27] : train loss 0.195370, val loss drop 0.0421 to 0.0338\n",
      "[61] : train loss 0.238343, val loss drop 0.0338 to 0.0232\n",
      "[89] : train loss 0.109395, val loss drop 0.0232 to 0.0203\n",
      "fold 2\n",
      "[1] : train loss 24.548638, val loss drop 10000000.0000 to 3.6075\n",
      "[2] : train loss 2.680981, val loss drop 3.6075 to 2.9893\n",
      "[3] : train loss 0.909623, val loss drop 2.9893 to 1.0831\n",
      "[4] : train loss 0.832026, val loss drop 1.0831 to 0.3802\n",
      "[5] : train loss 0.565176, val loss drop 0.3802 to 0.2497\n",
      "[8] : train loss 0.424077, val loss drop 0.2497 to 0.2427\n",
      "[13] : train loss 0.121801, val loss drop 0.2427 to 0.0780\n",
      "[14] : train loss 0.171172, val loss drop 0.0780 to 0.0699\n",
      "[18] : train loss 0.114275, val loss drop 0.0699 to 0.0636\n",
      "[20] : train loss 0.112945, val loss drop 0.0636 to 0.0605\n",
      "[26] : train loss 0.233206, val loss drop 0.0605 to 0.0525\n",
      "[41] : train loss 0.203767, val loss drop 0.0525 to 0.0436\n",
      "[62] : train loss 0.194241, val loss drop 0.0436 to 0.0384\n",
      "[68] : train loss 0.104199, val loss drop 0.0384 to 0.0380\n",
      "[96] : train loss 0.078877, val loss drop 0.0380 to 0.0321\n",
      "[99] : train loss 0.094742, val loss drop 0.0321 to 0.0174\n",
      "fold 3\n",
      "[1] : train loss 28.808219, val loss drop 10000000.0000 to 7.9073\n",
      "[2] : train loss 3.327241, val loss drop 7.9073 to 7.2369\n",
      "[3] : train loss 0.865820, val loss drop 7.2369 to 4.0583\n",
      "[4] : train loss 0.560193, val loss drop 4.0583 to 0.7403\n",
      "[5] : train loss 0.617930, val loss drop 0.7403 to 0.6218\n",
      "[6] : train loss 0.375926, val loss drop 0.6218 to 0.2021\n",
      "[8] : train loss 0.491002, val loss drop 0.2021 to 0.1259\n",
      "[11] : train loss 0.209523, val loss drop 0.1259 to 0.1207\n",
      "[12] : train loss 0.158382, val loss drop 0.1207 to 0.0909\n",
      "[23] : train loss 0.190179, val loss drop 0.0909 to 0.0884\n",
      "[33] : train loss 0.146441, val loss drop 0.0884 to 0.0640\n",
      "[34] : train loss 0.118078, val loss drop 0.0640 to 0.0386\n",
      "[37] : train loss 0.326457, val loss drop 0.0386 to 0.0381\n",
      "[47] : train loss 0.178202, val loss drop 0.0381 to 0.0378\n",
      "[62] : train loss 0.251969, val loss drop 0.0378 to 0.0357\n",
      "[81] : train loss 0.122510, val loss drop 0.0357 to 0.0294\n",
      "fold 4\n",
      "[1] : train loss 28.578081, val loss drop 10000000.0000 to 7.2559\n",
      "[3] : train loss 1.701700, val loss drop 7.2559 to 6.3142\n",
      "[4] : train loss 1.079998, val loss drop 6.3142 to 3.4812\n",
      "[5] : train loss 0.756700, val loss drop 3.4812 to 0.2899\n",
      "[7] : train loss 0.247588, val loss drop 0.2899 to 0.1630\n",
      "[9] : train loss 0.164550, val loss drop 0.1630 to 0.1131\n",
      "[18] : train loss 0.208021, val loss drop 0.1131 to 0.0695\n",
      "[28] : train loss 0.258365, val loss drop 0.0695 to 0.0468\n",
      "[37] : train loss 0.121682, val loss drop 0.0468 to 0.0442\n",
      "[38] : train loss 0.097730, val loss drop 0.0442 to 0.0366\n",
      "[53] : train loss 0.230449, val loss drop 0.0366 to 0.0352\n",
      "[64] : train loss 0.082837, val loss drop 0.0352 to 0.0310\n",
      "[70] : train loss 0.069897, val loss drop 0.0310 to 0.0272\n",
      "[71] : train loss 0.057419, val loss drop 0.0272 to 0.0175\n",
      "[72] : train loss 0.125223, val loss drop 0.0175 to 0.0156\n",
      "[73] : train loss 0.214962, val loss drop 0.0156 to 0.0126\n",
      "fold 5\n",
      "[1] : train loss 23.476898, val loss drop 10000000.0000 to 4.1423\n",
      "[2] : train loss 2.904867, val loss drop 4.1423 to 2.9950\n",
      "[3] : train loss 1.060874, val loss drop 2.9950 to 1.3497\n",
      "[4] : train loss 0.701363, val loss drop 1.3497 to 0.7651\n",
      "[5] : train loss 0.557119, val loss drop 0.7651 to 0.3692\n",
      "[6] : train loss 0.451742, val loss drop 0.3692 to 0.2386\n",
      "[8] : train loss 0.500591, val loss drop 0.2386 to 0.1658\n",
      "[9] : train loss 0.347648, val loss drop 0.1658 to 0.1442\n",
      "[11] : train loss 0.171995, val loss drop 0.1442 to 0.1134\n",
      "[12] : train loss 0.114904, val loss drop 0.1134 to 0.0579\n",
      "[20] : train loss 0.305063, val loss drop 0.0579 to 0.0566\n",
      "[25] : train loss 0.220123, val loss drop 0.0566 to 0.0441\n",
      "[40] : train loss 0.098891, val loss drop 0.0441 to 0.0286\n",
      "[53] : train loss 0.148712, val loss drop 0.0286 to 0.0253\n",
      "[58] : train loss 0.058115, val loss drop 0.0253 to 0.0213\n",
      "fold 6\n",
      "[1] : train loss 29.492357, val loss drop 10000000.0000 to 11.4319\n",
      "[2] : train loss 2.641398, val loss drop 11.4319 to 1.5293\n",
      "[5] : train loss 0.426758, val loss drop 1.5293 to 0.9551\n",
      "[6] : train loss 0.460361, val loss drop 0.9551 to 0.4317\n",
      "[7] : train loss 0.293994, val loss drop 0.4317 to 0.1519\n",
      "[11] : train loss 0.417843, val loss drop 0.1519 to 0.1374\n",
      "[12] : train loss 0.279848, val loss drop 0.1374 to 0.1202\n",
      "[16] : train loss 0.285136, val loss drop 0.1202 to 0.1093\n",
      "[17] : train loss 0.324380, val loss drop 0.1093 to 0.0995\n",
      "[22] : train loss 0.334692, val loss drop 0.0995 to 0.0581\n",
      "[25] : train loss 0.123763, val loss drop 0.0581 to 0.0376\n",
      "[38] : train loss 0.087711, val loss drop 0.0376 to 0.0334\n",
      "[43] : train loss 0.230032, val loss drop 0.0334 to 0.0299\n",
      "[58] : train loss 0.153190, val loss drop 0.0299 to 0.0281\n",
      "[97] : train loss 0.088577, val loss drop 0.0281 to 0.0238\n",
      "fold 7\n",
      "[1] : train loss 38.652679, val loss drop 10000000.0000 to 11.7662\n",
      "[2] : train loss 3.879405, val loss drop 11.7662 to 4.8376\n",
      "[3] : train loss 1.620292, val loss drop 4.8376 to 3.4381\n",
      "[4] : train loss 0.871080, val loss drop 3.4381 to 1.4195\n",
      "[5] : train loss 0.493798, val loss drop 1.4195 to 0.7045\n",
      "[6] : train loss 0.337986, val loss drop 0.7045 to 0.4062\n",
      "[7] : train loss 0.299576, val loss drop 0.4062 to 0.1983\n",
      "[15] : train loss 0.311856, val loss drop 0.1983 to 0.0839\n",
      "[25] : train loss 0.157752, val loss drop 0.0839 to 0.0476\n",
      "[40] : train loss 0.309705, val loss drop 0.0476 to 0.0353\n",
      "[60] : train loss 0.183173, val loss drop 0.0353 to 0.0347\n",
      "[88] : train loss 0.089758, val loss drop 0.0347 to 0.0279\n",
      "fold 8\n",
      "[1] : train loss 32.538872, val loss drop 10000000.0000 to 26.8615\n",
      "[2] : train loss 3.523837, val loss drop 26.8615 to 7.2003\n",
      "[3] : train loss 1.085167, val loss drop 7.2003 to 3.0426\n",
      "[4] : train loss 0.489599, val loss drop 3.0426 to 1.2341\n",
      "[5] : train loss 0.440206, val loss drop 1.2341 to 0.3522\n",
      "[8] : train loss 0.439916, val loss drop 0.3522 to 0.1871\n",
      "[9] : train loss 0.223036, val loss drop 0.1871 to 0.1641\n",
      "[17] : train loss 0.363577, val loss drop 0.1641 to 0.0777\n",
      "[31] : train loss 0.109417, val loss drop 0.0777 to 0.0699\n",
      "[32] : train loss 0.187123, val loss drop 0.0699 to 0.0540\n",
      "[38] : train loss 0.144828, val loss drop 0.0540 to 0.0512\n",
      "[39] : train loss 0.063496, val loss drop 0.0512 to 0.0429\n",
      "[58] : train loss 0.055773, val loss drop 0.0429 to 0.0332\n",
      "[59] : train loss 0.163022, val loss drop 0.0332 to 0.0232\n",
      "[79] : train loss 0.101692, val loss drop 0.0232 to 0.0195\n",
      "[83] : train loss 0.095369, val loss drop 0.0195 to 0.0157\n",
      "[84] : train loss 0.042159, val loss drop 0.0157 to 0.0143\n",
      "[98] : train loss 0.234674, val loss drop 0.0143 to 0.0119\n",
      "fold 9\n",
      "[1] : train loss 25.372282, val loss drop 10000000.0000 to 3.8136\n",
      "[3] : train loss 0.673231, val loss drop 3.8136 to 2.3677\n",
      "[5] : train loss 0.665918, val loss drop 2.3677 to 0.2214\n",
      "[7] : train loss 0.339388, val loss drop 0.2214 to 0.1950\n",
      "[9] : train loss 0.151850, val loss drop 0.1950 to 0.1700\n",
      "[12] : train loss 0.420140, val loss drop 0.1700 to 0.1682\n",
      "[13] : train loss 0.259537, val loss drop 0.1682 to 0.1674\n",
      "[14] : train loss 0.267547, val loss drop 0.1674 to 0.1075\n",
      "[17] : train loss 0.286953, val loss drop 0.1075 to 0.0586\n",
      "[19] : train loss 0.466515, val loss drop 0.0586 to 0.0484\n",
      "[20] : train loss 0.103718, val loss drop 0.0484 to 0.0465\n",
      "[26] : train loss 0.533157, val loss drop 0.0465 to 0.0413\n",
      "[54] : train loss 0.301051, val loss drop 0.0413 to 0.0372\n",
      "[58] : train loss 0.148105, val loss drop 0.0372 to 0.0291\n",
      "[75] : train loss 0.242849, val loss drop 0.0291 to 0.0199\n",
      "[77] : train loss 0.121331, val loss drop 0.0199 to 0.0176\n",
      "fold 10\n",
      "[1] : train loss 39.166102, val loss drop 10000000.0000 to 7.7357\n",
      "[2] : train loss 4.525819, val loss drop 7.7357 to 4.4938\n",
      "[4] : train loss 0.649299, val loss drop 4.4938 to 1.4780\n",
      "[5] : train loss 0.531136, val loss drop 1.4780 to 0.2900\n",
      "[6] : train loss 0.376234, val loss drop 0.2900 to 0.2056\n",
      "[12] : train loss 0.644427, val loss drop 0.2056 to 0.1864\n",
      "[14] : train loss 0.185449, val loss drop 0.1864 to 0.1592\n",
      "[15] : train loss 0.184477, val loss drop 0.1592 to 0.1295\n",
      "[16] : train loss 0.215613, val loss drop 0.1295 to 0.0617\n",
      "[31] : train loss 0.294684, val loss drop 0.0617 to 0.0376\n",
      "[53] : train loss 0.099072, val loss drop 0.0376 to 0.0207\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './model/20200627-210501/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-263.674805</td>\n",
       "      <td>-41.601894</td>\n",
       "      <td>112.711327</td>\n",
       "      <td>0.442152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>316.673340</td>\n",
       "      <td>-283.934509</td>\n",
       "      <td>89.251762</td>\n",
       "      <td>0.477493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-236.339676</td>\n",
       "      <td>129.708801</td>\n",
       "      <td>30.965876</td>\n",
       "      <td>0.368679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>156.548706</td>\n",
       "      <td>273.805786</td>\n",
       "      <td>28.209118</td>\n",
       "      <td>0.394775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-168.113861</td>\n",
       "      <td>184.381714</td>\n",
       "      <td>132.927673</td>\n",
       "      <td>0.435188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -263.674805  -41.601894  112.711327  0.442152\n",
       "1  2801  316.673340 -283.934509   89.251762  0.477493\n",
       "2  2802 -236.339676  129.708801   30.965876  0.368679\n",
       "3  2803  156.548706  273.805786   28.209118  0.394775\n",
       "4  2804 -168.113861  184.381714  132.927673  0.435188"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, 'submit.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

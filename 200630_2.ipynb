{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- normalized input을 channel-wise concat\n",
    "- 학습 속도 및 성능을 봤을 때 3x1, 4x1, 5x1이 가장 적당한듯\n",
    "- normalize 성능 향상됨 -> 다른 filter size로 합쳐서 ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "def fix_seed(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)\n",
    "        #return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis = 2):\n",
    "    mu = np.expand_dims(x.mean(axis = 2), axis = axis)\n",
    "    sd = np.expand_dims(x.std(axis = 2), axis = axis)\n",
    "\n",
    "    normalized = (x - mu) / sd\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "# concatenate normalized data\n",
    "train_norm = normalize(train_f)\n",
    "test_norm = normalize(test_f)\n",
    "\n",
    "train_f = np.concatenate((train_f, train_norm), axis = 1)\n",
    "test_f = np.concatenate((test_f, test_norm), axis = 1)\n",
    "\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "        fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        model = cnn_model(conv, fc)\n",
    "        #model = get_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.717575, val loss drop 10000000.0000 to 1.8826\n",
      "[2] : train loss 1.237440, val loss drop 1.8826 to 0.6406\n",
      "[3] : train loss 0.348820, val loss drop 0.6406 to 0.1182\n",
      "[4] : train loss 0.060406, val loss drop 0.1182 to 0.0899\n",
      "[5] : train loss 0.017982, val loss drop 0.0899 to 0.0280\n",
      "[6] : train loss 0.008308, val loss drop 0.0280 to 0.0170\n",
      "[7] : train loss 0.004943, val loss drop 0.0170 to 0.0054\n",
      "[9] : train loss 0.003202, val loss drop 0.0054 to 0.0035\n",
      "[10] : train loss 0.002429, val loss drop 0.0035 to 0.0034\n",
      "[11] : train loss 0.001864, val loss drop 0.0034 to 0.0029\n",
      "[14] : train loss 0.002581, val loss drop 0.0029 to 0.0015\n",
      "[21] : train loss 0.001630, val loss drop 0.0015 to 0.0011\n",
      "[25] : train loss 0.001366, val loss drop 0.0011 to 0.0008\n",
      "[84] : train loss 0.000490, val loss drop 0.0008 to 0.0007\n",
      "[87] : train loss 0.000828, val loss drop 0.0007 to 0.0007\n",
      "[89] : train loss 0.000671, val loss drop 0.0007 to 0.0005\n",
      "[91] : train loss 0.000303, val loss drop 0.0005 to 0.0004\n",
      "fold 2\n",
      "[1] : train loss 2.779078, val loss drop 10000000.0000 to 1.4413\n",
      "[2] : train loss 1.298068, val loss drop 1.4413 to 0.4844\n",
      "[3] : train loss 0.274513, val loss drop 0.4844 to 0.0626\n",
      "[5] : train loss 0.012823, val loss drop 0.0626 to 0.0548\n",
      "[6] : train loss 0.008139, val loss drop 0.0548 to 0.0457\n",
      "[7] : train loss 0.005193, val loss drop 0.0457 to 0.0067\n",
      "[8] : train loss 0.003199, val loss drop 0.0067 to 0.0058\n",
      "[9] : train loss 0.002548, val loss drop 0.0058 to 0.0027\n",
      "[10] : train loss 0.002003, val loss drop 0.0027 to 0.0023\n",
      "[11] : train loss 0.002004, val loss drop 0.0023 to 0.0009\n",
      "[20] : train loss 0.001714, val loss drop 0.0009 to 0.0007\n",
      "[29] : train loss 0.000754, val loss drop 0.0007 to 0.0006\n",
      "[55] : train loss 0.000461, val loss drop 0.0006 to 0.0006\n",
      "[65] : train loss 0.000333, val loss drop 0.0006 to 0.0004\n",
      "[82] : train loss 0.000412, val loss drop 0.0004 to 0.0004\n",
      "fold 3\n",
      "[1] : train loss 2.933222, val loss drop 10000000.0000 to 1.9133\n",
      "[2] : train loss 1.627162, val loss drop 1.9133 to 0.7698\n",
      "[3] : train loss 0.333445, val loss drop 0.7698 to 0.1178\n",
      "[5] : train loss 0.022330, val loss drop 0.1178 to 0.0895\n",
      "[6] : train loss 0.010889, val loss drop 0.0895 to 0.0135\n",
      "[7] : train loss 0.005755, val loss drop 0.0135 to 0.0078\n",
      "[8] : train loss 0.003980, val loss drop 0.0078 to 0.0031\n",
      "[11] : train loss 0.002584, val loss drop 0.0031 to 0.0019\n",
      "[15] : train loss 0.001410, val loss drop 0.0019 to 0.0013\n",
      "[18] : train loss 0.001244, val loss drop 0.0013 to 0.0011\n",
      "[22] : train loss 0.000871, val loss drop 0.0011 to 0.0007\n",
      "[48] : train loss 0.001100, val loss drop 0.0007 to 0.0006\n",
      "[62] : train loss 0.000620, val loss drop 0.0006 to 0.0006\n",
      "[66] : train loss 0.000592, val loss drop 0.0006 to 0.0006\n",
      "[71] : train loss 0.000618, val loss drop 0.0006 to 0.0005\n",
      "[75] : train loss 0.000471, val loss drop 0.0005 to 0.0004\n",
      "[76] : train loss 0.000400, val loss drop 0.0004 to 0.0003\n",
      "fold 4\n",
      "[1] : train loss 2.868277, val loss drop 10000000.0000 to 1.6507\n",
      "[2] : train loss 1.221345, val loss drop 1.6507 to 0.3313\n",
      "[3] : train loss 0.163002, val loss drop 0.3313 to 0.1347\n",
      "[5] : train loss 0.015632, val loss drop 0.1347 to 0.0451\n",
      "[6] : train loss 0.006450, val loss drop 0.0451 to 0.0199\n",
      "[7] : train loss 0.003756, val loss drop 0.0199 to 0.0059\n",
      "[8] : train loss 0.002734, val loss drop 0.0059 to 0.0036\n",
      "[9] : train loss 0.002454, val loss drop 0.0036 to 0.0033\n",
      "[10] : train loss 0.001637, val loss drop 0.0033 to 0.0017\n",
      "[11] : train loss 0.001550, val loss drop 0.0017 to 0.0017\n",
      "[12] : train loss 0.001191, val loss drop 0.0017 to 0.0016\n",
      "[14] : train loss 0.001089, val loss drop 0.0016 to 0.0013\n",
      "[16] : train loss 0.001484, val loss drop 0.0013 to 0.0012\n",
      "[20] : train loss 0.001349, val loss drop 0.0012 to 0.0011\n",
      "[21] : train loss 0.000909, val loss drop 0.0011 to 0.0008\n",
      "[22] : train loss 0.000616, val loss drop 0.0008 to 0.0008\n",
      "[23] : train loss 0.000897, val loss drop 0.0008 to 0.0005\n",
      "[40] : train loss 0.001127, val loss drop 0.0005 to 0.0005\n",
      "[67] : train loss 0.000961, val loss drop 0.0005 to 0.0005\n",
      "[80] : train loss 0.000772, val loss drop 0.0005 to 0.0004\n",
      "[82] : train loss 0.000623, val loss drop 0.0004 to 0.0004\n",
      "[95] : train loss 0.000699, val loss drop 0.0004 to 0.0003\n",
      "fold 5\n",
      "[1] : train loss 2.755486, val loss drop 10000000.0000 to 1.7543\n",
      "[2] : train loss 1.403035, val loss drop 1.7543 to 0.7210\n",
      "[3] : train loss 0.356592, val loss drop 0.7210 to 0.1239\n",
      "[5] : train loss 0.021468, val loss drop 0.1239 to 0.0576\n",
      "[6] : train loss 0.009820, val loss drop 0.0576 to 0.0141\n",
      "[7] : train loss 0.007602, val loss drop 0.0141 to 0.0074\n",
      "[9] : train loss 0.003519, val loss drop 0.0074 to 0.0039\n",
      "[10] : train loss 0.002940, val loss drop 0.0039 to 0.0035\n",
      "[11] : train loss 0.002245, val loss drop 0.0035 to 0.0018\n",
      "[17] : train loss 0.004744, val loss drop 0.0018 to 0.0011\n",
      "[43] : train loss 0.000860, val loss drop 0.0011 to 0.0007\n",
      "[61] : train loss 0.000892, val loss drop 0.0007 to 0.0005\n",
      "[66] : train loss 0.000840, val loss drop 0.0005 to 0.0004\n",
      "[89] : train loss 0.000464, val loss drop 0.0004 to 0.0004\n",
      "fold 6\n",
      "[1] : train loss 2.826221, val loss drop 10000000.0000 to 1.6693\n",
      "[2] : train loss 1.109338, val loss drop 1.6693 to 0.2530\n",
      "[3] : train loss 0.130418, val loss drop 0.2530 to 0.2259\n",
      "[4] : train loss 0.031722, val loss drop 0.2259 to 0.0407\n",
      "[5] : train loss 0.012994, val loss drop 0.0407 to 0.0145\n",
      "[6] : train loss 0.006615, val loss drop 0.0145 to 0.0134\n",
      "[8] : train loss 0.002904, val loss drop 0.0134 to 0.0046\n",
      "[10] : train loss 0.001811, val loss drop 0.0046 to 0.0027\n",
      "[11] : train loss 0.001401, val loss drop 0.0027 to 0.0017\n",
      "[13] : train loss 0.001314, val loss drop 0.0017 to 0.0015\n",
      "[14] : train loss 0.001525, val loss drop 0.0015 to 0.0012\n",
      "[22] : train loss 0.000705, val loss drop 0.0012 to 0.0006\n",
      "[29] : train loss 0.000544, val loss drop 0.0006 to 0.0006\n",
      "[56] : train loss 0.000725, val loss drop 0.0006 to 0.0004\n",
      "[59] : train loss 0.000530, val loss drop 0.0004 to 0.0003\n",
      "[77] : train loss 0.000914, val loss drop 0.0003 to 0.0003\n",
      "fold 7\n",
      "[1] : train loss 2.712942, val loss drop 10000000.0000 to 1.1766\n",
      "[2] : train loss 0.819774, val loss drop 1.1766 to 0.1525\n",
      "[3] : train loss 0.092946, val loss drop 0.1525 to 0.0583\n",
      "[5] : train loss 0.011254, val loss drop 0.0583 to 0.0381\n",
      "[6] : train loss 0.005195, val loss drop 0.0381 to 0.0137\n",
      "[7] : train loss 0.003894, val loss drop 0.0137 to 0.0094\n",
      "[9] : train loss 0.002180, val loss drop 0.0094 to 0.0031\n",
      "[11] : train loss 0.001077, val loss drop 0.0031 to 0.0017\n",
      "[14] : train loss 0.001031, val loss drop 0.0017 to 0.0010\n",
      "[18] : train loss 0.001038, val loss drop 0.0010 to 0.0008\n",
      "[30] : train loss 0.000725, val loss drop 0.0008 to 0.0006\n",
      "[35] : train loss 0.000867, val loss drop 0.0006 to 0.0005\n",
      "[71] : train loss 0.001150, val loss drop 0.0005 to 0.0004\n",
      "[85] : train loss 0.000592, val loss drop 0.0004 to 0.0003\n",
      "fold 8\n",
      "[1] : train loss 2.818778, val loss drop 10000000.0000 to 1.3907\n",
      "[2] : train loss 1.021902, val loss drop 1.3907 to 0.2120\n",
      "[3] : train loss 0.098806, val loss drop 0.2120 to 0.1378\n",
      "[4] : train loss 0.026125, val loss drop 0.1378 to 0.1020\n",
      "[5] : train loss 0.011234, val loss drop 0.1020 to 0.0561\n",
      "[6] : train loss 0.005518, val loss drop 0.0561 to 0.0161\n",
      "[7] : train loss 0.003375, val loss drop 0.0161 to 0.0091\n",
      "[8] : train loss 0.002243, val loss drop 0.0091 to 0.0023\n",
      "[10] : train loss 0.001717, val loss drop 0.0023 to 0.0017\n",
      "[15] : train loss 0.001543, val loss drop 0.0017 to 0.0009\n",
      "[17] : train loss 0.000862, val loss drop 0.0009 to 0.0009\n",
      "[18] : train loss 0.000978, val loss drop 0.0009 to 0.0009\n",
      "[36] : train loss 0.001236, val loss drop 0.0009 to 0.0008\n",
      "[37] : train loss 0.000622, val loss drop 0.0008 to 0.0007\n",
      "[43] : train loss 0.000956, val loss drop 0.0007 to 0.0006\n",
      "[46] : train loss 0.000818, val loss drop 0.0006 to 0.0006\n",
      "[50] : train loss 0.000801, val loss drop 0.0006 to 0.0003\n",
      "[62] : train loss 0.000508, val loss drop 0.0003 to 0.0003\n",
      "[79] : train loss 0.000512, val loss drop 0.0003 to 0.0003\n",
      "fold 9\n",
      "[1] : train loss 2.867933, val loss drop 10000000.0000 to 1.8136\n",
      "[2] : train loss 1.634080, val loss drop 1.8136 to 0.5976\n",
      "[3] : train loss 0.382953, val loss drop 0.5976 to 0.1512\n",
      "[5] : train loss 0.021773, val loss drop 0.1512 to 0.1509\n",
      "[6] : train loss 0.009729, val loss drop 0.1509 to 0.0104\n",
      "[8] : train loss 0.003630, val loss drop 0.0104 to 0.0092\n",
      "[10] : train loss 0.002277, val loss drop 0.0092 to 0.0088\n",
      "[12] : train loss 0.002691, val loss drop 0.0088 to 0.0035\n",
      "[16] : train loss 0.001903, val loss drop 0.0035 to 0.0033\n",
      "[17] : train loss 0.002050, val loss drop 0.0033 to 0.0019\n",
      "[21] : train loss 0.001057, val loss drop 0.0019 to 0.0012\n",
      "[26] : train loss 0.001147, val loss drop 0.0012 to 0.0012\n",
      "[28] : train loss 0.001052, val loss drop 0.0012 to 0.0010\n",
      "[35] : train loss 0.001037, val loss drop 0.0010 to 0.0005\n",
      "[61] : train loss 0.000772, val loss drop 0.0005 to 0.0005\n",
      "[69] : train loss 0.000728, val loss drop 0.0005 to 0.0005\n",
      "fold 10\n",
      "[1] : train loss 2.773946, val loss drop 10000000.0000 to 1.5123\n",
      "[2] : train loss 1.065801, val loss drop 1.5123 to 0.2112\n",
      "[3] : train loss 0.112045, val loss drop 0.2112 to 0.0887\n",
      "[4] : train loss 0.030537, val loss drop 0.0887 to 0.0771\n",
      "[5] : train loss 0.012963, val loss drop 0.0771 to 0.0449\n",
      "[6] : train loss 0.007050, val loss drop 0.0449 to 0.0128\n",
      "[7] : train loss 0.004362, val loss drop 0.0128 to 0.0028\n",
      "[9] : train loss 0.002099, val loss drop 0.0028 to 0.0022\n",
      "[12] : train loss 0.001737, val loss drop 0.0022 to 0.0019\n",
      "[13] : train loss 0.001278, val loss drop 0.0019 to 0.0015\n",
      "[14] : train loss 0.001608, val loss drop 0.0015 to 0.0013\n",
      "[16] : train loss 0.001286, val loss drop 0.0013 to 0.0011\n",
      "[21] : train loss 0.001174, val loss drop 0.0011 to 0.0008\n",
      "[22] : train loss 0.000638, val loss drop 0.0008 to 0.0006\n",
      "[28] : train loss 0.000626, val loss drop 0.0006 to 0.0005\n",
      "[33] : train loss 0.000533, val loss drop 0.0005 to 0.0005\n",
      "[44] : train loss 0.000607, val loss drop 0.0005 to 0.0004\n",
      "[56] : train loss 0.000690, val loss drop 0.0004 to 0.0004\n",
      "[58] : train loss 0.000880, val loss drop 0.0004 to 0.0004\n",
      "[65] : train loss 0.000562, val loss drop 0.0004 to 0.0003\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 14.767842, val loss drop 10000000.0000 to 1.0615\n",
      "[2] : train loss 0.340495, val loss drop 1.0615 to 0.3819\n",
      "[3] : train loss 0.123522, val loss drop 0.3819 to 0.0984\n",
      "[4] : train loss 0.050528, val loss drop 0.0984 to 0.0254\n",
      "[5] : train loss 0.025217, val loss drop 0.0254 to 0.0195\n",
      "[6] : train loss 0.016287, val loss drop 0.0195 to 0.0157\n",
      "[7] : train loss 0.014373, val loss drop 0.0157 to 0.0125\n",
      "[10] : train loss 0.008727, val loss drop 0.0125 to 0.0095\n",
      "[11] : train loss 0.007905, val loss drop 0.0095 to 0.0084\n",
      "[12] : train loss 0.006612, val loss drop 0.0084 to 0.0069\n",
      "[17] : train loss 0.007845, val loss drop 0.0069 to 0.0068\n",
      "[19] : train loss 0.006822, val loss drop 0.0068 to 0.0054\n",
      "[20] : train loss 0.004595, val loss drop 0.0054 to 0.0043\n",
      "[25] : train loss 0.003664, val loss drop 0.0043 to 0.0037\n",
      "[29] : train loss 0.002764, val loss drop 0.0037 to 0.0034\n",
      "[34] : train loss 0.004235, val loss drop 0.0034 to 0.0031\n",
      "[45] : train loss 0.002386, val loss drop 0.0031 to 0.0029\n",
      "[49] : train loss 0.002145, val loss drop 0.0029 to 0.0022\n",
      "[59] : train loss 0.001992, val loss drop 0.0022 to 0.0020\n",
      "[77] : train loss 0.002659, val loss drop 0.0020 to 0.0016\n",
      "[85] : train loss 0.001245, val loss drop 0.0016 to 0.0015\n",
      "[86] : train loss 0.002861, val loss drop 0.0015 to 0.0014\n",
      "[95] : train loss 0.001168, val loss drop 0.0014 to 0.0013\n",
      "[96] : train loss 0.001248, val loss drop 0.0013 to 0.0012\n",
      "fold 2\n",
      "[1] : train loss 3.085566, val loss drop 10000000.0000 to 1.0213\n",
      "[2] : train loss 0.192188, val loss drop 1.0213 to 0.0753\n",
      "[3] : train loss 0.039872, val loss drop 0.0753 to 0.0374\n",
      "[4] : train loss 0.017667, val loss drop 0.0374 to 0.0239\n",
      "[5] : train loss 0.009714, val loss drop 0.0239 to 0.0224\n",
      "[6] : train loss 0.006468, val loss drop 0.0224 to 0.0070\n",
      "[7] : train loss 0.005808, val loss drop 0.0070 to 0.0033\n",
      "[10] : train loss 0.003310, val loss drop 0.0033 to 0.0032\n",
      "[12] : train loss 0.002464, val loss drop 0.0032 to 0.0031\n",
      "[13] : train loss 0.002274, val loss drop 0.0031 to 0.0019\n",
      "[17] : train loss 0.002664, val loss drop 0.0019 to 0.0019\n",
      "[23] : train loss 0.003235, val loss drop 0.0019 to 0.0019\n",
      "[25] : train loss 0.002362, val loss drop 0.0019 to 0.0017\n",
      "[26] : train loss 0.002024, val loss drop 0.0017 to 0.0013\n",
      "[30] : train loss 0.001672, val loss drop 0.0013 to 0.0012\n",
      "[34] : train loss 0.001703, val loss drop 0.0012 to 0.0010\n",
      "[36] : train loss 0.001398, val loss drop 0.0010 to 0.0010\n",
      "[49] : train loss 0.001433, val loss drop 0.0010 to 0.0008\n",
      "[53] : train loss 0.000922, val loss drop 0.0008 to 0.0008\n",
      "[54] : train loss 0.001001, val loss drop 0.0008 to 0.0008\n",
      "[71] : train loss 0.000784, val loss drop 0.0008 to 0.0006\n",
      "[79] : train loss 0.001555, val loss drop 0.0006 to 0.0005\n",
      "fold 3\n",
      "[1] : train loss 21.889457, val loss drop 10000000.0000 to 0.6191\n",
      "[2] : train loss 0.266390, val loss drop 0.6191 to 0.2479\n",
      "[3] : train loss 0.093546, val loss drop 0.2479 to 0.0839\n",
      "[4] : train loss 0.042891, val loss drop 0.0839 to 0.0282\n",
      "[5] : train loss 0.026084, val loss drop 0.0282 to 0.0251\n",
      "[6] : train loss 0.019064, val loss drop 0.0251 to 0.0198\n",
      "[7] : train loss 0.014971, val loss drop 0.0198 to 0.0173\n",
      "[8] : train loss 0.013062, val loss drop 0.0173 to 0.0143\n",
      "[9] : train loss 0.010799, val loss drop 0.0143 to 0.0099\n",
      "[15] : train loss 0.008776, val loss drop 0.0099 to 0.0093\n",
      "[17] : train loss 0.005542, val loss drop 0.0093 to 0.0091\n",
      "[18] : train loss 0.004849, val loss drop 0.0091 to 0.0082\n",
      "[19] : train loss 0.005433, val loss drop 0.0082 to 0.0073\n",
      "[23] : train loss 0.005679, val loss drop 0.0073 to 0.0067\n",
      "[25] : train loss 0.003769, val loss drop 0.0067 to 0.0054\n",
      "[28] : train loss 0.006751, val loss drop 0.0054 to 0.0053\n",
      "[33] : train loss 0.004268, val loss drop 0.0053 to 0.0042\n",
      "[36] : train loss 0.002771, val loss drop 0.0042 to 0.0037\n",
      "[39] : train loss 0.003766, val loss drop 0.0037 to 0.0037\n",
      "[53] : train loss 0.002822, val loss drop 0.0037 to 0.0030\n",
      "[66] : train loss 0.003086, val loss drop 0.0030 to 0.0022\n",
      "[73] : train loss 0.001246, val loss drop 0.0022 to 0.0019\n",
      "fold 4\n",
      "[1] : train loss 6.808199, val loss drop 10000000.0000 to 1.2217\n",
      "[2] : train loss 0.132380, val loss drop 1.2217 to 0.3646\n",
      "[3] : train loss 0.052633, val loss drop 0.3646 to 0.0331\n",
      "[4] : train loss 0.029391, val loss drop 0.0331 to 0.0212\n",
      "[5] : train loss 0.017247, val loss drop 0.0212 to 0.0114\n",
      "[9] : train loss 0.006836, val loss drop 0.0114 to 0.0112\n",
      "[10] : train loss 0.007842, val loss drop 0.0112 to 0.0075\n",
      "[12] : train loss 0.011775, val loss drop 0.0075 to 0.0056\n",
      "[13] : train loss 0.005605, val loss drop 0.0056 to 0.0054\n",
      "[14] : train loss 0.006689, val loss drop 0.0054 to 0.0046\n",
      "[18] : train loss 0.004612, val loss drop 0.0046 to 0.0033\n",
      "[30] : train loss 0.005084, val loss drop 0.0033 to 0.0024\n",
      "[35] : train loss 0.003503, val loss drop 0.0024 to 0.0023\n",
      "[36] : train loss 0.003401, val loss drop 0.0023 to 0.0022\n",
      "[37] : train loss 0.002151, val loss drop 0.0022 to 0.0019\n",
      "[40] : train loss 0.003579, val loss drop 0.0019 to 0.0017\n",
      "[46] : train loss 0.001970, val loss drop 0.0017 to 0.0013\n",
      "[53] : train loss 0.001703, val loss drop 0.0013 to 0.0013\n",
      "[67] : train loss 0.001954, val loss drop 0.0013 to 0.0008\n",
      "[71] : train loss 0.002761, val loss drop 0.0008 to 0.0008\n",
      "[88] : train loss 0.001516, val loss drop 0.0008 to 0.0008\n",
      "fold 5\n",
      "[1] : train loss 11.849733, val loss drop 10000000.0000 to 2.5645\n",
      "[2] : train loss 0.160618, val loss drop 2.5645 to 0.0772\n",
      "[3] : train loss 0.045147, val loss drop 0.0772 to 0.0156\n",
      "[4] : train loss 0.016035, val loss drop 0.0156 to 0.0122\n",
      "[5] : train loss 0.010192, val loss drop 0.0122 to 0.0090\n",
      "[6] : train loss 0.006779, val loss drop 0.0090 to 0.0057\n",
      "[7] : train loss 0.005263, val loss drop 0.0057 to 0.0046\n",
      "[9] : train loss 0.003999, val loss drop 0.0046 to 0.0044\n",
      "[10] : train loss 0.003518, val loss drop 0.0044 to 0.0038\n",
      "[11] : train loss 0.003267, val loss drop 0.0038 to 0.0031\n",
      "[12] : train loss 0.003017, val loss drop 0.0031 to 0.0029\n",
      "[14] : train loss 0.002700, val loss drop 0.0029 to 0.0023\n",
      "[19] : train loss 0.002109, val loss drop 0.0023 to 0.0022\n",
      "[20] : train loss 0.001638, val loss drop 0.0022 to 0.0022\n",
      "[24] : train loss 0.002027, val loss drop 0.0022 to 0.0022\n",
      "[25] : train loss 0.001542, val loss drop 0.0022 to 0.0020\n",
      "[28] : train loss 0.001445, val loss drop 0.0020 to 0.0015\n",
      "[30] : train loss 0.001409, val loss drop 0.0015 to 0.0015\n",
      "[31] : train loss 0.002042, val loss drop 0.0015 to 0.0014\n",
      "[32] : train loss 0.001663, val loss drop 0.0014 to 0.0014\n",
      "[36] : train loss 0.001757, val loss drop 0.0014 to 0.0013\n",
      "[37] : train loss 0.000884, val loss drop 0.0013 to 0.0011\n",
      "[38] : train loss 0.001051, val loss drop 0.0011 to 0.0011\n",
      "[54] : train loss 0.001774, val loss drop 0.0011 to 0.0010\n",
      "[60] : train loss 0.001424, val loss drop 0.0010 to 0.0009\n",
      "[65] : train loss 0.000938, val loss drop 0.0009 to 0.0009\n",
      "[71] : train loss 0.002509, val loss drop 0.0009 to 0.0009\n",
      "[73] : train loss 0.001459, val loss drop 0.0009 to 0.0009\n",
      "[81] : train loss 0.001368, val loss drop 0.0009 to 0.0007\n",
      "[84] : train loss 0.002201, val loss drop 0.0007 to 0.0006\n",
      "[89] : train loss 0.001245, val loss drop 0.0006 to 0.0006\n",
      "[96] : train loss 0.000647, val loss drop 0.0006 to 0.0005\n",
      "fold 6\n",
      "[1] : train loss 3.021727, val loss drop 10000000.0000 to 0.3857\n",
      "[2] : train loss 0.188790, val loss drop 0.3857 to 0.0749\n",
      "[3] : train loss 0.037890, val loss drop 0.0749 to 0.0458\n",
      "[4] : train loss 0.022407, val loss drop 0.0458 to 0.0238\n",
      "[6] : train loss 0.013765, val loss drop 0.0238 to 0.0085\n",
      "[9] : train loss 0.010527, val loss drop 0.0085 to 0.0029\n",
      "[12] : train loss 0.003566, val loss drop 0.0029 to 0.0021\n",
      "[21] : train loss 0.003237, val loss drop 0.0021 to 0.0016\n",
      "[31] : train loss 0.003031, val loss drop 0.0016 to 0.0015\n",
      "[40] : train loss 0.001553, val loss drop 0.0015 to 0.0011\n",
      "[42] : train loss 0.002219, val loss drop 0.0011 to 0.0008\n",
      "[57] : train loss 0.002768, val loss drop 0.0008 to 0.0008\n",
      "[64] : train loss 0.005485, val loss drop 0.0008 to 0.0007\n",
      "[69] : train loss 0.002249, val loss drop 0.0007 to 0.0006\n",
      "[71] : train loss 0.001666, val loss drop 0.0006 to 0.0005\n",
      "fold 7\n",
      "[1] : train loss 12.011013, val loss drop 10000000.0000 to 2.1881\n",
      "[2] : train loss 0.489501, val loss drop 2.1881 to 0.4261\n",
      "[3] : train loss 0.152586, val loss drop 0.4261 to 0.1464\n",
      "[4] : train loss 0.052156, val loss drop 0.1464 to 0.0489\n",
      "[5] : train loss 0.025704, val loss drop 0.0489 to 0.0306\n",
      "[6] : train loss 0.018510, val loss drop 0.0306 to 0.0156\n",
      "[7] : train loss 0.011842, val loss drop 0.0156 to 0.0139\n",
      "[8] : train loss 0.008486, val loss drop 0.0139 to 0.0082\n",
      "[9] : train loss 0.006924, val loss drop 0.0082 to 0.0074\n",
      "[12] : train loss 0.006267, val loss drop 0.0074 to 0.0062\n",
      "[13] : train loss 0.006939, val loss drop 0.0062 to 0.0057\n",
      "[15] : train loss 0.004995, val loss drop 0.0057 to 0.0050\n",
      "[17] : train loss 0.005655, val loss drop 0.0050 to 0.0045\n",
      "[18] : train loss 0.003769, val loss drop 0.0045 to 0.0043\n",
      "[23] : train loss 0.004807, val loss drop 0.0043 to 0.0040\n",
      "[28] : train loss 0.002718, val loss drop 0.0040 to 0.0040\n",
      "[29] : train loss 0.002838, val loss drop 0.0040 to 0.0040\n",
      "[33] : train loss 0.003291, val loss drop 0.0040 to 0.0023\n",
      "[41] : train loss 0.003042, val loss drop 0.0023 to 0.0022\n",
      "[62] : train loss 0.002519, val loss drop 0.0022 to 0.0016\n",
      "[73] : train loss 0.001658, val loss drop 0.0016 to 0.0014\n",
      "[94] : train loss 0.000925, val loss drop 0.0014 to 0.0011\n",
      "[98] : train loss 0.000966, val loss drop 0.0011 to 0.0010\n",
      "fold 8\n",
      "[1] : train loss 7.410054, val loss drop 10000000.0000 to 2.1114\n",
      "[2] : train loss 0.230211, val loss drop 2.1114 to 0.0506\n",
      "[3] : train loss 0.071464, val loss drop 0.0506 to 0.0283\n",
      "[4] : train loss 0.032941, val loss drop 0.0283 to 0.0149\n",
      "[6] : train loss 0.009894, val loss drop 0.0149 to 0.0073\n",
      "[7] : train loss 0.006441, val loss drop 0.0073 to 0.0068\n",
      "[10] : train loss 0.006798, val loss drop 0.0068 to 0.0043\n",
      "[12] : train loss 0.004204, val loss drop 0.0043 to 0.0042\n",
      "[18] : train loss 0.005037, val loss drop 0.0042 to 0.0032\n",
      "[23] : train loss 0.005049, val loss drop 0.0032 to 0.0021\n",
      "[42] : train loss 0.001315, val loss drop 0.0021 to 0.0020\n",
      "[44] : train loss 0.001730, val loss drop 0.0020 to 0.0019\n",
      "[45] : train loss 0.001430, val loss drop 0.0019 to 0.0017\n",
      "[63] : train loss 0.002008, val loss drop 0.0017 to 0.0016\n",
      "[65] : train loss 0.001502, val loss drop 0.0016 to 0.0016\n",
      "[68] : train loss 0.001703, val loss drop 0.0016 to 0.0009\n",
      "[98] : train loss 0.001324, val loss drop 0.0009 to 0.0008\n",
      "[99] : train loss 0.001278, val loss drop 0.0008 to 0.0007\n",
      "[100] : train loss 0.001903, val loss drop 0.0007 to 0.0007\n",
      "fold 9\n",
      "[1] : train loss 7.873681, val loss drop 10000000.0000 to 0.1262\n",
      "[2] : train loss 0.162304, val loss drop 0.1262 to 0.0495\n",
      "[3] : train loss 0.047439, val loss drop 0.0495 to 0.0129\n",
      "[5] : train loss 0.012579, val loss drop 0.0129 to 0.0062\n",
      "[9] : train loss 0.006144, val loss drop 0.0062 to 0.0051\n",
      "[10] : train loss 0.004845, val loss drop 0.0051 to 0.0042\n",
      "[11] : train loss 0.004082, val loss drop 0.0042 to 0.0037\n",
      "[13] : train loss 0.003552, val loss drop 0.0037 to 0.0027\n",
      "[19] : train loss 0.002892, val loss drop 0.0027 to 0.0022\n",
      "[22] : train loss 0.002354, val loss drop 0.0022 to 0.0021\n",
      "[24] : train loss 0.002343, val loss drop 0.0021 to 0.0019\n",
      "[27] : train loss 0.002023, val loss drop 0.0019 to 0.0015\n",
      "[30] : train loss 0.001718, val loss drop 0.0015 to 0.0014\n",
      "[37] : train loss 0.001917, val loss drop 0.0014 to 0.0014\n",
      "[42] : train loss 0.001471, val loss drop 0.0014 to 0.0012\n",
      "[50] : train loss 0.001524, val loss drop 0.0012 to 0.0009\n",
      "[60] : train loss 0.000845, val loss drop 0.0009 to 0.0009\n",
      "[62] : train loss 0.001429, val loss drop 0.0009 to 0.0008\n",
      "[68] : train loss 0.000814, val loss drop 0.0008 to 0.0007\n",
      "[76] : train loss 0.000962, val loss drop 0.0007 to 0.0006\n",
      "[78] : train loss 0.000594, val loss drop 0.0006 to 0.0006\n",
      "[86] : train loss 0.000845, val loss drop 0.0006 to 0.0005\n",
      "[94] : train loss 0.000706, val loss drop 0.0005 to 0.0005\n",
      "fold 10\n",
      "[1] : train loss 24.934775, val loss drop 10000000.0000 to 0.3508\n",
      "[2] : train loss 0.225521, val loss drop 0.3508 to 0.2026\n",
      "[3] : train loss 0.099087, val loss drop 0.2026 to 0.0788\n",
      "[4] : train loss 0.051371, val loss drop 0.0788 to 0.0469\n",
      "[5] : train loss 0.027362, val loss drop 0.0469 to 0.0174\n",
      "[6] : train loss 0.017920, val loss drop 0.0174 to 0.0149\n",
      "[7] : train loss 0.013350, val loss drop 0.0149 to 0.0146\n",
      "[8] : train loss 0.010417, val loss drop 0.0146 to 0.0096\n",
      "[11] : train loss 0.012206, val loss drop 0.0096 to 0.0080\n",
      "[13] : train loss 0.006461, val loss drop 0.0080 to 0.0052\n",
      "[23] : train loss 0.009075, val loss drop 0.0052 to 0.0042\n",
      "[27] : train loss 0.005243, val loss drop 0.0042 to 0.0037\n",
      "[30] : train loss 0.003113, val loss drop 0.0037 to 0.0033\n",
      "[32] : train loss 0.002071, val loss drop 0.0033 to 0.0026\n",
      "[42] : train loss 0.002587, val loss drop 0.0026 to 0.0021\n",
      "[51] : train loss 0.002252, val loss drop 0.0021 to 0.0019\n",
      "[52] : train loss 0.001846, val loss drop 0.0019 to 0.0018\n",
      "[72] : train loss 0.003060, val loss drop 0.0018 to 0.0016\n",
      "[74] : train loss 0.005768, val loss drop 0.0016 to 0.0014\n",
      "[79] : train loss 0.001249, val loss drop 0.0014 to 0.0012\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 31.284349, val loss drop 10000000.0000 to 217.9117\n",
      "[2] : train loss 3.011064, val loss drop 217.9117 to 30.4565\n",
      "[3] : train loss 0.951060, val loss drop 30.4565 to 10.0790\n",
      "[4] : train loss 0.534269, val loss drop 10.0790 to 0.5190\n",
      "[5] : train loss 0.658035, val loss drop 0.5190 to 0.2627\n",
      "[8] : train loss 0.263172, val loss drop 0.2627 to 0.1336\n",
      "[11] : train loss 0.281868, val loss drop 0.1336 to 0.0965\n",
      "[12] : train loss 0.211297, val loss drop 0.0965 to 0.0751\n",
      "[18] : train loss 0.482838, val loss drop 0.0751 to 0.0751\n",
      "[20] : train loss 0.089957, val loss drop 0.0751 to 0.0482\n",
      "[32] : train loss 0.120608, val loss drop 0.0482 to 0.0366\n",
      "[48] : train loss 0.078441, val loss drop 0.0366 to 0.0319\n",
      "[49] : train loss 0.075229, val loss drop 0.0319 to 0.0263\n",
      "[92] : train loss 0.057640, val loss drop 0.0263 to 0.0225\n",
      "[94] : train loss 0.046554, val loss drop 0.0225 to 0.0173\n",
      "fold 2\n",
      "[1] : train loss 40.614749, val loss drop 10000000.0000 to 7.4561\n",
      "[3] : train loss 1.402432, val loss drop 7.4561 to 4.7755\n",
      "[4] : train loss 0.799737, val loss drop 4.7755 to 2.4080\n",
      "[5] : train loss 0.462715, val loss drop 2.4080 to 0.3744\n",
      "[6] : train loss 0.333931, val loss drop 0.3744 to 0.2230\n",
      "[9] : train loss 0.236540, val loss drop 0.2230 to 0.1827\n",
      "[12] : train loss 0.258682, val loss drop 0.1827 to 0.1549\n",
      "[15] : train loss 0.133703, val loss drop 0.1549 to 0.1532\n",
      "[16] : train loss 0.235857, val loss drop 0.1532 to 0.0781\n",
      "[25] : train loss 0.121786, val loss drop 0.0781 to 0.0749\n",
      "[26] : train loss 0.152413, val loss drop 0.0749 to 0.0383\n",
      "[35] : train loss 0.165979, val loss drop 0.0383 to 0.0329\n",
      "[36] : train loss 0.175793, val loss drop 0.0329 to 0.0278\n",
      "[44] : train loss 0.116201, val loss drop 0.0278 to 0.0274\n",
      "[65] : train loss 0.098093, val loss drop 0.0274 to 0.0263\n",
      "[94] : train loss 0.090150, val loss drop 0.0263 to 0.0213\n",
      "fold 3\n",
      "[1] : train loss 28.943898, val loss drop 10000000.0000 to 112.0329\n",
      "[2] : train loss 3.215816, val loss drop 112.0329 to 20.6566\n",
      "[3] : train loss 1.321475, val loss drop 20.6566 to 5.4564\n",
      "[4] : train loss 0.910841, val loss drop 5.4564 to 0.5438\n",
      "[6] : train loss 0.631638, val loss drop 0.5438 to 0.2113\n",
      "[7] : train loss 0.397560, val loss drop 0.2113 to 0.1959\n",
      "[8] : train loss 0.470781, val loss drop 0.1959 to 0.1505\n",
      "[9] : train loss 0.483646, val loss drop 0.1505 to 0.1360\n",
      "[10] : train loss 0.473431, val loss drop 0.1360 to 0.1044\n",
      "[11] : train loss 0.232161, val loss drop 0.1044 to 0.0973\n",
      "[12] : train loss 0.171519, val loss drop 0.0973 to 0.0846\n",
      "[14] : train loss 0.293028, val loss drop 0.0846 to 0.0632\n",
      "[18] : train loss 0.151318, val loss drop 0.0632 to 0.0527\n",
      "[19] : train loss 0.166557, val loss drop 0.0527 to 0.0519\n",
      "[32] : train loss 0.093533, val loss drop 0.0519 to 0.0491\n",
      "[44] : train loss 0.262096, val loss drop 0.0491 to 0.0247\n",
      "[50] : train loss 0.293942, val loss drop 0.0247 to 0.0239\n",
      "[51] : train loss 0.081774, val loss drop 0.0239 to 0.0211\n",
      "[55] : train loss 0.070488, val loss drop 0.0211 to 0.0211\n",
      "[85] : train loss 0.092047, val loss drop 0.0211 to 0.0183\n",
      "[90] : train loss 0.093543, val loss drop 0.0183 to 0.0099\n",
      "fold 4\n",
      "[1] : train loss 28.871781, val loss drop 10000000.0000 to 22.5086\n",
      "[2] : train loss 1.948237, val loss drop 22.5086 to 9.1778\n",
      "[4] : train loss 0.436364, val loss drop 9.1778 to 3.3291\n",
      "[5] : train loss 0.330391, val loss drop 3.3291 to 0.8241\n",
      "[6] : train loss 0.228974, val loss drop 0.8241 to 0.1377\n",
      "[7] : train loss 0.481578, val loss drop 0.1377 to 0.1179\n",
      "[13] : train loss 0.184929, val loss drop 0.1179 to 0.0988\n",
      "[14] : train loss 0.187145, val loss drop 0.0988 to 0.0605\n",
      "[26] : train loss 0.095865, val loss drop 0.0605 to 0.0314\n",
      "[42] : train loss 0.144358, val loss drop 0.0314 to 0.0310\n",
      "[43] : train loss 0.073969, val loss drop 0.0310 to 0.0251\n",
      "[48] : train loss 0.189845, val loss drop 0.0251 to 0.0185\n",
      "[65] : train loss 0.238031, val loss drop 0.0185 to 0.0149\n",
      "[82] : train loss 0.099676, val loss drop 0.0149 to 0.0105\n",
      "fold 5\n",
      "[1] : train loss 33.176124, val loss drop 10000000.0000 to 94.5358\n",
      "[2] : train loss 3.160175, val loss drop 94.5358 to 38.3470\n",
      "[3] : train loss 1.004681, val loss drop 38.3470 to 13.5275\n",
      "[4] : train loss 0.770237, val loss drop 13.5275 to 6.0769\n",
      "[5] : train loss 0.439946, val loss drop 6.0769 to 0.3558\n",
      "[6] : train loss 0.381222, val loss drop 0.3558 to 0.2293\n",
      "[9] : train loss 0.419433, val loss drop 0.2293 to 0.2237\n",
      "[15] : train loss 0.237380, val loss drop 0.2237 to 0.1810\n",
      "[19] : train loss 0.173071, val loss drop 0.1810 to 0.1331\n",
      "[21] : train loss 0.217448, val loss drop 0.1331 to 0.0943\n",
      "[28] : train loss 0.202241, val loss drop 0.0943 to 0.0805\n",
      "[33] : train loss 0.171755, val loss drop 0.0805 to 0.0669\n",
      "[35] : train loss 0.224667, val loss drop 0.0669 to 0.0289\n",
      "[40] : train loss 0.211045, val loss drop 0.0289 to 0.0258\n",
      "[64] : train loss 0.117075, val loss drop 0.0258 to 0.0179\n",
      "fold 6\n",
      "[1] : train loss 58.830308, val loss drop 10000000.0000 to 45.3061\n",
      "[2] : train loss 8.195004, val loss drop 45.3061 to 25.4425\n",
      "[3] : train loss 2.145320, val loss drop 25.4425 to 24.2823\n",
      "[4] : train loss 0.915199, val loss drop 24.2823 to 7.2980\n",
      "[5] : train loss 0.520065, val loss drop 7.2980 to 0.7082\n",
      "[6] : train loss 0.402518, val loss drop 0.7082 to 0.4149\n",
      "[8] : train loss 0.546391, val loss drop 0.4149 to 0.1292\n",
      "[14] : train loss 0.172227, val loss drop 0.1292 to 0.0775\n",
      "[17] : train loss 0.178778, val loss drop 0.0775 to 0.0500\n",
      "[22] : train loss 0.263562, val loss drop 0.0500 to 0.0371\n",
      "[46] : train loss 0.122244, val loss drop 0.0371 to 0.0187\n",
      "[81] : train loss 0.048603, val loss drop 0.0187 to 0.0184\n",
      "[85] : train loss 0.090599, val loss drop 0.0184 to 0.0125\n",
      "[86] : train loss 0.094583, val loss drop 0.0125 to 0.0071\n",
      "fold 7\n",
      "[1] : train loss 27.090916, val loss drop 10000000.0000 to 127.5786\n",
      "[2] : train loss 2.734631, val loss drop 127.5786 to 29.5609\n",
      "[3] : train loss 0.939111, val loss drop 29.5609 to 10.8567\n",
      "[4] : train loss 0.533001, val loss drop 10.8567 to 3.8751\n",
      "[5] : train loss 0.313771, val loss drop 3.8751 to 0.5317\n",
      "[6] : train loss 0.248696, val loss drop 0.5317 to 0.1463\n",
      "[8] : train loss 0.153572, val loss drop 0.1463 to 0.1038\n",
      "[9] : train loss 0.249213, val loss drop 0.1038 to 0.0989\n",
      "[12] : train loss 0.633855, val loss drop 0.0989 to 0.0931\n",
      "[18] : train loss 0.167075, val loss drop 0.0931 to 0.0525\n",
      "[29] : train loss 0.163161, val loss drop 0.0525 to 0.0321\n",
      "[54] : train loss 0.090997, val loss drop 0.0321 to 0.0195\n",
      "fold 8\n",
      "[1] : train loss 43.700164, val loss drop 10000000.0000 to 8.9572\n",
      "[4] : train loss 0.735265, val loss drop 8.9572 to 4.4881\n",
      "[5] : train loss 0.436489, val loss drop 4.4881 to 1.5684\n",
      "[6] : train loss 0.251886, val loss drop 1.5684 to 0.1980\n",
      "[8] : train loss 0.223881, val loss drop 0.1980 to 0.1661\n",
      "[9] : train loss 0.199241, val loss drop 0.1661 to 0.1312\n",
      "[10] : train loss 0.146568, val loss drop 0.1312 to 0.1058\n",
      "[15] : train loss 0.282583, val loss drop 0.1058 to 0.0614\n",
      "[26] : train loss 0.399519, val loss drop 0.0614 to 0.0382\n",
      "[40] : train loss 0.144841, val loss drop 0.0382 to 0.0373\n",
      "[43] : train loss 0.087735, val loss drop 0.0373 to 0.0181\n",
      "[46] : train loss 0.185289, val loss drop 0.0181 to 0.0162\n",
      "[92] : train loss 0.184209, val loss drop 0.0162 to 0.0125\n",
      "[96] : train loss 0.168414, val loss drop 0.0125 to 0.0072\n",
      "fold 9\n",
      "[1] : train loss 69.695871, val loss drop 10000000.0000 to 29.5434\n",
      "[2] : train loss 14.867110, val loss drop 29.5434 to 12.3238\n",
      "[4] : train loss 1.361029, val loss drop 12.3238 to 2.4738\n",
      "[5] : train loss 0.616862, val loss drop 2.4738 to 0.7747\n",
      "[6] : train loss 0.546416, val loss drop 0.7747 to 0.3214\n",
      "[7] : train loss 0.361007, val loss drop 0.3214 to 0.2868\n",
      "[8] : train loss 0.395221, val loss drop 0.2868 to 0.1797\n",
      "[20] : train loss 0.207929, val loss drop 0.1797 to 0.1349\n",
      "[21] : train loss 0.236746, val loss drop 0.1349 to 0.0652\n",
      "[32] : train loss 0.227875, val loss drop 0.0652 to 0.0471\n",
      "[48] : train loss 0.096866, val loss drop 0.0471 to 0.0253\n",
      "[59] : train loss 0.159965, val loss drop 0.0253 to 0.0169\n",
      "[73] : train loss 0.082958, val loss drop 0.0169 to 0.0144\n",
      "fold 10\n",
      "[1] : train loss 25.161762, val loss drop 10000000.0000 to 54.6630\n",
      "[2] : train loss 2.549283, val loss drop 54.6630 to 12.2284\n",
      "[3] : train loss 0.896588, val loss drop 12.2284 to 4.8849\n",
      "[4] : train loss 0.557266, val loss drop 4.8849 to 0.6834\n",
      "[5] : train loss 0.433706, val loss drop 0.6834 to 0.1904\n",
      "[6] : train loss 0.238899, val loss drop 0.1904 to 0.0874\n",
      "[11] : train loss 0.236452, val loss drop 0.0874 to 0.0829\n",
      "[12] : train loss 0.081365, val loss drop 0.0829 to 0.0828\n",
      "[14] : train loss 0.271181, val loss drop 0.0828 to 0.0630\n",
      "[18] : train loss 0.098298, val loss drop 0.0630 to 0.0385\n",
      "[19] : train loss 0.142752, val loss drop 0.0385 to 0.0291\n",
      "[25] : train loss 0.116260, val loss drop 0.0291 to 0.0190\n",
      "[91] : train loss 0.184727, val loss drop 0.0190 to 0.0182\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-264.400940</td>\n",
       "      <td>-46.865772</td>\n",
       "      <td>112.156418</td>\n",
       "      <td>0.463586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>312.851410</td>\n",
       "      <td>-288.306091</td>\n",
       "      <td>89.418922</td>\n",
       "      <td>0.469719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-243.856247</td>\n",
       "      <td>130.565353</td>\n",
       "      <td>29.935131</td>\n",
       "      <td>0.370846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>167.890717</td>\n",
       "      <td>277.492157</td>\n",
       "      <td>26.770218</td>\n",
       "      <td>0.412109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-159.517502</td>\n",
       "      <td>195.941254</td>\n",
       "      <td>130.575378</td>\n",
       "      <td>0.467343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -264.400940  -46.865772  112.156418  0.463586\n",
       "1  2801  312.851410 -288.306091   89.418922  0.469719\n",
       "2  2802 -243.856247  130.565353   29.935131  0.370846\n",
       "3  2803  167.890717  277.492157   26.770218  0.412109\n",
       "4  2804 -159.517502  195.941254  130.575378  0.467343"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, '{}.csv'.format(save_path.split('/')[-1])), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

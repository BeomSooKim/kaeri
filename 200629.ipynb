{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3x1, 4x1, 5x1 세 개의 filter size마다 cv 모델 생성후 ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)\n",
    "        #return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (5, 1))\n",
    "        fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        model = cnn_model(conv, fc)\n",
    "        #model = get_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.711469, val loss drop 10000000.0000 to 1.8523\n",
      "[2] : train loss 1.089728, val loss drop 1.8523 to 0.3741\n",
      "[3] : train loss 0.135929, val loss drop 0.3741 to 0.1119\n",
      "[4] : train loss 0.041429, val loss drop 0.1119 to 0.1090\n",
      "[5] : train loss 0.014850, val loss drop 0.1090 to 0.0154\n",
      "[7] : train loss 0.004752, val loss drop 0.0154 to 0.0050\n",
      "[8] : train loss 0.002916, val loss drop 0.0050 to 0.0032\n",
      "[10] : train loss 0.003183, val loss drop 0.0032 to 0.0020\n",
      "[12] : train loss 0.001593, val loss drop 0.0020 to 0.0019\n",
      "[13] : train loss 0.001594, val loss drop 0.0019 to 0.0016\n",
      "[17] : train loss 0.001161, val loss drop 0.0016 to 0.0013\n",
      "[18] : train loss 0.001306, val loss drop 0.0013 to 0.0011\n",
      "[21] : train loss 0.001433, val loss drop 0.0011 to 0.0011\n",
      "[25] : train loss 0.001472, val loss drop 0.0011 to 0.0008\n",
      "[26] : train loss 0.001528, val loss drop 0.0008 to 0.0008\n",
      "[29] : train loss 0.001347, val loss drop 0.0008 to 0.0007\n",
      "[38] : train loss 0.001104, val loss drop 0.0007 to 0.0005\n",
      "[74] : train loss 0.000900, val loss drop 0.0005 to 0.0005\n",
      "[80] : train loss 0.000809, val loss drop 0.0005 to 0.0004\n",
      "[81] : train loss 0.001132, val loss drop 0.0004 to 0.0004\n",
      "fold 2\n",
      "[1] : train loss 2.815461, val loss drop 10000000.0000 to 1.7953\n",
      "[2] : train loss 1.321251, val loss drop 1.7953 to 0.5728\n",
      "[3] : train loss 0.300512, val loss drop 0.5728 to 0.1159\n",
      "[6] : train loss 0.008176, val loss drop 0.1159 to 0.0077\n",
      "[8] : train loss 0.002982, val loss drop 0.0077 to 0.0043\n",
      "[9] : train loss 0.001927, val loss drop 0.0043 to 0.0030\n",
      "[10] : train loss 0.002223, val loss drop 0.0030 to 0.0027\n",
      "[11] : train loss 0.002263, val loss drop 0.0027 to 0.0023\n",
      "[12] : train loss 0.001538, val loss drop 0.0023 to 0.0014\n",
      "[18] : train loss 0.002716, val loss drop 0.0014 to 0.0014\n",
      "[20] : train loss 0.001168, val loss drop 0.0014 to 0.0010\n",
      "[22] : train loss 0.001083, val loss drop 0.0010 to 0.0006\n",
      "[25] : train loss 0.000997, val loss drop 0.0006 to 0.0005\n",
      "[28] : train loss 0.000693, val loss drop 0.0005 to 0.0005\n",
      "[41] : train loss 0.000871, val loss drop 0.0005 to 0.0005\n",
      "[44] : train loss 0.000744, val loss drop 0.0005 to 0.0003\n",
      "[48] : train loss 0.000411, val loss drop 0.0003 to 0.0003\n",
      "[77] : train loss 0.000474, val loss drop 0.0003 to 0.0003\n",
      "[94] : train loss 0.000464, val loss drop 0.0003 to 0.0002\n",
      "fold 3\n",
      "[1] : train loss 2.699570, val loss drop 10000000.0000 to 1.8307\n",
      "[2] : train loss 1.145413, val loss drop 1.8307 to 0.5773\n",
      "[3] : train loss 0.389268, val loss drop 0.5773 to 0.0951\n",
      "[4] : train loss 0.063753, val loss drop 0.0951 to 0.0360\n",
      "[6] : train loss 0.011597, val loss drop 0.0360 to 0.0192\n",
      "[7] : train loss 0.005732, val loss drop 0.0192 to 0.0098\n",
      "[8] : train loss 0.004337, val loss drop 0.0098 to 0.0046\n",
      "[9] : train loss 0.003669, val loss drop 0.0046 to 0.0032\n",
      "[13] : train loss 0.002334, val loss drop 0.0032 to 0.0025\n",
      "[15] : train loss 0.001392, val loss drop 0.0025 to 0.0016\n",
      "[16] : train loss 0.001270, val loss drop 0.0016 to 0.0012\n",
      "[19] : train loss 0.002594, val loss drop 0.0012 to 0.0009\n",
      "[37] : train loss 0.000909, val loss drop 0.0009 to 0.0009\n",
      "[41] : train loss 0.001036, val loss drop 0.0009 to 0.0007\n",
      "[45] : train loss 0.001427, val loss drop 0.0007 to 0.0005\n",
      "[47] : train loss 0.000962, val loss drop 0.0005 to 0.0005\n",
      "[60] : train loss 0.001424, val loss drop 0.0005 to 0.0004\n",
      "[76] : train loss 0.000553, val loss drop 0.0004 to 0.0004\n",
      "fold 4\n",
      "[1] : train loss 2.884789, val loss drop 10000000.0000 to 1.9468\n",
      "[2] : train loss 1.342731, val loss drop 1.9468 to 0.3831\n",
      "[3] : train loss 0.148447, val loss drop 0.3831 to 0.0774\n",
      "[6] : train loss 0.007015, val loss drop 0.0774 to 0.0170\n",
      "[8] : train loss 0.002978, val loss drop 0.0170 to 0.0023\n",
      "[10] : train loss 0.002044, val loss drop 0.0023 to 0.0019\n",
      "[11] : train loss 0.002158, val loss drop 0.0019 to 0.0018\n",
      "[16] : train loss 0.001061, val loss drop 0.0018 to 0.0011\n",
      "[24] : train loss 0.001791, val loss drop 0.0011 to 0.0010\n",
      "[30] : train loss 0.001106, val loss drop 0.0010 to 0.0007\n",
      "[33] : train loss 0.001475, val loss drop 0.0007 to 0.0006\n",
      "[60] : train loss 0.000503, val loss drop 0.0006 to 0.0005\n",
      "[76] : train loss 0.000514, val loss drop 0.0005 to 0.0003\n",
      "fold 5\n",
      "[1] : train loss 2.973636, val loss drop 10000000.0000 to 2.1147\n",
      "[2] : train loss 1.739345, val loss drop 2.1147 to 0.9574\n",
      "[3] : train loss 0.564155, val loss drop 0.9574 to 0.1941\n",
      "[4] : train loss 0.080254, val loss drop 0.1941 to 0.1197\n",
      "[6] : train loss 0.013910, val loss drop 0.1197 to 0.0211\n",
      "[7] : train loss 0.007826, val loss drop 0.0211 to 0.0062\n",
      "[8] : train loss 0.005612, val loss drop 0.0062 to 0.0060\n",
      "[9] : train loss 0.003650, val loss drop 0.0060 to 0.0040\n",
      "[10] : train loss 0.003916, val loss drop 0.0040 to 0.0026\n",
      "[13] : train loss 0.001492, val loss drop 0.0026 to 0.0025\n",
      "[14] : train loss 0.001511, val loss drop 0.0025 to 0.0020\n",
      "[15] : train loss 0.001685, val loss drop 0.0020 to 0.0015\n",
      "[21] : train loss 0.000958, val loss drop 0.0015 to 0.0013\n",
      "[22] : train loss 0.000861, val loss drop 0.0013 to 0.0013\n",
      "[26] : train loss 0.001679, val loss drop 0.0013 to 0.0010\n",
      "[32] : train loss 0.000813, val loss drop 0.0010 to 0.0009\n",
      "[36] : train loss 0.000862, val loss drop 0.0009 to 0.0008\n",
      "[48] : train loss 0.001022, val loss drop 0.0008 to 0.0008\n",
      "[66] : train loss 0.000753, val loss drop 0.0008 to 0.0005\n",
      "[81] : train loss 0.000550, val loss drop 0.0005 to 0.0004\n",
      "[96] : train loss 0.000741, val loss drop 0.0004 to 0.0003\n",
      "fold 6\n",
      "[1] : train loss 2.798595, val loss drop 10000000.0000 to 1.7535\n",
      "[2] : train loss 1.085675, val loss drop 1.7535 to 0.3475\n",
      "[3] : train loss 0.138037, val loss drop 0.3475 to 0.0882\n",
      "[5] : train loss 0.013021, val loss drop 0.0882 to 0.0786\n",
      "[6] : train loss 0.007779, val loss drop 0.0786 to 0.0090\n",
      "[7] : train loss 0.005036, val loss drop 0.0090 to 0.0057\n",
      "[9] : train loss 0.002109, val loss drop 0.0057 to 0.0035\n",
      "[10] : train loss 0.001702, val loss drop 0.0035 to 0.0023\n",
      "[11] : train loss 0.001616, val loss drop 0.0023 to 0.0022\n",
      "[12] : train loss 0.001191, val loss drop 0.0022 to 0.0019\n",
      "[13] : train loss 0.001138, val loss drop 0.0019 to 0.0016\n",
      "[14] : train loss 0.001158, val loss drop 0.0016 to 0.0014\n",
      "[17] : train loss 0.001626, val loss drop 0.0014 to 0.0008\n",
      "[27] : train loss 0.001194, val loss drop 0.0008 to 0.0007\n",
      "[39] : train loss 0.000653, val loss drop 0.0007 to 0.0005\n",
      "[45] : train loss 0.000652, val loss drop 0.0005 to 0.0004\n",
      "[83] : train loss 0.000401, val loss drop 0.0004 to 0.0003\n",
      "fold 7\n",
      "[1] : train loss 2.745308, val loss drop 10000000.0000 to 1.6433\n",
      "[2] : train loss 1.281786, val loss drop 1.6433 to 0.5640\n",
      "[3] : train loss 0.311436, val loss drop 0.5640 to 0.1255\n",
      "[4] : train loss 0.057442, val loss drop 0.1255 to 0.1072\n",
      "[5] : train loss 0.022418, val loss drop 0.1072 to 0.0801\n",
      "[6] : train loss 0.009409, val loss drop 0.0801 to 0.0128\n",
      "[7] : train loss 0.005524, val loss drop 0.0128 to 0.0078\n",
      "[9] : train loss 0.003045, val loss drop 0.0078 to 0.0024\n",
      "[11] : train loss 0.001676, val loss drop 0.0024 to 0.0019\n",
      "[12] : train loss 0.001555, val loss drop 0.0019 to 0.0012\n",
      "[24] : train loss 0.002111, val loss drop 0.0012 to 0.0011\n",
      "[46] : train loss 0.000889, val loss drop 0.0011 to 0.0005\n",
      "[55] : train loss 0.000676, val loss drop 0.0005 to 0.0005\n",
      "[60] : train loss 0.001278, val loss drop 0.0005 to 0.0005\n",
      "[68] : train loss 0.001040, val loss drop 0.0005 to 0.0003\n",
      "fold 8\n",
      "[1] : train loss 2.829694, val loss drop 10000000.0000 to 1.8485\n",
      "[2] : train loss 1.262629, val loss drop 1.8485 to 0.6894\n",
      "[3] : train loss 0.289222, val loss drop 0.6894 to 0.0768\n",
      "[5] : train loss 0.018464, val loss drop 0.0768 to 0.0740\n",
      "[6] : train loss 0.007347, val loss drop 0.0740 to 0.0181\n",
      "[7] : train loss 0.005190, val loss drop 0.0181 to 0.0051\n",
      "[8] : train loss 0.003642, val loss drop 0.0051 to 0.0034\n",
      "[12] : train loss 0.001560, val loss drop 0.0034 to 0.0011\n",
      "[26] : train loss 0.001768, val loss drop 0.0011 to 0.0010\n",
      "[29] : train loss 0.000990, val loss drop 0.0010 to 0.0007\n",
      "[31] : train loss 0.001329, val loss drop 0.0007 to 0.0006\n",
      "[41] : train loss 0.000723, val loss drop 0.0006 to 0.0005\n",
      "[63] : train loss 0.000733, val loss drop 0.0005 to 0.0004\n",
      "[64] : train loss 0.000413, val loss drop 0.0004 to 0.0004\n",
      "[95] : train loss 0.000489, val loss drop 0.0004 to 0.0003\n",
      "fold 9\n",
      "[1] : train loss 2.891436, val loss drop 10000000.0000 to 1.9715\n",
      "[2] : train loss 1.314080, val loss drop 1.9715 to 0.2896\n",
      "[3] : train loss 0.143799, val loss drop 0.2896 to 0.1008\n",
      "[5] : train loss 0.018479, val loss drop 0.1008 to 0.0861\n",
      "[6] : train loss 0.008658, val loss drop 0.0861 to 0.0058\n",
      "[10] : train loss 0.002227, val loss drop 0.0058 to 0.0023\n",
      "[12] : train loss 0.001698, val loss drop 0.0023 to 0.0019\n",
      "[15] : train loss 0.001541, val loss drop 0.0019 to 0.0012\n",
      "[20] : train loss 0.000884, val loss drop 0.0012 to 0.0007\n",
      "[26] : train loss 0.000722, val loss drop 0.0007 to 0.0007\n",
      "[39] : train loss 0.000687, val loss drop 0.0007 to 0.0006\n",
      "[50] : train loss 0.000651, val loss drop 0.0006 to 0.0004\n",
      "[87] : train loss 0.000655, val loss drop 0.0004 to 0.0004\n",
      "fold 10\n",
      "[1] : train loss 2.769107, val loss drop 10000000.0000 to 1.7286\n",
      "[2] : train loss 1.162510, val loss drop 1.7286 to 0.3872\n",
      "[3] : train loss 0.157949, val loss drop 0.3872 to 0.1078\n",
      "[4] : train loss 0.035701, val loss drop 0.1078 to 0.0651\n",
      "[5] : train loss 0.015068, val loss drop 0.0651 to 0.0255\n",
      "[6] : train loss 0.007456, val loss drop 0.0255 to 0.0151\n",
      "[7] : train loss 0.005607, val loss drop 0.0151 to 0.0094\n",
      "[8] : train loss 0.003082, val loss drop 0.0094 to 0.0042\n",
      "[10] : train loss 0.001814, val loss drop 0.0042 to 0.0038\n",
      "[11] : train loss 0.001849, val loss drop 0.0038 to 0.0025\n",
      "[14] : train loss 0.001188, val loss drop 0.0025 to 0.0015\n",
      "[19] : train loss 0.001728, val loss drop 0.0015 to 0.0013\n",
      "[23] : train loss 0.001775, val loss drop 0.0013 to 0.0009\n",
      "[25] : train loss 0.000611, val loss drop 0.0009 to 0.0008\n",
      "[36] : train loss 0.001411, val loss drop 0.0008 to 0.0008\n",
      "[48] : train loss 0.001007, val loss drop 0.0008 to 0.0007\n",
      "[49] : train loss 0.000897, val loss drop 0.0007 to 0.0007\n",
      "[53] : train loss 0.000595, val loss drop 0.0007 to 0.0007\n",
      "[58] : train loss 0.000808, val loss drop 0.0007 to 0.0005\n",
      "[66] : train loss 0.000914, val loss drop 0.0005 to 0.0004\n",
      "[87] : train loss 0.001042, val loss drop 0.0004 to 0.0004\n",
      "[94] : train loss 0.000753, val loss drop 0.0004 to 0.0003\n",
      "[98] : train loss 0.000656, val loss drop 0.0003 to 0.0003\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 7.419645, val loss drop 10000000.0000 to 0.8801\n",
      "[2] : train loss 0.081107, val loss drop 0.8801 to 0.0837\n",
      "[3] : train loss 0.033138, val loss drop 0.0837 to 0.0174\n",
      "[5] : train loss 0.013214, val loss drop 0.0174 to 0.0132\n",
      "[6] : train loss 0.010156, val loss drop 0.0132 to 0.0112\n",
      "[7] : train loss 0.009565, val loss drop 0.0112 to 0.0096\n",
      "[8] : train loss 0.007965, val loss drop 0.0096 to 0.0087\n",
      "[9] : train loss 0.006217, val loss drop 0.0087 to 0.0073\n",
      "[10] : train loss 0.005523, val loss drop 0.0073 to 0.0060\n",
      "[12] : train loss 0.004533, val loss drop 0.0060 to 0.0041\n",
      "[17] : train loss 0.003218, val loss drop 0.0041 to 0.0035\n",
      "[22] : train loss 0.003721, val loss drop 0.0035 to 0.0031\n",
      "[23] : train loss 0.003869, val loss drop 0.0031 to 0.0030\n",
      "[25] : train loss 0.002308, val loss drop 0.0030 to 0.0029\n",
      "[26] : train loss 0.002702, val loss drop 0.0029 to 0.0028\n",
      "[31] : train loss 0.001851, val loss drop 0.0028 to 0.0024\n",
      "[32] : train loss 0.001625, val loss drop 0.0024 to 0.0022\n",
      "[33] : train loss 0.001832, val loss drop 0.0022 to 0.0021\n",
      "[41] : train loss 0.002286, val loss drop 0.0021 to 0.0018\n",
      "[42] : train loss 0.001800, val loss drop 0.0018 to 0.0017\n",
      "[43] : train loss 0.001755, val loss drop 0.0017 to 0.0017\n",
      "[46] : train loss 0.001657, val loss drop 0.0017 to 0.0015\n",
      "[56] : train loss 0.001129, val loss drop 0.0015 to 0.0014\n",
      "[58] : train loss 0.002571, val loss drop 0.0014 to 0.0012\n",
      "[59] : train loss 0.001051, val loss drop 0.0012 to 0.0011\n",
      "[84] : train loss 0.001139, val loss drop 0.0011 to 0.0009\n",
      "[85] : train loss 0.000856, val loss drop 0.0009 to 0.0008\n",
      "fold 2\n",
      "[1] : train loss 5.689797, val loss drop 10000000.0000 to 0.4231\n",
      "[3] : train loss 0.123203, val loss drop 0.4231 to 0.1077\n",
      "[4] : train loss 0.041151, val loss drop 0.1077 to 0.0450\n",
      "[5] : train loss 0.023776, val loss drop 0.0450 to 0.0175\n",
      "[6] : train loss 0.013908, val loss drop 0.0175 to 0.0149\n",
      "[7] : train loss 0.013944, val loss drop 0.0149 to 0.0089\n",
      "[8] : train loss 0.008991, val loss drop 0.0089 to 0.0077\n",
      "[12] : train loss 0.009674, val loss drop 0.0077 to 0.0073\n",
      "[14] : train loss 0.005414, val loss drop 0.0073 to 0.0050\n",
      "[16] : train loss 0.007354, val loss drop 0.0050 to 0.0049\n",
      "[17] : train loss 0.007658, val loss drop 0.0049 to 0.0044\n",
      "[18] : train loss 0.003177, val loss drop 0.0044 to 0.0043\n",
      "[19] : train loss 0.002944, val loss drop 0.0043 to 0.0031\n",
      "[24] : train loss 0.003466, val loss drop 0.0031 to 0.0030\n",
      "[32] : train loss 0.004030, val loss drop 0.0030 to 0.0027\n",
      "[45] : train loss 0.003796, val loss drop 0.0027 to 0.0020\n",
      "[65] : train loss 0.002121, val loss drop 0.0020 to 0.0017\n",
      "[84] : train loss 0.003314, val loss drop 0.0017 to 0.0016\n",
      "[98] : train loss 0.002879, val loss drop 0.0016 to 0.0015\n",
      "fold 3\n",
      "[1] : train loss 11.672647, val loss drop 10000000.0000 to 0.7860\n",
      "[2] : train loss 0.250218, val loss drop 0.7860 to 0.1044\n",
      "[3] : train loss 0.083001, val loss drop 0.1044 to 0.0416\n",
      "[4] : train loss 0.037995, val loss drop 0.0416 to 0.0355\n",
      "[5] : train loss 0.024850, val loss drop 0.0355 to 0.0186\n",
      "[7] : train loss 0.014925, val loss drop 0.0186 to 0.0151\n",
      "[8] : train loss 0.010285, val loss drop 0.0151 to 0.0119\n",
      "[9] : train loss 0.007810, val loss drop 0.0119 to 0.0098\n",
      "[10] : train loss 0.007344, val loss drop 0.0098 to 0.0082\n",
      "[12] : train loss 0.006612, val loss drop 0.0082 to 0.0066\n",
      "[14] : train loss 0.005332, val loss drop 0.0066 to 0.0061\n",
      "[16] : train loss 0.005079, val loss drop 0.0061 to 0.0056\n",
      "[18] : train loss 0.004490, val loss drop 0.0056 to 0.0045\n",
      "[19] : train loss 0.003500, val loss drop 0.0045 to 0.0038\n",
      "[29] : train loss 0.003363, val loss drop 0.0038 to 0.0032\n",
      "[30] : train loss 0.002049, val loss drop 0.0032 to 0.0031\n",
      "[36] : train loss 0.002093, val loss drop 0.0031 to 0.0027\n",
      "[45] : train loss 0.005104, val loss drop 0.0027 to 0.0026\n",
      "[55] : train loss 0.002263, val loss drop 0.0026 to 0.0022\n",
      "[65] : train loss 0.003215, val loss drop 0.0022 to 0.0020\n",
      "[78] : train loss 0.001655, val loss drop 0.0020 to 0.0018\n",
      "[82] : train loss 0.003630, val loss drop 0.0018 to 0.0016\n",
      "[83] : train loss 0.001455, val loss drop 0.0016 to 0.0014\n",
      "fold 4\n",
      "[1] : train loss 10.363562, val loss drop 10000000.0000 to 1.0891\n",
      "[2] : train loss 0.099441, val loss drop 1.0891 to 0.0920\n",
      "[3] : train loss 0.044264, val loss drop 0.0920 to 0.0246\n",
      "[4] : train loss 0.023202, val loss drop 0.0246 to 0.0138\n",
      "[5] : train loss 0.017593, val loss drop 0.0138 to 0.0128\n",
      "[10] : train loss 0.010940, val loss drop 0.0128 to 0.0078\n",
      "[11] : train loss 0.011819, val loss drop 0.0078 to 0.0051\n",
      "[15] : train loss 0.008639, val loss drop 0.0051 to 0.0046\n",
      "[17] : train loss 0.004790, val loss drop 0.0046 to 0.0035\n",
      "[18] : train loss 0.003096, val loss drop 0.0035 to 0.0034\n",
      "[23] : train loss 0.002894, val loss drop 0.0034 to 0.0028\n",
      "[25] : train loss 0.007641, val loss drop 0.0028 to 0.0026\n",
      "[26] : train loss 0.005960, val loss drop 0.0026 to 0.0020\n",
      "[33] : train loss 0.003430, val loss drop 0.0020 to 0.0019\n",
      "[37] : train loss 0.001605, val loss drop 0.0019 to 0.0016\n",
      "[40] : train loss 0.002543, val loss drop 0.0016 to 0.0016\n",
      "[46] : train loss 0.000970, val loss drop 0.0016 to 0.0013\n",
      "[56] : train loss 0.004881, val loss drop 0.0013 to 0.0012\n",
      "[64] : train loss 0.001114, val loss drop 0.0012 to 0.0011\n",
      "[71] : train loss 0.001243, val loss drop 0.0011 to 0.0008\n",
      "[83] : train loss 0.000640, val loss drop 0.0008 to 0.0008\n",
      "[85] : train loss 0.001432, val loss drop 0.0008 to 0.0008\n",
      "[89] : train loss 0.001390, val loss drop 0.0008 to 0.0007\n",
      "[95] : train loss 0.003027, val loss drop 0.0007 to 0.0007\n",
      "fold 5\n",
      "[1] : train loss 13.802984, val loss drop 10000000.0000 to 0.6308\n",
      "[2] : train loss 0.085237, val loss drop 0.6308 to 0.0788\n",
      "[3] : train loss 0.037853, val loss drop 0.0788 to 0.0762\n",
      "[4] : train loss 0.023288, val loss drop 0.0762 to 0.0252\n",
      "[5] : train loss 0.020139, val loss drop 0.0252 to 0.0142\n",
      "[8] : train loss 0.015360, val loss drop 0.0142 to 0.0086\n",
      "[10] : train loss 0.010531, val loss drop 0.0086 to 0.0072\n",
      "[12] : train loss 0.007761, val loss drop 0.0072 to 0.0056\n",
      "[20] : train loss 0.008059, val loss drop 0.0056 to 0.0035\n",
      "[26] : train loss 0.003313, val loss drop 0.0035 to 0.0032\n",
      "[27] : train loss 0.002783, val loss drop 0.0032 to 0.0028\n",
      "[32] : train loss 0.003622, val loss drop 0.0028 to 0.0017\n",
      "[45] : train loss 0.001678, val loss drop 0.0017 to 0.0013\n",
      "[46] : train loss 0.001379, val loss drop 0.0013 to 0.0011\n",
      "[52] : train loss 0.001536, val loss drop 0.0011 to 0.0011\n",
      "[65] : train loss 0.001433, val loss drop 0.0011 to 0.0011\n",
      "[86] : train loss 0.003623, val loss drop 0.0011 to 0.0009\n",
      "[95] : train loss 0.000636, val loss drop 0.0009 to 0.0006\n",
      "fold 6\n",
      "[1] : train loss 57.632604, val loss drop 10000000.0000 to 0.4044\n",
      "[2] : train loss 0.174264, val loss drop 0.4044 to 0.0359\n",
      "[3] : train loss 0.045967, val loss drop 0.0359 to 0.0245\n",
      "[4] : train loss 0.025184, val loss drop 0.0245 to 0.0120\n",
      "[5] : train loss 0.015130, val loss drop 0.0120 to 0.0088\n",
      "[6] : train loss 0.011026, val loss drop 0.0088 to 0.0087\n",
      "[7] : train loss 0.009225, val loss drop 0.0087 to 0.0069\n",
      "[9] : train loss 0.006443, val loss drop 0.0069 to 0.0055\n",
      "[10] : train loss 0.005895, val loss drop 0.0055 to 0.0045\n",
      "[11] : train loss 0.005284, val loss drop 0.0045 to 0.0033\n",
      "[15] : train loss 0.003792, val loss drop 0.0033 to 0.0032\n",
      "[16] : train loss 0.003772, val loss drop 0.0032 to 0.0028\n",
      "[17] : train loss 0.003282, val loss drop 0.0028 to 0.0026\n",
      "[19] : train loss 0.003097, val loss drop 0.0026 to 0.0026\n",
      "[21] : train loss 0.002870, val loss drop 0.0026 to 0.0024\n",
      "[24] : train loss 0.002486, val loss drop 0.0024 to 0.0022\n",
      "[27] : train loss 0.002121, val loss drop 0.0022 to 0.0022\n",
      "[28] : train loss 0.001926, val loss drop 0.0022 to 0.0017\n",
      "[36] : train loss 0.001635, val loss drop 0.0017 to 0.0015\n",
      "[37] : train loss 0.001706, val loss drop 0.0015 to 0.0012\n",
      "[41] : train loss 0.001527, val loss drop 0.0012 to 0.0011\n",
      "[53] : train loss 0.001355, val loss drop 0.0011 to 0.0010\n",
      "[57] : train loss 0.001154, val loss drop 0.0010 to 0.0009\n",
      "[59] : train loss 0.001167, val loss drop 0.0009 to 0.0008\n",
      "[61] : train loss 0.001075, val loss drop 0.0008 to 0.0007\n",
      "[74] : train loss 0.001691, val loss drop 0.0007 to 0.0006\n",
      "[85] : train loss 0.000770, val loss drop 0.0006 to 0.0006\n",
      "[99] : train loss 0.000603, val loss drop 0.0006 to 0.0006\n",
      "fold 7\n",
      "[1] : train loss 34.169602, val loss drop 10000000.0000 to 14.0034\n",
      "[2] : train loss 0.406539, val loss drop 14.0034 to 0.9861\n",
      "[3] : train loss 0.130460, val loss drop 0.9861 to 0.0852\n",
      "[4] : train loss 0.060341, val loss drop 0.0852 to 0.0325\n",
      "[5] : train loss 0.036160, val loss drop 0.0325 to 0.0254\n",
      "[6] : train loss 0.019610, val loss drop 0.0254 to 0.0186\n",
      "[7] : train loss 0.017338, val loss drop 0.0186 to 0.0123\n",
      "[8] : train loss 0.016236, val loss drop 0.0123 to 0.0115\n",
      "[9] : train loss 0.013941, val loss drop 0.0115 to 0.0115\n",
      "[10] : train loss 0.010729, val loss drop 0.0115 to 0.0102\n",
      "[11] : train loss 0.010925, val loss drop 0.0102 to 0.0087\n",
      "[15] : train loss 0.007791, val loss drop 0.0087 to 0.0080\n",
      "[16] : train loss 0.006428, val loss drop 0.0080 to 0.0068\n",
      "[21] : train loss 0.006082, val loss drop 0.0068 to 0.0049\n",
      "[25] : train loss 0.004812, val loss drop 0.0049 to 0.0047\n",
      "[26] : train loss 0.004029, val loss drop 0.0047 to 0.0045\n",
      "[36] : train loss 0.004513, val loss drop 0.0045 to 0.0041\n",
      "[40] : train loss 0.004029, val loss drop 0.0041 to 0.0039\n",
      "[41] : train loss 0.003498, val loss drop 0.0039 to 0.0033\n",
      "[43] : train loss 0.001924, val loss drop 0.0033 to 0.0030\n",
      "[48] : train loss 0.003196, val loss drop 0.0030 to 0.0028\n",
      "[59] : train loss 0.002659, val loss drop 0.0028 to 0.0027\n",
      "[60] : train loss 0.002716, val loss drop 0.0027 to 0.0024\n",
      "[65] : train loss 0.002941, val loss drop 0.0024 to 0.0024\n",
      "[70] : train loss 0.004080, val loss drop 0.0024 to 0.0023\n",
      "[83] : train loss 0.001063, val loss drop 0.0023 to 0.0022\n",
      "[85] : train loss 0.002685, val loss drop 0.0022 to 0.0022\n",
      "[92] : train loss 0.001896, val loss drop 0.0022 to 0.0022\n",
      "[98] : train loss 0.001469, val loss drop 0.0022 to 0.0019\n",
      "[100] : train loss 0.001556, val loss drop 0.0019 to 0.0018\n",
      "fold 8\n",
      "[1] : train loss 6.569751, val loss drop 10000000.0000 to 0.3210\n",
      "[2] : train loss 0.246546, val loss drop 0.3210 to 0.1638\n",
      "[3] : train loss 0.089041, val loss drop 0.1638 to 0.1134\n",
      "[4] : train loss 0.036171, val loss drop 0.1134 to 0.0280\n",
      "[5] : train loss 0.018710, val loss drop 0.0280 to 0.0135\n",
      "[6] : train loss 0.012774, val loss drop 0.0135 to 0.0110\n",
      "[8] : train loss 0.006888, val loss drop 0.0110 to 0.0082\n",
      "[9] : train loss 0.007213, val loss drop 0.0082 to 0.0074\n",
      "[12] : train loss 0.006576, val loss drop 0.0074 to 0.0056\n",
      "[19] : train loss 0.008795, val loss drop 0.0056 to 0.0053\n",
      "[21] : train loss 0.005112, val loss drop 0.0053 to 0.0030\n",
      "[28] : train loss 0.008640, val loss drop 0.0030 to 0.0029\n",
      "[31] : train loss 0.004152, val loss drop 0.0029 to 0.0021\n",
      "[59] : train loss 0.002093, val loss drop 0.0021 to 0.0019\n",
      "[63] : train loss 0.004888, val loss drop 0.0019 to 0.0017\n",
      "[70] : train loss 0.007600, val loss drop 0.0017 to 0.0012\n",
      "[80] : train loss 0.001310, val loss drop 0.0012 to 0.0010\n",
      "fold 9\n",
      "[1] : train loss 3.491818, val loss drop 10000000.0000 to 0.9776\n",
      "[2] : train loss 0.069054, val loss drop 0.9776 to 0.1309\n",
      "[3] : train loss 0.030071, val loss drop 0.1309 to 0.0709\n",
      "[4] : train loss 0.014277, val loss drop 0.0709 to 0.0119\n",
      "[7] : train loss 0.009272, val loss drop 0.0119 to 0.0105\n",
      "[8] : train loss 0.008985, val loss drop 0.0105 to 0.0080\n",
      "[9] : train loss 0.006753, val loss drop 0.0080 to 0.0037\n",
      "[14] : train loss 0.002671, val loss drop 0.0037 to 0.0032\n",
      "[17] : train loss 0.001811, val loss drop 0.0032 to 0.0026\n",
      "[20] : train loss 0.005824, val loss drop 0.0026 to 0.0020\n",
      "[23] : train loss 0.001412, val loss drop 0.0020 to 0.0017\n",
      "[28] : train loss 0.001372, val loss drop 0.0017 to 0.0016\n",
      "[31] : train loss 0.001528, val loss drop 0.0016 to 0.0013\n",
      "[47] : train loss 0.001892, val loss drop 0.0013 to 0.0010\n",
      "[53] : train loss 0.001084, val loss drop 0.0010 to 0.0010\n",
      "[54] : train loss 0.001086, val loss drop 0.0010 to 0.0007\n",
      "[77] : train loss 0.000887, val loss drop 0.0007 to 0.0005\n",
      "fold 10\n",
      "[1] : train loss 6.158185, val loss drop 10000000.0000 to 1.4417\n",
      "[2] : train loss 0.279236, val loss drop 1.4417 to 0.2701\n",
      "[3] : train loss 0.106931, val loss drop 0.2701 to 0.1770\n",
      "[4] : train loss 0.045270, val loss drop 0.1770 to 0.0485\n",
      "[5] : train loss 0.023952, val loss drop 0.0485 to 0.0151\n",
      "[7] : train loss 0.014608, val loss drop 0.0151 to 0.0125\n",
      "[9] : train loss 0.007226, val loss drop 0.0125 to 0.0097\n",
      "[12] : train loss 0.007464, val loss drop 0.0097 to 0.0075\n",
      "[15] : train loss 0.006269, val loss drop 0.0075 to 0.0072\n",
      "[16] : train loss 0.008593, val loss drop 0.0072 to 0.0058\n",
      "[18] : train loss 0.010958, val loss drop 0.0058 to 0.0049\n",
      "[19] : train loss 0.004477, val loss drop 0.0049 to 0.0034\n",
      "[30] : train loss 0.007095, val loss drop 0.0034 to 0.0034\n",
      "[32] : train loss 0.002610, val loss drop 0.0034 to 0.0030\n",
      "[38] : train loss 0.004192, val loss drop 0.0030 to 0.0023\n",
      "[41] : train loss 0.004537, val loss drop 0.0023 to 0.0021\n",
      "[44] : train loss 0.003439, val loss drop 0.0021 to 0.0018\n",
      "[53] : train loss 0.003572, val loss drop 0.0018 to 0.0018\n",
      "[73] : train loss 0.001557, val loss drop 0.0018 to 0.0012\n",
      "[80] : train loss 0.001131, val loss drop 0.0012 to 0.0010\n",
      "[85] : train loss 0.002818, val loss drop 0.0010 to 0.0009\n",
      "[99] : train loss 0.002161, val loss drop 0.0009 to 0.0007\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 21.977800, val loss drop 10000000.0000 to 35.3634\n",
      "[2] : train loss 1.718683, val loss drop 35.3634 to 32.9376\n",
      "[3] : train loss 0.706800, val loss drop 32.9376 to 4.3029\n",
      "[4] : train loss 0.497373, val loss drop 4.3029 to 0.7112\n",
      "[6] : train loss 0.343263, val loss drop 0.7112 to 0.1766\n",
      "[7] : train loss 0.225966, val loss drop 0.1766 to 0.1490\n",
      "[9] : train loss 0.265143, val loss drop 0.1490 to 0.1426\n",
      "[17] : train loss 0.172686, val loss drop 0.1426 to 0.1253\n",
      "[18] : train loss 0.103372, val loss drop 0.1253 to 0.0865\n",
      "[21] : train loss 0.186345, val loss drop 0.0865 to 0.0417\n",
      "[60] : train loss 0.089943, val loss drop 0.0417 to 0.0237\n",
      "[62] : train loss 0.070882, val loss drop 0.0237 to 0.0196\n",
      "[68] : train loss 0.080564, val loss drop 0.0196 to 0.0147\n",
      "fold 2\n",
      "[1] : train loss 32.880714, val loss drop 10000000.0000 to 12.9777\n",
      "[3] : train loss 1.189457, val loss drop 12.9777 to 6.0566\n",
      "[4] : train loss 0.791524, val loss drop 6.0566 to 1.3414\n",
      "[5] : train loss 0.452838, val loss drop 1.3414 to 0.3856\n",
      "[6] : train loss 0.370952, val loss drop 0.3856 to 0.3251\n",
      "[8] : train loss 0.386509, val loss drop 0.3251 to 0.1977\n",
      "[9] : train loss 0.179611, val loss drop 0.1977 to 0.0918\n",
      "[12] : train loss 0.134205, val loss drop 0.0918 to 0.0789\n",
      "[13] : train loss 0.125369, val loss drop 0.0789 to 0.0505\n",
      "[16] : train loss 0.102168, val loss drop 0.0505 to 0.0376\n",
      "[19] : train loss 0.220841, val loss drop 0.0376 to 0.0307\n",
      "[22] : train loss 0.168776, val loss drop 0.0307 to 0.0295\n",
      "[60] : train loss 0.078609, val loss drop 0.0295 to 0.0209\n",
      "[65] : train loss 0.061250, val loss drop 0.0209 to 0.0156\n",
      "[91] : train loss 0.112362, val loss drop 0.0156 to 0.0122\n",
      "fold 3\n",
      "[1] : train loss 75.258141, val loss drop 10000000.0000 to 35.0446\n",
      "[2] : train loss 15.565440, val loss drop 35.0446 to 15.2716\n",
      "[5] : train loss 0.806848, val loss drop 15.2716 to 1.2834\n",
      "[6] : train loss 0.560403, val loss drop 1.2834 to 0.7029\n",
      "[7] : train loss 0.476883, val loss drop 0.7029 to 0.3075\n",
      "[8] : train loss 0.398489, val loss drop 0.3075 to 0.2488\n",
      "[10] : train loss 0.260025, val loss drop 0.2488 to 0.1508\n",
      "[17] : train loss 0.195721, val loss drop 0.1508 to 0.1241\n",
      "[18] : train loss 0.146275, val loss drop 0.1241 to 0.0909\n",
      "[21] : train loss 0.249481, val loss drop 0.0909 to 0.0796\n",
      "[28] : train loss 0.113663, val loss drop 0.0796 to 0.0451\n",
      "[34] : train loss 0.098523, val loss drop 0.0451 to 0.0335\n",
      "[36] : train loss 0.105363, val loss drop 0.0335 to 0.0290\n",
      "[64] : train loss 0.163162, val loss drop 0.0290 to 0.0281\n",
      "[85] : train loss 0.071907, val loss drop 0.0281 to 0.0235\n",
      "[95] : train loss 0.095914, val loss drop 0.0235 to 0.0144\n",
      "[100] : train loss 0.160649, val loss drop 0.0144 to 0.0142\n",
      "fold 4\n",
      "[1] : train loss 48.508835, val loss drop 10000000.0000 to 13.0991\n",
      "[4] : train loss 1.027016, val loss drop 13.0991 to 1.5019\n",
      "[5] : train loss 0.576061, val loss drop 1.5019 to 0.4948\n",
      "[6] : train loss 0.385202, val loss drop 0.4948 to 0.2369\n",
      "[7] : train loss 0.330427, val loss drop 0.2369 to 0.1636\n",
      "[8] : train loss 0.267344, val loss drop 0.1636 to 0.1463\n",
      "[18] : train loss 0.278361, val loss drop 0.1463 to 0.0852\n",
      "[21] : train loss 0.388031, val loss drop 0.0852 to 0.0689\n",
      "[22] : train loss 0.151060, val loss drop 0.0689 to 0.0357\n",
      "[40] : train loss 0.101154, val loss drop 0.0357 to 0.0133\n",
      "[87] : train loss 0.076731, val loss drop 0.0133 to 0.0109\n",
      "fold 5\n",
      "[1] : train loss 41.563445, val loss drop 10000000.0000 to 14.2626\n",
      "[4] : train loss 0.599972, val loss drop 14.2626 to 6.5332\n",
      "[5] : train loss 0.380376, val loss drop 6.5332 to 0.8244\n",
      "[6] : train loss 0.282798, val loss drop 0.8244 to 0.3283\n",
      "[7] : train loss 0.297925, val loss drop 0.3283 to 0.3216\n",
      "[13] : train loss 0.211698, val loss drop 0.3216 to 0.3148\n",
      "[14] : train loss 0.292700, val loss drop 0.3148 to 0.1596\n",
      "[15] : train loss 0.146501, val loss drop 0.1596 to 0.1104\n",
      "[16] : train loss 0.193084, val loss drop 0.1104 to 0.0751\n",
      "[18] : train loss 0.238606, val loss drop 0.0751 to 0.0628\n",
      "[24] : train loss 0.177486, val loss drop 0.0628 to 0.0333\n",
      "[33] : train loss 0.369620, val loss drop 0.0333 to 0.0281\n",
      "[41] : train loss 0.246242, val loss drop 0.0281 to 0.0182\n",
      "[59] : train loss 0.149085, val loss drop 0.0182 to 0.0178\n",
      "[89] : train loss 0.114794, val loss drop 0.0178 to 0.0178\n",
      "[92] : train loss 0.089557, val loss drop 0.0178 to 0.0167\n",
      "[97] : train loss 0.082998, val loss drop 0.0167 to 0.0140\n",
      "fold 6\n",
      "[1] : train loss 36.536044, val loss drop 10000000.0000 to 88.6809\n",
      "[2] : train loss 4.279909, val loss drop 88.6809 to 9.3253\n",
      "[3] : train loss 1.507904, val loss drop 9.3253 to 2.9870\n",
      "[5] : train loss 0.632173, val loss drop 2.9870 to 1.4299\n",
      "[6] : train loss 0.476695, val loss drop 1.4299 to 0.2811\n",
      "[8] : train loss 0.245749, val loss drop 0.2811 to 0.1611\n",
      "[16] : train loss 0.152475, val loss drop 0.1611 to 0.0970\n",
      "[20] : train loss 0.125793, val loss drop 0.0970 to 0.0598\n",
      "[27] : train loss 0.100263, val loss drop 0.0598 to 0.0432\n",
      "[40] : train loss 0.134113, val loss drop 0.0432 to 0.0259\n",
      "[69] : train loss 0.058019, val loss drop 0.0259 to 0.0219\n",
      "[86] : train loss 0.084151, val loss drop 0.0219 to 0.0202\n",
      "fold 7\n",
      "[1] : train loss 41.155616, val loss drop 10000000.0000 to 11.3121\n",
      "[4] : train loss 0.675568, val loss drop 11.3121 to 5.2117\n",
      "[5] : train loss 0.540155, val loss drop 5.2117 to 1.7834\n",
      "[6] : train loss 0.433670, val loss drop 1.7834 to 0.3513\n",
      "[7] : train loss 0.333076, val loss drop 0.3513 to 0.2979\n",
      "[8] : train loss 0.346243, val loss drop 0.2979 to 0.2423\n",
      "[12] : train loss 0.240963, val loss drop 0.2423 to 0.2377\n",
      "[17] : train loss 0.308961, val loss drop 0.2377 to 0.1193\n",
      "[23] : train loss 0.193719, val loss drop 0.1193 to 0.0991\n",
      "[27] : train loss 0.187139, val loss drop 0.0991 to 0.0705\n",
      "[33] : train loss 0.215997, val loss drop 0.0705 to 0.0390\n",
      "[36] : train loss 0.215528, val loss drop 0.0390 to 0.0195\n",
      "[52] : train loss 0.105511, val loss drop 0.0195 to 0.0177\n",
      "[68] : train loss 0.078767, val loss drop 0.0177 to 0.0148\n",
      "[96] : train loss 0.090745, val loss drop 0.0148 to 0.0074\n",
      "fold 8\n",
      "[1] : train loss 25.863940, val loss drop 10000000.0000 to 69.9255\n",
      "[2] : train loss 2.092517, val loss drop 69.9255 to 9.1316\n",
      "[3] : train loss 1.130971, val loss drop 9.1316 to 2.1364\n",
      "[4] : train loss 0.612841, val loss drop 2.1364 to 0.7406\n",
      "[5] : train loss 0.399728, val loss drop 0.7406 to 0.2638\n",
      "[6] : train loss 0.454324, val loss drop 0.2638 to 0.2249\n",
      "[8] : train loss 0.488794, val loss drop 0.2249 to 0.1888\n",
      "[11] : train loss 0.460784, val loss drop 0.1888 to 0.1128\n",
      "[16] : train loss 0.300863, val loss drop 0.1128 to 0.0927\n",
      "[20] : train loss 0.158735, val loss drop 0.0927 to 0.0576\n",
      "[26] : train loss 0.191154, val loss drop 0.0576 to 0.0292\n",
      "[37] : train loss 0.101130, val loss drop 0.0292 to 0.0277\n",
      "[58] : train loss 0.101398, val loss drop 0.0277 to 0.0268\n",
      "[62] : train loss 0.196854, val loss drop 0.0268 to 0.0261\n",
      "[67] : train loss 0.180710, val loss drop 0.0261 to 0.0222\n",
      "[92] : train loss 0.101567, val loss drop 0.0222 to 0.0142\n",
      "fold 9\n",
      "[1] : train loss 31.195380, val loss drop 10000000.0000 to 88.0609\n",
      "[2] : train loss 2.511595, val loss drop 88.0609 to 37.2489\n",
      "[3] : train loss 0.952967, val loss drop 37.2489 to 7.4944\n",
      "[4] : train loss 0.471562, val loss drop 7.4944 to 2.8883\n",
      "[5] : train loss 0.380258, val loss drop 2.8883 to 0.2060\n",
      "[6] : train loss 0.742375, val loss drop 0.2060 to 0.1283\n",
      "[7] : train loss 0.345753, val loss drop 0.1283 to 0.0905\n",
      "[8] : train loss 0.302282, val loss drop 0.0905 to 0.0793\n",
      "[17] : train loss 0.125141, val loss drop 0.0793 to 0.0320\n",
      "[22] : train loss 0.206367, val loss drop 0.0320 to 0.0269\n",
      "[31] : train loss 0.229455, val loss drop 0.0269 to 0.0149\n",
      "[52] : train loss 0.127555, val loss drop 0.0149 to 0.0147\n",
      "[72] : train loss 0.186749, val loss drop 0.0147 to 0.0093\n",
      "[81] : train loss 0.072801, val loss drop 0.0093 to 0.0073\n",
      "fold 10\n",
      "[1] : train loss 61.784230, val loss drop 10000000.0000 to 12.6607\n",
      "[2] : train loss 8.514555, val loss drop 12.6607 to 7.2885\n",
      "[4] : train loss 1.082445, val loss drop 7.2885 to 3.5780\n",
      "[5] : train loss 0.722926, val loss drop 3.5780 to 0.6095\n",
      "[6] : train loss 0.449514, val loss drop 0.6095 to 0.5595\n",
      "[7] : train loss 0.321263, val loss drop 0.5595 to 0.1778\n",
      "[14] : train loss 0.301353, val loss drop 0.1778 to 0.1156\n",
      "[18] : train loss 0.306256, val loss drop 0.1156 to 0.0895\n",
      "[21] : train loss 0.273280, val loss drop 0.0895 to 0.0301\n",
      "[32] : train loss 0.198163, val loss drop 0.0301 to 0.0289\n",
      "[39] : train loss 0.126897, val loss drop 0.0289 to 0.0244\n",
      "[58] : train loss 0.090437, val loss drop 0.0244 to 0.0205\n",
      "[65] : train loss 0.095170, val loss drop 0.0205 to 0.0198\n",
      "[78] : train loss 0.124728, val loss drop 0.0198 to 0.0110\n",
      "[98] : train loss 0.080692, val loss drop 0.0110 to 0.0107\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-262.867859</td>\n",
       "      <td>-48.068932</td>\n",
       "      <td>112.388695</td>\n",
       "      <td>0.450539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>317.558258</td>\n",
       "      <td>-290.576965</td>\n",
       "      <td>91.111610</td>\n",
       "      <td>0.462198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-241.859161</td>\n",
       "      <td>130.613403</td>\n",
       "      <td>29.760815</td>\n",
       "      <td>0.398097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>170.298904</td>\n",
       "      <td>282.266907</td>\n",
       "      <td>27.049686</td>\n",
       "      <td>0.410139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-157.874191</td>\n",
       "      <td>196.382736</td>\n",
       "      <td>130.103378</td>\n",
       "      <td>0.446006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -262.867859  -48.068932  112.388695  0.450539\n",
       "1  2801  317.558258 -290.576965   91.111610  0.462198\n",
       "2  2802 -241.859161  130.613403   29.760815  0.398097\n",
       "3  2803  170.298904  282.266907   27.049686  0.410139\n",
       "4  2804 -157.874191  196.382736  130.103378  0.446006"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, 'submit.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6x1, 7x1, 8x1 세 개의 filter size마다 cv 모델 생성후 ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)\n",
    "        #return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6x1 : 256* 6 * 4\n",
    "- 7x1 : 256* 5 * 4\n",
    "- 8x1 : 256* 4 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        conv = conv_block([32, 64, 128, 256, 512], [1, 375, n_features], (6, 1))\n",
    "        fc = classifier([128, 64, 32, 16], input_size = 512*6*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        model = cnn_model(conv, fc)\n",
    "        #model = get_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.066870, val loss drop 10000000.0000 to 0.5910\n",
      "[3] : train loss 0.084366, val loss drop 0.5910 to 0.1397\n",
      "[4] : train loss 0.030894, val loss drop 0.1397 to 0.0450\n",
      "[5] : train loss 0.013724, val loss drop 0.0450 to 0.0168\n",
      "[6] : train loss 0.006933, val loss drop 0.0168 to 0.0098\n",
      "[7] : train loss 0.004332, val loss drop 0.0098 to 0.0055\n",
      "[9] : train loss 0.002308, val loss drop 0.0055 to 0.0048\n",
      "[10] : train loss 0.001696, val loss drop 0.0048 to 0.0023\n",
      "[16] : train loss 0.001262, val loss drop 0.0023 to 0.0015\n",
      "[20] : train loss 0.001133, val loss drop 0.0015 to 0.0013\n",
      "[21] : train loss 0.000844, val loss drop 0.0013 to 0.0011\n",
      "[23] : train loss 0.001119, val loss drop 0.0011 to 0.0009\n",
      "[34] : train loss 0.001034, val loss drop 0.0009 to 0.0008\n",
      "[37] : train loss 0.000647, val loss drop 0.0008 to 0.0007\n",
      "[46] : train loss 0.000657, val loss drop 0.0007 to 0.0007\n",
      "[47] : train loss 0.000428, val loss drop 0.0007 to 0.0006\n",
      "[75] : train loss 0.000632, val loss drop 0.0006 to 0.0005\n",
      "fold 2\n",
      "[1] : train loss 2.427596, val loss drop 10000000.0000 to 1.0867\n",
      "[2] : train loss 0.481413, val loss drop 1.0867 to 0.2808\n",
      "[3] : train loss 0.085177, val loss drop 0.2808 to 0.2165\n",
      "[4] : train loss 0.028260, val loss drop 0.2165 to 0.0742\n",
      "[5] : train loss 0.011941, val loss drop 0.0742 to 0.0305\n",
      "[6] : train loss 0.006681, val loss drop 0.0305 to 0.0124\n",
      "[7] : train loss 0.004313, val loss drop 0.0124 to 0.0047\n",
      "[9] : train loss 0.004203, val loss drop 0.0047 to 0.0041\n",
      "[12] : train loss 0.001762, val loss drop 0.0041 to 0.0028\n",
      "[17] : train loss 0.001686, val loss drop 0.0028 to 0.0021\n",
      "[18] : train loss 0.001203, val loss drop 0.0021 to 0.0016\n",
      "[35] : train loss 0.001027, val loss drop 0.0016 to 0.0010\n",
      "[37] : train loss 0.001259, val loss drop 0.0010 to 0.0008\n",
      "[47] : train loss 0.000567, val loss drop 0.0008 to 0.0005\n",
      "[67] : train loss 0.000502, val loss drop 0.0005 to 0.0005\n",
      "[81] : train loss 0.000809, val loss drop 0.0005 to 0.0004\n",
      "[83] : train loss 0.000382, val loss drop 0.0004 to 0.0004\n",
      "fold 3\n",
      "[1] : train loss 2.252133, val loss drop 10000000.0000 to 1.2187\n",
      "[2] : train loss 0.682732, val loss drop 1.2187 to 0.3628\n",
      "[3] : train loss 0.119651, val loss drop 0.3628 to 0.0800\n",
      "[5] : train loss 0.014291, val loss drop 0.0800 to 0.0437\n",
      "[6] : train loss 0.009516, val loss drop 0.0437 to 0.0206\n",
      "[7] : train loss 0.005110, val loss drop 0.0206 to 0.0040\n",
      "[8] : train loss 0.003134, val loss drop 0.0040 to 0.0037\n",
      "[9] : train loss 0.002182, val loss drop 0.0037 to 0.0029\n",
      "[10] : train loss 0.002735, val loss drop 0.0029 to 0.0026\n",
      "[13] : train loss 0.001838, val loss drop 0.0026 to 0.0015\n",
      "[14] : train loss 0.001286, val loss drop 0.0015 to 0.0014\n",
      "[20] : train loss 0.001757, val loss drop 0.0014 to 0.0013\n",
      "[22] : train loss 0.001268, val loss drop 0.0013 to 0.0010\n",
      "[23] : train loss 0.001020, val loss drop 0.0010 to 0.0008\n",
      "[28] : train loss 0.001243, val loss drop 0.0008 to 0.0006\n",
      "[51] : train loss 0.002018, val loss drop 0.0006 to 0.0006\n",
      "[82] : train loss 0.000352, val loss drop 0.0006 to 0.0003\n",
      "fold 4\n",
      "[1] : train loss 2.058376, val loss drop 10000000.0000 to 0.5469\n",
      "[3] : train loss 0.075801, val loss drop 0.5469 to 0.1561\n",
      "[4] : train loss 0.024097, val loss drop 0.1561 to 0.0515\n",
      "[5] : train loss 0.010466, val loss drop 0.0515 to 0.0147\n",
      "[6] : train loss 0.005408, val loss drop 0.0147 to 0.0055\n",
      "[9] : train loss 0.002170, val loss drop 0.0055 to 0.0053\n",
      "[10] : train loss 0.002573, val loss drop 0.0053 to 0.0043\n",
      "[11] : train loss 0.002148, val loss drop 0.0043 to 0.0015\n",
      "[18] : train loss 0.001288, val loss drop 0.0015 to 0.0015\n",
      "[21] : train loss 0.001665, val loss drop 0.0015 to 0.0014\n",
      "[32] : train loss 0.000898, val loss drop 0.0014 to 0.0012\n",
      "[36] : train loss 0.000723, val loss drop 0.0012 to 0.0010\n",
      "[49] : train loss 0.000553, val loss drop 0.0010 to 0.0006\n",
      "[52] : train loss 0.000377, val loss drop 0.0006 to 0.0004\n",
      "[55] : train loss 0.000786, val loss drop 0.0004 to 0.0004\n",
      "fold 5\n",
      "[1] : train loss 2.469155, val loss drop 10000000.0000 to 1.0702\n",
      "[2] : train loss 0.440567, val loss drop 1.0702 to 0.4124\n",
      "[3] : train loss 0.082884, val loss drop 0.4124 to 0.1722\n",
      "[5] : train loss 0.010888, val loss drop 0.1722 to 0.0326\n",
      "[6] : train loss 0.005886, val loss drop 0.0326 to 0.0090\n",
      "[7] : train loss 0.003921, val loss drop 0.0090 to 0.0029\n",
      "[8] : train loss 0.002054, val loss drop 0.0029 to 0.0021\n",
      "[9] : train loss 0.001788, val loss drop 0.0021 to 0.0021\n",
      "[10] : train loss 0.001761, val loss drop 0.0021 to 0.0013\n",
      "[13] : train loss 0.001342, val loss drop 0.0013 to 0.0013\n",
      "[16] : train loss 0.001430, val loss drop 0.0013 to 0.0007\n",
      "[38] : train loss 0.000309, val loss drop 0.0007 to 0.0004\n",
      "[95] : train loss 0.000355, val loss drop 0.0004 to 0.0002\n",
      "fold 6\n",
      "[1] : train loss 2.296119, val loss drop 10000000.0000 to 1.1367\n",
      "[2] : train loss 0.606877, val loss drop 1.1367 to 0.3629\n",
      "[3] : train loss 0.123522, val loss drop 0.3629 to 0.1101\n",
      "[4] : train loss 0.040410, val loss drop 0.1101 to 0.0597\n",
      "[5] : train loss 0.016974, val loss drop 0.0597 to 0.0338\n",
      "[6] : train loss 0.009377, val loss drop 0.0338 to 0.0119\n",
      "[7] : train loss 0.005032, val loss drop 0.0119 to 0.0076\n",
      "[8] : train loss 0.003279, val loss drop 0.0076 to 0.0046\n",
      "[11] : train loss 0.002429, val loss drop 0.0046 to 0.0029\n",
      "[12] : train loss 0.001895, val loss drop 0.0029 to 0.0026\n",
      "[16] : train loss 0.001636, val loss drop 0.0026 to 0.0018\n",
      "[17] : train loss 0.001479, val loss drop 0.0018 to 0.0018\n",
      "[27] : train loss 0.001032, val loss drop 0.0018 to 0.0013\n",
      "[36] : train loss 0.000859, val loss drop 0.0013 to 0.0012\n",
      "[49] : train loss 0.001285, val loss drop 0.0012 to 0.0011\n",
      "[53] : train loss 0.001817, val loss drop 0.0011 to 0.0009\n",
      "[67] : train loss 0.000711, val loss drop 0.0009 to 0.0005\n",
      "[79] : train loss 0.000614, val loss drop 0.0005 to 0.0005\n",
      "[81] : train loss 0.000858, val loss drop 0.0005 to 0.0005\n",
      "[82] : train loss 0.000240, val loss drop 0.0005 to 0.0003\n",
      "fold 7\n",
      "[1] : train loss 2.055000, val loss drop 10000000.0000 to 0.5320\n",
      "[3] : train loss 0.056614, val loss drop 0.5320 to 0.1151\n",
      "[4] : train loss 0.026206, val loss drop 0.1151 to 0.0468\n",
      "[5] : train loss 0.016183, val loss drop 0.0468 to 0.0061\n",
      "[6] : train loss 0.008160, val loss drop 0.0061 to 0.0059\n",
      "[7] : train loss 0.006329, val loss drop 0.0059 to 0.0044\n",
      "[8] : train loss 0.003251, val loss drop 0.0044 to 0.0029\n",
      "[9] : train loss 0.002662, val loss drop 0.0029 to 0.0019\n",
      "[16] : train loss 0.001275, val loss drop 0.0019 to 0.0014\n",
      "[19] : train loss 0.003256, val loss drop 0.0014 to 0.0013\n",
      "[22] : train loss 0.001136, val loss drop 0.0013 to 0.0010\n",
      "[41] : train loss 0.000643, val loss drop 0.0010 to 0.0008\n",
      "[55] : train loss 0.001274, val loss drop 0.0008 to 0.0006\n",
      "[77] : train loss 0.001130, val loss drop 0.0006 to 0.0006\n",
      "[92] : train loss 0.000504, val loss drop 0.0006 to 0.0004\n",
      "[93] : train loss 0.000465, val loss drop 0.0004 to 0.0004\n",
      "[97] : train loss 0.000246, val loss drop 0.0004 to 0.0004\n",
      "[98] : train loss 0.000291, val loss drop 0.0004 to 0.0004\n",
      "fold 8\n",
      "[1] : train loss 2.214515, val loss drop 10000000.0000 to 0.6354\n",
      "[2] : train loss 0.339905, val loss drop 0.6354 to 0.5812\n",
      "[3] : train loss 0.089391, val loss drop 0.5812 to 0.3471\n",
      "[4] : train loss 0.033438, val loss drop 0.3471 to 0.0430\n",
      "[6] : train loss 0.006840, val loss drop 0.0430 to 0.0047\n",
      "[9] : train loss 0.002202, val loss drop 0.0047 to 0.0021\n",
      "[12] : train loss 0.001391, val loss drop 0.0021 to 0.0018\n",
      "[13] : train loss 0.001164, val loss drop 0.0018 to 0.0012\n",
      "[14] : train loss 0.000898, val loss drop 0.0012 to 0.0010\n",
      "[19] : train loss 0.000865, val loss drop 0.0010 to 0.0007\n",
      "[25] : train loss 0.000819, val loss drop 0.0007 to 0.0005\n",
      "[27] : train loss 0.000264, val loss drop 0.0005 to 0.0004\n",
      "[33] : train loss 0.000652, val loss drop 0.0004 to 0.0004\n",
      "[34] : train loss 0.000347, val loss drop 0.0004 to 0.0003\n",
      "[56] : train loss 0.000191, val loss drop 0.0003 to 0.0002\n",
      "fold 9\n",
      "[1] : train loss 2.351578, val loss drop 10000000.0000 to 0.8895\n",
      "[2] : train loss 0.443845, val loss drop 0.8895 to 0.5735\n",
      "[3] : train loss 0.088213, val loss drop 0.5735 to 0.2006\n",
      "[4] : train loss 0.031476, val loss drop 0.2006 to 0.1828\n",
      "[5] : train loss 0.017080, val loss drop 0.1828 to 0.0667\n",
      "[6] : train loss 0.008269, val loss drop 0.0667 to 0.0068\n",
      "[7] : train loss 0.004264, val loss drop 0.0068 to 0.0063\n",
      "[9] : train loss 0.002080, val loss drop 0.0063 to 0.0053\n",
      "[10] : train loss 0.001650, val loss drop 0.0053 to 0.0038\n",
      "[11] : train loss 0.001350, val loss drop 0.0038 to 0.0011\n",
      "[12] : train loss 0.001193, val loss drop 0.0011 to 0.0011\n",
      "[15] : train loss 0.000767, val loss drop 0.0011 to 0.0010\n",
      "[19] : train loss 0.001186, val loss drop 0.0010 to 0.0008\n",
      "[20] : train loss 0.000674, val loss drop 0.0008 to 0.0007\n",
      "[36] : train loss 0.000670, val loss drop 0.0007 to 0.0004\n",
      "[44] : train loss 0.000319, val loss drop 0.0004 to 0.0003\n",
      "[49] : train loss 0.000404, val loss drop 0.0003 to 0.0002\n",
      "fold 10\n",
      "[1] : train loss 2.148781, val loss drop 10000000.0000 to 0.6686\n",
      "[3] : train loss 0.075756, val loss drop 0.6686 to 0.2156\n",
      "[4] : train loss 0.027794, val loss drop 0.2156 to 0.1283\n",
      "[5] : train loss 0.012452, val loss drop 0.1283 to 0.0421\n",
      "[6] : train loss 0.006549, val loss drop 0.0421 to 0.0091\n",
      "[7] : train loss 0.003714, val loss drop 0.0091 to 0.0046\n",
      "[9] : train loss 0.001819, val loss drop 0.0046 to 0.0031\n",
      "[11] : train loss 0.001745, val loss drop 0.0031 to 0.0021\n",
      "[18] : train loss 0.001229, val loss drop 0.0021 to 0.0015\n",
      "[23] : train loss 0.001504, val loss drop 0.0015 to 0.0013\n",
      "[26] : train loss 0.000844, val loss drop 0.0013 to 0.0010\n",
      "[39] : train loss 0.000612, val loss drop 0.0010 to 0.0006\n",
      "[54] : train loss 0.000615, val loss drop 0.0006 to 0.0005\n",
      "[55] : train loss 0.000377, val loss drop 0.0005 to 0.0005\n",
      "[56] : train loss 0.000381, val loss drop 0.0005 to 0.0004\n",
      "[78] : train loss 0.000289, val loss drop 0.0004 to 0.0003\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 38.517350, val loss drop 10000000.0000 to 3.1644\n",
      "[2] : train loss 0.292342, val loss drop 3.1644 to 1.1660\n",
      "[3] : train loss 0.093459, val loss drop 1.1660 to 0.0729\n",
      "[4] : train loss 0.039169, val loss drop 0.0729 to 0.0582\n",
      "[5] : train loss 0.023215, val loss drop 0.0582 to 0.0497\n",
      "[6] : train loss 0.033497, val loss drop 0.0497 to 0.0174\n",
      "[10] : train loss 0.037699, val loss drop 0.0174 to 0.0057\n",
      "[13] : train loss 0.021218, val loss drop 0.0057 to 0.0053\n",
      "[17] : train loss 0.022352, val loss drop 0.0053 to 0.0042\n",
      "[18] : train loss 0.008813, val loss drop 0.0042 to 0.0037\n",
      "[28] : train loss 0.006021, val loss drop 0.0037 to 0.0029\n",
      "[40] : train loss 0.008782, val loss drop 0.0029 to 0.0012\n",
      "[44] : train loss 0.001528, val loss drop 0.0012 to 0.0010\n",
      "[48] : train loss 0.002859, val loss drop 0.0010 to 0.0009\n",
      "[67] : train loss 0.005712, val loss drop 0.0009 to 0.0009\n",
      "[81] : train loss 0.002753, val loss drop 0.0009 to 0.0007\n",
      "fold 2\n",
      "[1] : train loss 80.757758, val loss drop 10000000.0000 to 11.5080\n",
      "[2] : train loss 2.708252, val loss drop 11.5080 to 4.6924\n",
      "[3] : train loss 1.116853, val loss drop 4.6924 to 1.5275\n",
      "[4] : train loss 0.457828, val loss drop 1.5275 to 0.3831\n",
      "[5] : train loss 0.179897, val loss drop 0.3831 to 0.0747\n",
      "[6] : train loss 0.110570, val loss drop 0.0747 to 0.0625\n",
      "[11] : train loss 0.050104, val loss drop 0.0625 to 0.0278\n",
      "[13] : train loss 0.031510, val loss drop 0.0278 to 0.0209\n",
      "[16] : train loss 0.016594, val loss drop 0.0209 to 0.0199\n",
      "[18] : train loss 0.042565, val loss drop 0.0199 to 0.0177\n",
      "[26] : train loss 0.051554, val loss drop 0.0177 to 0.0110\n",
      "[32] : train loss 0.038749, val loss drop 0.0110 to 0.0100\n",
      "[35] : train loss 0.055754, val loss drop 0.0100 to 0.0092\n",
      "[44] : train loss 0.020854, val loss drop 0.0092 to 0.0086\n",
      "[65] : train loss 0.050973, val loss drop 0.0086 to 0.0063\n",
      "[73] : train loss 0.018709, val loss drop 0.0063 to 0.0058\n",
      "fold 3\n",
      "[1] : train loss 20.519261, val loss drop 10000000.0000 to 1.6194\n",
      "[3] : train loss 0.159986, val loss drop 1.6194 to 0.2972\n",
      "[4] : train loss 0.076656, val loss drop 0.2972 to 0.1492\n",
      "[5] : train loss 0.038201, val loss drop 0.1492 to 0.0374\n",
      "[6] : train loss 0.018710, val loss drop 0.0374 to 0.0142\n",
      "[7] : train loss 0.013867, val loss drop 0.0142 to 0.0087\n",
      "[9] : train loss 0.009886, val loss drop 0.0087 to 0.0077\n",
      "[11] : train loss 0.007352, val loss drop 0.0077 to 0.0056\n",
      "[14] : train loss 0.005162, val loss drop 0.0056 to 0.0052\n",
      "[15] : train loss 0.004486, val loss drop 0.0052 to 0.0044\n",
      "[18] : train loss 0.004243, val loss drop 0.0044 to 0.0035\n",
      "[23] : train loss 0.002720, val loss drop 0.0035 to 0.0032\n",
      "[24] : train loss 0.003103, val loss drop 0.0032 to 0.0031\n",
      "[36] : train loss 0.002670, val loss drop 0.0031 to 0.0023\n",
      "[40] : train loss 0.002422, val loss drop 0.0023 to 0.0022\n",
      "[44] : train loss 0.002096, val loss drop 0.0022 to 0.0012\n",
      "[75] : train loss 0.001314, val loss drop 0.0012 to 0.0011\n",
      "[88] : train loss 0.000577, val loss drop 0.0011 to 0.0010\n",
      "fold 4\n",
      "[1] : train loss 31.252790, val loss drop 10000000.0000 to 2.8898\n",
      "[2] : train loss 1.904176, val loss drop 2.8898 to 2.3258\n",
      "[3] : train loss 0.816030, val loss drop 2.3258 to 1.3433\n",
      "[4] : train loss 0.472516, val loss drop 1.3433 to 0.2126\n",
      "[5] : train loss 0.263644, val loss drop 0.2126 to 0.2118\n",
      "[6] : train loss 0.133817, val loss drop 0.2118 to 0.0837\n",
      "[7] : train loss 0.047037, val loss drop 0.0837 to 0.0600\n",
      "[8] : train loss 0.022071, val loss drop 0.0600 to 0.0216\n",
      "[14] : train loss 0.035977, val loss drop 0.0216 to 0.0203\n",
      "[16] : train loss 0.021090, val loss drop 0.0203 to 0.0104\n",
      "[27] : train loss 0.012114, val loss drop 0.0104 to 0.0084\n",
      "[31] : train loss 0.012984, val loss drop 0.0084 to 0.0075\n",
      "[33] : train loss 0.011102, val loss drop 0.0075 to 0.0061\n",
      "[48] : train loss 0.033084, val loss drop 0.0061 to 0.0058\n",
      "[56] : train loss 0.002767, val loss drop 0.0058 to 0.0034\n",
      "[57] : train loss 0.005742, val loss drop 0.0034 to 0.0019\n",
      "fold 5\n",
      "[1] : train loss 26.531118, val loss drop 10000000.0000 to 22.9589\n",
      "[2] : train loss 1.111113, val loss drop 22.9589 to 1.1058\n",
      "[3] : train loss 0.317168, val loss drop 1.1058 to 0.7797\n",
      "[4] : train loss 0.188647, val loss drop 0.7797 to 0.2097\n",
      "[5] : train loss 0.078764, val loss drop 0.2097 to 0.0285\n",
      "[8] : train loss 0.027453, val loss drop 0.0285 to 0.0107\n",
      "[9] : train loss 0.010684, val loss drop 0.0107 to 0.0089\n",
      "[15] : train loss 0.050509, val loss drop 0.0089 to 0.0083\n",
      "[16] : train loss 0.016772, val loss drop 0.0083 to 0.0055\n",
      "[22] : train loss 0.006988, val loss drop 0.0055 to 0.0050\n",
      "[28] : train loss 0.012397, val loss drop 0.0050 to 0.0040\n",
      "[42] : train loss 0.005716, val loss drop 0.0040 to 0.0032\n",
      "[56] : train loss 0.033598, val loss drop 0.0032 to 0.0025\n",
      "[73] : train loss 0.004065, val loss drop 0.0025 to 0.0024\n",
      "[88] : train loss 0.007119, val loss drop 0.0024 to 0.0023\n",
      "[97] : train loss 0.005600, val loss drop 0.0023 to 0.0017\n",
      "fold 6\n",
      "[1] : train loss 13.877531, val loss drop 10000000.0000 to 0.2508\n",
      "[2] : train loss 0.147220, val loss drop 0.2508 to 0.0722\n",
      "[3] : train loss 0.054175, val loss drop 0.0722 to 0.0286\n",
      "[5] : train loss 0.032465, val loss drop 0.0286 to 0.0215\n",
      "[6] : train loss 0.037447, val loss drop 0.0215 to 0.0058\n",
      "[9] : train loss 0.006570, val loss drop 0.0058 to 0.0054\n",
      "[10] : train loss 0.003823, val loss drop 0.0054 to 0.0051\n",
      "[15] : train loss 0.006384, val loss drop 0.0051 to 0.0024\n",
      "[16] : train loss 0.002998, val loss drop 0.0024 to 0.0021\n",
      "[23] : train loss 0.002114, val loss drop 0.0021 to 0.0020\n",
      "[51] : train loss 0.002678, val loss drop 0.0020 to 0.0012\n",
      "[67] : train loss 0.008739, val loss drop 0.0012 to 0.0007\n",
      "fold 7\n",
      "[1] : train loss 137.383709, val loss drop 10000000.0000 to 7.4281\n",
      "[2] : train loss 0.654326, val loss drop 7.4281 to 1.0053\n",
      "[3] : train loss 0.256594, val loss drop 1.0053 to 0.2986\n",
      "[4] : train loss 0.140053, val loss drop 0.2986 to 0.1149\n",
      "[5] : train loss 0.071530, val loss drop 0.1149 to 0.0453\n",
      "[7] : train loss 0.095538, val loss drop 0.0453 to 0.0450\n",
      "[11] : train loss 0.064785, val loss drop 0.0450 to 0.0417\n",
      "[12] : train loss 0.057328, val loss drop 0.0417 to 0.0134\n",
      "[30] : train loss 0.070067, val loss drop 0.0134 to 0.0115\n",
      "[44] : train loss 0.097566, val loss drop 0.0115 to 0.0077\n",
      "[47] : train loss 0.011671, val loss drop 0.0077 to 0.0074\n",
      "[48] : train loss 0.017191, val loss drop 0.0074 to 0.0048\n",
      "[86] : train loss 0.007400, val loss drop 0.0048 to 0.0041\n",
      "fold 8\n",
      "[1] : train loss 6.617387, val loss drop 10000000.0000 to 0.7987\n",
      "[2] : train loss 0.045810, val loss drop 0.7987 to 0.0783\n",
      "[3] : train loss 0.025516, val loss drop 0.0783 to 0.0676\n",
      "[4] : train loss 0.024502, val loss drop 0.0676 to 0.0642\n",
      "[5] : train loss 0.026155, val loss drop 0.0642 to 0.0123\n",
      "[6] : train loss 0.009718, val loss drop 0.0123 to 0.0057\n",
      "[10] : train loss 0.009467, val loss drop 0.0057 to 0.0035\n",
      "[27] : train loss 0.009022, val loss drop 0.0035 to 0.0018\n",
      "[30] : train loss 0.005897, val loss drop 0.0018 to 0.0010\n",
      "[82] : train loss 0.002096, val loss drop 0.0010 to 0.0006\n",
      "fold 9\n",
      "[1] : train loss 114.170395, val loss drop 10000000.0000 to 18.2078\n",
      "[2] : train loss 2.538622, val loss drop 18.2078 to 1.3237\n",
      "[3] : train loss 0.769266, val loss drop 1.3237 to 0.2333\n",
      "[4] : train loss 0.294176, val loss drop 0.2333 to 0.1892\n",
      "[5] : train loss 0.148223, val loss drop 0.1892 to 0.0695\n",
      "[6] : train loss 0.051286, val loss drop 0.0695 to 0.0417\n",
      "[7] : train loss 0.032427, val loss drop 0.0417 to 0.0214\n",
      "[8] : train loss 0.032663, val loss drop 0.0214 to 0.0137\n",
      "[9] : train loss 0.020495, val loss drop 0.0137 to 0.0131\n",
      "[11] : train loss 0.024789, val loss drop 0.0131 to 0.0107\n",
      "[12] : train loss 0.016353, val loss drop 0.0107 to 0.0079\n",
      "[23] : train loss 0.015130, val loss drop 0.0079 to 0.0064\n",
      "[25] : train loss 0.009149, val loss drop 0.0064 to 0.0051\n",
      "[28] : train loss 0.009427, val loss drop 0.0051 to 0.0043\n",
      "[36] : train loss 0.013394, val loss drop 0.0043 to 0.0033\n",
      "[46] : train loss 0.006100, val loss drop 0.0033 to 0.0031\n",
      "[54] : train loss 0.008416, val loss drop 0.0031 to 0.0026\n",
      "[78] : train loss 0.003791, val loss drop 0.0026 to 0.0025\n",
      "[90] : train loss 0.003786, val loss drop 0.0025 to 0.0019\n",
      "fold 10\n",
      "[1] : train loss 55.311332, val loss drop 10000000.0000 to 5.2975\n",
      "[2] : train loss 0.550110, val loss drop 5.2975 to 3.8222\n",
      "[3] : train loss 0.243544, val loss drop 3.8222 to 0.3696\n",
      "[4] : train loss 0.166292, val loss drop 0.3696 to 0.2269\n",
      "[5] : train loss 0.110020, val loss drop 0.2269 to 0.0472\n",
      "[6] : train loss 0.050895, val loss drop 0.0472 to 0.0305\n",
      "[10] : train loss 0.022128, val loss drop 0.0305 to 0.0185\n",
      "[19] : train loss 0.025159, val loss drop 0.0185 to 0.0183\n",
      "[21] : train loss 0.009503, val loss drop 0.0183 to 0.0162\n",
      "[22] : train loss 0.015697, val loss drop 0.0162 to 0.0123\n",
      "[25] : train loss 0.043264, val loss drop 0.0123 to 0.0106\n",
      "[26] : train loss 0.017626, val loss drop 0.0106 to 0.0100\n",
      "[41] : train loss 0.026041, val loss drop 0.0100 to 0.0084\n",
      "[45] : train loss 0.004286, val loss drop 0.0084 to 0.0072\n",
      "[52] : train loss 0.007643, val loss drop 0.0072 to 0.0061\n",
      "[54] : train loss 0.016348, val loss drop 0.0061 to 0.0055\n",
      "[78] : train loss 0.005591, val loss drop 0.0055 to 0.0053\n",
      "[80] : train loss 0.008221, val loss drop 0.0053 to 0.0048\n",
      "[94] : train loss 0.008189, val loss drop 0.0048 to 0.0042\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 27.950205, val loss drop 10000000.0000 to 38.4412\n",
      "[2] : train loss 1.520308, val loss drop 38.4412 to 17.9524\n",
      "[3] : train loss 1.136112, val loss drop 17.9524 to 5.6191\n",
      "[4] : train loss 0.729250, val loss drop 5.6191 to 0.3493\n",
      "[7] : train loss 0.406565, val loss drop 0.3493 to 0.2326\n",
      "[9] : train loss 0.281697, val loss drop 0.2326 to 0.1646\n",
      "[10] : train loss 0.218309, val loss drop 0.1646 to 0.1039\n",
      "[11] : train loss 0.220843, val loss drop 0.1039 to 0.0990\n",
      "[19] : train loss 0.160932, val loss drop 0.0990 to 0.0937\n",
      "[28] : train loss 0.195827, val loss drop 0.0937 to 0.0835\n",
      "[29] : train loss 0.310515, val loss drop 0.0835 to 0.0805\n",
      "[32] : train loss 0.303306, val loss drop 0.0805 to 0.0505\n",
      "[33] : train loss 0.160419, val loss drop 0.0505 to 0.0434\n",
      "[42] : train loss 0.258850, val loss drop 0.0434 to 0.0342\n",
      "[45] : train loss 0.088242, val loss drop 0.0342 to 0.0331\n",
      "[64] : train loss 0.096372, val loss drop 0.0331 to 0.0279\n",
      "[74] : train loss 0.241686, val loss drop 0.0279 to 0.0172\n",
      "fold 2\n",
      "[1] : train loss 25.634244, val loss drop 10000000.0000 to 102.5712\n",
      "[2] : train loss 2.280172, val loss drop 102.5712 to 48.7814\n",
      "[3] : train loss 1.864793, val loss drop 48.7814 to 1.5238\n",
      "[5] : train loss 0.363918, val loss drop 1.5238 to 0.2316\n",
      "[7] : train loss 0.194497, val loss drop 0.2316 to 0.1545\n",
      "[11] : train loss 0.519207, val loss drop 0.1545 to 0.0867\n",
      "[16] : train loss 0.190926, val loss drop 0.0867 to 0.0571\n",
      "[18] : train loss 0.124696, val loss drop 0.0571 to 0.0481\n",
      "[23] : train loss 0.238558, val loss drop 0.0481 to 0.0335\n",
      "[51] : train loss 0.260500, val loss drop 0.0335 to 0.0298\n",
      "[86] : train loss 0.100565, val loss drop 0.0298 to 0.0224\n",
      "[95] : train loss 0.093911, val loss drop 0.0224 to 0.0123\n",
      "fold 3\n",
      "[1] : train loss 25.813856, val loss drop 10000000.0000 to 126.9104\n",
      "[2] : train loss 2.268056, val loss drop 126.9104 to 13.2214\n",
      "[3] : train loss 0.770004, val loss drop 13.2214 to 4.2207\n",
      "[4] : train loss 0.610953, val loss drop 4.2207 to 0.9197\n",
      "[5] : train loss 0.554113, val loss drop 0.9197 to 0.3981\n",
      "[7] : train loss 0.333855, val loss drop 0.3981 to 0.1319\n",
      "[8] : train loss 0.208944, val loss drop 0.1319 to 0.0699\n",
      "[10] : train loss 0.536561, val loss drop 0.0699 to 0.0671\n",
      "[14] : train loss 0.149463, val loss drop 0.0671 to 0.0622\n",
      "[15] : train loss 0.154640, val loss drop 0.0622 to 0.0497\n",
      "[16] : train loss 0.236668, val loss drop 0.0497 to 0.0429\n",
      "[17] : train loss 0.177294, val loss drop 0.0429 to 0.0393\n",
      "[22] : train loss 0.096251, val loss drop 0.0393 to 0.0378\n",
      "[35] : train loss 0.228097, val loss drop 0.0378 to 0.0338\n",
      "[67] : train loss 0.055132, val loss drop 0.0338 to 0.0281\n",
      "[68] : train loss 0.098353, val loss drop 0.0281 to 0.0259\n",
      "[85] : train loss 0.128544, val loss drop 0.0259 to 0.0145\n",
      "fold 4\n",
      "[1] : train loss 33.140855, val loss drop 10000000.0000 to 11.5213\n",
      "[2] : train loss 2.325201, val loss drop 11.5213 to 3.0483\n",
      "[4] : train loss 0.874247, val loss drop 3.0483 to 0.5841\n",
      "[5] : train loss 0.494793, val loss drop 0.5841 to 0.4559\n",
      "[6] : train loss 0.281966, val loss drop 0.4559 to 0.4219\n",
      "[7] : train loss 0.414871, val loss drop 0.4219 to 0.1934\n",
      "[8] : train loss 0.246373, val loss drop 0.1934 to 0.1694\n",
      "[14] : train loss 0.355238, val loss drop 0.1694 to 0.1086\n",
      "[19] : train loss 0.217752, val loss drop 0.1086 to 0.0764\n",
      "[20] : train loss 0.201649, val loss drop 0.0764 to 0.0462\n",
      "[22] : train loss 0.098987, val loss drop 0.0462 to 0.0384\n",
      "[28] : train loss 0.237434, val loss drop 0.0384 to 0.0242\n",
      "[31] : train loss 0.118707, val loss drop 0.0242 to 0.0215\n",
      "[53] : train loss 0.312574, val loss drop 0.0215 to 0.0183\n",
      "[63] : train loss 0.179962, val loss drop 0.0183 to 0.0171\n",
      "fold 5\n",
      "[1] : train loss 27.852873, val loss drop 10000000.0000 to 151.2683\n",
      "[2] : train loss 2.005072, val loss drop 151.2683 to 33.9465\n",
      "[3] : train loss 0.862789, val loss drop 33.9465 to 5.7920\n",
      "[4] : train loss 0.490707, val loss drop 5.7920 to 0.6583\n",
      "[5] : train loss 0.305372, val loss drop 0.6583 to 0.4154\n",
      "[6] : train loss 0.312576, val loss drop 0.4154 to 0.4010\n",
      "[7] : train loss 0.410753, val loss drop 0.4010 to 0.2066\n",
      "[10] : train loss 0.246163, val loss drop 0.2066 to 0.1149\n",
      "[11] : train loss 0.256962, val loss drop 0.1149 to 0.0715\n",
      "[16] : train loss 0.214278, val loss drop 0.0715 to 0.0644\n",
      "[17] : train loss 0.235340, val loss drop 0.0644 to 0.0547\n",
      "[35] : train loss 0.177283, val loss drop 0.0547 to 0.0462\n",
      "[36] : train loss 0.172633, val loss drop 0.0462 to 0.0283\n",
      "[47] : train loss 0.164948, val loss drop 0.0283 to 0.0224\n",
      "[82] : train loss 0.195374, val loss drop 0.0224 to 0.0180\n",
      "fold 6\n",
      "[1] : train loss 21.854732, val loss drop 10000000.0000 to 48.8136\n",
      "[2] : train loss 2.013015, val loss drop 48.8136 to 5.2795\n",
      "[3] : train loss 1.076805, val loss drop 5.2795 to 2.9782\n",
      "[4] : train loss 0.617234, val loss drop 2.9782 to 0.6491\n",
      "[5] : train loss 0.653765, val loss drop 0.6491 to 0.3552\n",
      "[6] : train loss 0.675161, val loss drop 0.3552 to 0.3093\n",
      "[7] : train loss 0.532896, val loss drop 0.3093 to 0.1337\n",
      "[8] : train loss 0.173568, val loss drop 0.1337 to 0.1200\n",
      "[10] : train loss 0.363858, val loss drop 0.1200 to 0.0770\n",
      "[14] : train loss 0.122737, val loss drop 0.0770 to 0.0526\n",
      "[49] : train loss 0.095989, val loss drop 0.0526 to 0.0229\n",
      "[50] : train loss 0.081736, val loss drop 0.0229 to 0.0165\n",
      "fold 7\n",
      "[1] : train loss 26.915217, val loss drop 10000000.0000 to 20.7123\n",
      "[2] : train loss 1.848726, val loss drop 20.7123 to 18.2544\n",
      "[3] : train loss 1.058885, val loss drop 18.2544 to 5.9063\n",
      "[4] : train loss 0.779507, val loss drop 5.9063 to 1.5985\n",
      "[5] : train loss 0.392794, val loss drop 1.5985 to 0.4925\n",
      "[6] : train loss 0.346715, val loss drop 0.4925 to 0.3762\n",
      "[7] : train loss 0.234189, val loss drop 0.3762 to 0.1053\n",
      "[9] : train loss 0.352376, val loss drop 0.1053 to 0.0816\n",
      "[13] : train loss 0.176847, val loss drop 0.0816 to 0.0690\n",
      "[14] : train loss 0.126438, val loss drop 0.0690 to 0.0541\n",
      "[30] : train loss 0.158639, val loss drop 0.0541 to 0.0290\n",
      "[65] : train loss 0.113123, val loss drop 0.0290 to 0.0288\n",
      "[66] : train loss 0.086089, val loss drop 0.0288 to 0.0184\n",
      "[84] : train loss 0.071610, val loss drop 0.0184 to 0.0155\n",
      "fold 8\n",
      "[1] : train loss 26.219558, val loss drop 10000000.0000 to 49.5242\n",
      "[2] : train loss 2.605958, val loss drop 49.5242 to 16.7387\n",
      "[3] : train loss 1.105261, val loss drop 16.7387 to 2.9539\n",
      "[4] : train loss 0.599769, val loss drop 2.9539 to 2.0721\n",
      "[5] : train loss 0.526290, val loss drop 2.0721 to 1.6638\n",
      "[8] : train loss 0.798921, val loss drop 1.6638 to 0.5645\n",
      "[9] : train loss 0.481401, val loss drop 0.5645 to 0.1880\n",
      "[14] : train loss 0.199293, val loss drop 0.1880 to 0.0647\n",
      "[25] : train loss 0.208142, val loss drop 0.0647 to 0.0347\n",
      "[44] : train loss 0.393356, val loss drop 0.0347 to 0.0344\n",
      "[54] : train loss 0.396918, val loss drop 0.0344 to 0.0268\n",
      "[74] : train loss 0.104550, val loss drop 0.0268 to 0.0100\n",
      "fold 9\n",
      "[1] : train loss 28.435083, val loss drop 10000000.0000 to 104.7074\n",
      "[2] : train loss 2.939090, val loss drop 104.7074 to 28.0348\n",
      "[3] : train loss 1.091769, val loss drop 28.0348 to 4.4619\n",
      "[4] : train loss 0.983107, val loss drop 4.4619 to 0.4439\n",
      "[5] : train loss 0.527622, val loss drop 0.4439 to 0.2733\n",
      "[6] : train loss 0.299953, val loss drop 0.2733 to 0.1824\n",
      "[12] : train loss 0.218728, val loss drop 0.1824 to 0.0901\n",
      "[18] : train loss 0.349828, val loss drop 0.0901 to 0.0803\n",
      "[20] : train loss 0.265754, val loss drop 0.0803 to 0.0618\n",
      "[26] : train loss 0.122253, val loss drop 0.0618 to 0.0549\n",
      "[27] : train loss 0.243817, val loss drop 0.0549 to 0.0285\n",
      "[43] : train loss 0.115707, val loss drop 0.0285 to 0.0270\n",
      "[47] : train loss 0.175193, val loss drop 0.0270 to 0.0196\n",
      "[72] : train loss 0.198338, val loss drop 0.0196 to 0.0185\n",
      "[78] : train loss 0.130022, val loss drop 0.0185 to 0.0138\n",
      "[94] : train loss 0.039268, val loss drop 0.0138 to 0.0093\n",
      "[96] : train loss 0.056301, val loss drop 0.0093 to 0.0084\n",
      "fold 10\n",
      "[1] : train loss 26.723125, val loss drop 10000000.0000 to 24.4705\n",
      "[2] : train loss 3.666091, val loss drop 24.4705 to 4.7543\n",
      "[3] : train loss 1.235552, val loss drop 4.7543 to 2.8612\n",
      "[4] : train loss 0.630979, val loss drop 2.8612 to 0.6502\n",
      "[5] : train loss 0.384459, val loss drop 0.6502 to 0.1489\n",
      "[6] : train loss 0.267626, val loss drop 0.1489 to 0.1441\n",
      "[7] : train loss 0.333343, val loss drop 0.1441 to 0.1169\n",
      "[8] : train loss 0.403288, val loss drop 0.1169 to 0.1066\n",
      "[14] : train loss 0.299497, val loss drop 0.1066 to 0.0857\n",
      "[17] : train loss 0.303691, val loss drop 0.0857 to 0.0766\n",
      "[19] : train loss 0.305408, val loss drop 0.0766 to 0.0634\n",
      "[21] : train loss 0.161089, val loss drop 0.0634 to 0.0567\n",
      "[22] : train loss 0.124615, val loss drop 0.0567 to 0.0532\n",
      "[25] : train loss 0.274091, val loss drop 0.0532 to 0.0361\n",
      "[27] : train loss 0.264542, val loss drop 0.0361 to 0.0330\n",
      "[83] : train loss 0.208644, val loss drop 0.0330 to 0.0322\n",
      "[84] : train loss 0.140965, val loss drop 0.0322 to 0.0275\n",
      "[86] : train loss 0.140373, val loss drop 0.0275 to 0.0256\n",
      "[99] : train loss 0.185891, val loss drop 0.0256 to 0.0140\n",
      "[100] : train loss 0.088913, val loss drop 0.0140 to 0.0108\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([32, 64, 128, 256, 512], [1, 375, n_features], (6, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*6*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([32, 64, 128, 256, 512], [1, 375, n_features], (6, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*6*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([32, 64, 128, 256, 512], [1, 375, n_features], (6, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*6*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-262.549469</td>\n",
       "      <td>-43.139381</td>\n",
       "      <td>112.939957</td>\n",
       "      <td>0.493597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>305.662506</td>\n",
       "      <td>-280.862122</td>\n",
       "      <td>85.900970</td>\n",
       "      <td>0.483432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-231.270584</td>\n",
       "      <td>138.257217</td>\n",
       "      <td>30.534260</td>\n",
       "      <td>0.396861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>153.628372</td>\n",
       "      <td>282.001404</td>\n",
       "      <td>27.178936</td>\n",
       "      <td>0.399658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-161.359268</td>\n",
       "      <td>184.475433</td>\n",
       "      <td>130.145233</td>\n",
       "      <td>0.466030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -262.549469  -43.139381  112.939957  0.493597\n",
       "1  2801  305.662506 -280.862122   85.900970  0.483432\n",
       "2  2802 -231.270584  138.257217   30.534260  0.396861\n",
       "3  2803  153.628372  282.001404   27.178936  0.399658\n",
       "4  2804 -161.359268  184.475433  130.145233  0.466030"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, 'submit.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

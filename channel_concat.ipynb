{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)\n",
    "        #return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "train_f_r = train_f[:,:,::-1,:]\n",
    "train_f = np.concatenate((train_f, train_f_r), axis = 1)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f_r = test_f[:,:,::-1,:]\n",
    "test_f = np.concatenate((test_f, test_f_r), axis = 1)\n",
    "\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.02, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (3, 1))\n",
    "        fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        model = cnn_model(conv, fc)\n",
    "        #model = get_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.604049, val loss drop 10000000.0000 to 1.2417\n",
      "[2] : train loss 0.696046, val loss drop 1.2417 to 0.1852\n",
      "[3] : train loss 0.124312, val loss drop 0.1852 to 0.0903\n",
      "[4] : train loss 0.047578, val loss drop 0.0903 to 0.0600\n",
      "[5] : train loss 0.023293, val loss drop 0.0600 to 0.0192\n",
      "[7] : train loss 0.008824, val loss drop 0.0192 to 0.0132\n",
      "[8] : train loss 0.005325, val loss drop 0.0132 to 0.0098\n",
      "[9] : train loss 0.003818, val loss drop 0.0098 to 0.0041\n",
      "[12] : train loss 0.001704, val loss drop 0.0041 to 0.0027\n",
      "[13] : train loss 0.001691, val loss drop 0.0027 to 0.0026\n",
      "[24] : train loss 0.000805, val loss drop 0.0026 to 0.0024\n",
      "[27] : train loss 0.001142, val loss drop 0.0024 to 0.0018\n",
      "[28] : train loss 0.001390, val loss drop 0.0018 to 0.0015\n",
      "[30] : train loss 0.000788, val loss drop 0.0015 to 0.0013\n",
      "[33] : train loss 0.000865, val loss drop 0.0013 to 0.0012\n",
      "[37] : train loss 0.000601, val loss drop 0.0012 to 0.0009\n",
      "[55] : train loss 0.000593, val loss drop 0.0009 to 0.0009\n",
      "[57] : train loss 0.000760, val loss drop 0.0009 to 0.0008\n",
      "[89] : train loss 0.000592, val loss drop 0.0008 to 0.0007\n",
      "fold 2\n",
      "[1] : train loss 2.783666, val loss drop 10000000.0000 to 2.1073\n",
      "[2] : train loss 1.369121, val loss drop 2.1073 to 0.5137\n",
      "[3] : train loss 0.240819, val loss drop 0.5137 to 0.1401\n",
      "[4] : train loss 0.062686, val loss drop 0.1401 to 0.0493\n",
      "[5] : train loss 0.026992, val loss drop 0.0493 to 0.0236\n",
      "[6] : train loss 0.013566, val loss drop 0.0236 to 0.0177\n",
      "[7] : train loss 0.008970, val loss drop 0.0177 to 0.0072\n",
      "[9] : train loss 0.005112, val loss drop 0.0072 to 0.0067\n",
      "[10] : train loss 0.003781, val loss drop 0.0067 to 0.0048\n",
      "[11] : train loss 0.002481, val loss drop 0.0048 to 0.0030\n",
      "[12] : train loss 0.002353, val loss drop 0.0030 to 0.0029\n",
      "[13] : train loss 0.001877, val loss drop 0.0029 to 0.0028\n",
      "[14] : train loss 0.001257, val loss drop 0.0028 to 0.0022\n",
      "[15] : train loss 0.001368, val loss drop 0.0022 to 0.0020\n",
      "[19] : train loss 0.001417, val loss drop 0.0020 to 0.0019\n",
      "[20] : train loss 0.001143, val loss drop 0.0019 to 0.0015\n",
      "[25] : train loss 0.001339, val loss drop 0.0015 to 0.0013\n",
      "[36] : train loss 0.001088, val loss drop 0.0013 to 0.0011\n",
      "[37] : train loss 0.000664, val loss drop 0.0011 to 0.0011\n",
      "[38] : train loss 0.000528, val loss drop 0.0011 to 0.0011\n",
      "[41] : train loss 0.000453, val loss drop 0.0011 to 0.0009\n",
      "[56] : train loss 0.000615, val loss drop 0.0009 to 0.0008\n",
      "[60] : train loss 0.000597, val loss drop 0.0008 to 0.0008\n",
      "[61] : train loss 0.000501, val loss drop 0.0008 to 0.0008\n",
      "[85] : train loss 0.001082, val loss drop 0.0008 to 0.0008\n",
      "[87] : train loss 0.000456, val loss drop 0.0008 to 0.0006\n",
      "[89] : train loss 0.000530, val loss drop 0.0006 to 0.0006\n",
      "[90] : train loss 0.000376, val loss drop 0.0006 to 0.0005\n",
      "fold 3\n",
      "[1] : train loss 2.665992, val loss drop 10000000.0000 to 1.6033\n",
      "[2] : train loss 0.899581, val loss drop 1.6033 to 0.3057\n",
      "[3] : train loss 0.149723, val loss drop 0.3057 to 0.1122\n",
      "[4] : train loss 0.050764, val loss drop 0.1122 to 0.0604\n",
      "[5] : train loss 0.024650, val loss drop 0.0604 to 0.0232\n",
      "[6] : train loss 0.012085, val loss drop 0.0232 to 0.0161\n",
      "[7] : train loss 0.006802, val loss drop 0.0161 to 0.0126\n",
      "[8] : train loss 0.004892, val loss drop 0.0126 to 0.0051\n",
      "[10] : train loss 0.004052, val loss drop 0.0051 to 0.0046\n",
      "[11] : train loss 0.003484, val loss drop 0.0046 to 0.0032\n",
      "[12] : train loss 0.002459, val loss drop 0.0032 to 0.0028\n",
      "[14] : train loss 0.001517, val loss drop 0.0028 to 0.0023\n",
      "[15] : train loss 0.001719, val loss drop 0.0023 to 0.0022\n",
      "[18] : train loss 0.001926, val loss drop 0.0022 to 0.0017\n",
      "[26] : train loss 0.002204, val loss drop 0.0017 to 0.0014\n",
      "[40] : train loss 0.000844, val loss drop 0.0014 to 0.0012\n",
      "[41] : train loss 0.000785, val loss drop 0.0012 to 0.0010\n",
      "[42] : train loss 0.000732, val loss drop 0.0010 to 0.0008\n",
      "[87] : train loss 0.000715, val loss drop 0.0008 to 0.0007\n",
      "[95] : train loss 0.000512, val loss drop 0.0007 to 0.0007\n",
      "fold 4\n",
      "[1] : train loss 2.744179, val loss drop 10000000.0000 to 1.3643\n",
      "[2] : train loss 0.832567, val loss drop 1.3643 to 0.2408\n",
      "[3] : train loss 0.143225, val loss drop 0.2408 to 0.0722\n",
      "[4] : train loss 0.046342, val loss drop 0.0722 to 0.0364\n",
      "[5] : train loss 0.020245, val loss drop 0.0364 to 0.0218\n",
      "[6] : train loss 0.012173, val loss drop 0.0218 to 0.0114\n",
      "[7] : train loss 0.006842, val loss drop 0.0114 to 0.0047\n",
      "[8] : train loss 0.004677, val loss drop 0.0047 to 0.0038\n",
      "[11] : train loss 0.002142, val loss drop 0.0038 to 0.0026\n",
      "[13] : train loss 0.001668, val loss drop 0.0026 to 0.0024\n",
      "[14] : train loss 0.002184, val loss drop 0.0024 to 0.0019\n",
      "[17] : train loss 0.001107, val loss drop 0.0019 to 0.0015\n",
      "[21] : train loss 0.000880, val loss drop 0.0015 to 0.0012\n",
      "[23] : train loss 0.000907, val loss drop 0.0012 to 0.0010\n",
      "[33] : train loss 0.000659, val loss drop 0.0010 to 0.0009\n",
      "[45] : train loss 0.000547, val loss drop 0.0009 to 0.0008\n",
      "[56] : train loss 0.000542, val loss drop 0.0008 to 0.0006\n",
      "[70] : train loss 0.001405, val loss drop 0.0006 to 0.0005\n",
      "[91] : train loss 0.000449, val loss drop 0.0005 to 0.0005\n",
      "fold 5\n",
      "[1] : train loss 2.914258, val loss drop 10000000.0000 to 1.8331\n",
      "[2] : train loss 1.179809, val loss drop 1.8331 to 0.4404\n",
      "[3] : train loss 0.205165, val loss drop 0.4404 to 0.1610\n",
      "[4] : train loss 0.073557, val loss drop 0.1610 to 0.0870\n",
      "[5] : train loss 0.031307, val loss drop 0.0870 to 0.0270\n",
      "[6] : train loss 0.015796, val loss drop 0.0270 to 0.0136\n",
      "[7] : train loss 0.009825, val loss drop 0.0136 to 0.0076\n",
      "[8] : train loss 0.006246, val loss drop 0.0076 to 0.0070\n",
      "[9] : train loss 0.005705, val loss drop 0.0070 to 0.0048\n",
      "[10] : train loss 0.003838, val loss drop 0.0048 to 0.0039\n",
      "[12] : train loss 0.002945, val loss drop 0.0039 to 0.0023\n",
      "[14] : train loss 0.001895, val loss drop 0.0023 to 0.0021\n",
      "[15] : train loss 0.001850, val loss drop 0.0021 to 0.0019\n",
      "[21] : train loss 0.000851, val loss drop 0.0019 to 0.0016\n",
      "[22] : train loss 0.000792, val loss drop 0.0016 to 0.0012\n",
      "[34] : train loss 0.000701, val loss drop 0.0012 to 0.0011\n",
      "[37] : train loss 0.000590, val loss drop 0.0011 to 0.0011\n",
      "[40] : train loss 0.001064, val loss drop 0.0011 to 0.0008\n",
      "[47] : train loss 0.000635, val loss drop 0.0008 to 0.0008\n",
      "[53] : train loss 0.000866, val loss drop 0.0008 to 0.0007\n",
      "[71] : train loss 0.000894, val loss drop 0.0007 to 0.0007\n",
      "[72] : train loss 0.000374, val loss drop 0.0007 to 0.0006\n",
      "[88] : train loss 0.000577, val loss drop 0.0006 to 0.0005\n",
      "fold 6\n",
      "[1] : train loss 2.783389, val loss drop 10000000.0000 to 1.7584\n",
      "[2] : train loss 1.432720, val loss drop 1.7584 to 0.8862\n",
      "[3] : train loss 0.530371, val loss drop 0.8862 to 0.2523\n",
      "[4] : train loss 0.109997, val loss drop 0.2523 to 0.0838\n",
      "[5] : train loss 0.040607, val loss drop 0.0838 to 0.0361\n",
      "[6] : train loss 0.019691, val loss drop 0.0361 to 0.0180\n",
      "[7] : train loss 0.011178, val loss drop 0.0180 to 0.0131\n",
      "[8] : train loss 0.007405, val loss drop 0.0131 to 0.0077\n",
      "[9] : train loss 0.005568, val loss drop 0.0077 to 0.0038\n",
      "[12] : train loss 0.002997, val loss drop 0.0038 to 0.0034\n",
      "[16] : train loss 0.001968, val loss drop 0.0034 to 0.0030\n",
      "[18] : train loss 0.002195, val loss drop 0.0030 to 0.0021\n",
      "[19] : train loss 0.001624, val loss drop 0.0021 to 0.0019\n",
      "[23] : train loss 0.001535, val loss drop 0.0019 to 0.0012\n",
      "[33] : train loss 0.001363, val loss drop 0.0012 to 0.0010\n",
      "[39] : train loss 0.001398, val loss drop 0.0010 to 0.0009\n",
      "[55] : train loss 0.000720, val loss drop 0.0009 to 0.0006\n",
      "[60] : train loss 0.000858, val loss drop 0.0006 to 0.0006\n",
      "fold 7\n",
      "[1] : train loss 2.711618, val loss drop 10000000.0000 to 1.8009\n",
      "[2] : train loss 1.303933, val loss drop 1.8009 to 0.6932\n",
      "[3] : train loss 0.579281, val loss drop 0.6932 to 0.2586\n",
      "[4] : train loss 0.237476, val loss drop 0.2586 to 0.1449\n",
      "[5] : train loss 0.085242, val loss drop 0.1449 to 0.0861\n",
      "[6] : train loss 0.038787, val loss drop 0.0861 to 0.0414\n",
      "[7] : train loss 0.023412, val loss drop 0.0414 to 0.0216\n",
      "[8] : train loss 0.012770, val loss drop 0.0216 to 0.0172\n",
      "[9] : train loss 0.008323, val loss drop 0.0172 to 0.0097\n",
      "[11] : train loss 0.006570, val loss drop 0.0097 to 0.0074\n",
      "[13] : train loss 0.003763, val loss drop 0.0074 to 0.0041\n",
      "[16] : train loss 0.002466, val loss drop 0.0041 to 0.0030\n",
      "[18] : train loss 0.001437, val loss drop 0.0030 to 0.0019\n",
      "[28] : train loss 0.001513, val loss drop 0.0019 to 0.0018\n",
      "[29] : train loss 0.001079, val loss drop 0.0018 to 0.0014\n",
      "[57] : train loss 0.001189, val loss drop 0.0014 to 0.0013\n",
      "[61] : train loss 0.001053, val loss drop 0.0013 to 0.0012\n",
      "[67] : train loss 0.000822, val loss drop 0.0012 to 0.0010\n",
      "[100] : train loss 0.001337, val loss drop 0.0010 to 0.0007\n",
      "fold 8\n",
      "[1] : train loss 2.845879, val loss drop 10000000.0000 to 2.2832\n",
      "[2] : train loss 1.637352, val loss drop 2.2832 to 1.0342\n",
      "[3] : train loss 0.587772, val loss drop 1.0342 to 0.1911\n",
      "[4] : train loss 0.093528, val loss drop 0.1911 to 0.0798\n",
      "[5] : train loss 0.038641, val loss drop 0.0798 to 0.0418\n",
      "[6] : train loss 0.018775, val loss drop 0.0418 to 0.0208\n",
      "[7] : train loss 0.012878, val loss drop 0.0208 to 0.0093\n",
      "[8] : train loss 0.007994, val loss drop 0.0093 to 0.0085\n",
      "[9] : train loss 0.006803, val loss drop 0.0085 to 0.0075\n",
      "[10] : train loss 0.004832, val loss drop 0.0075 to 0.0060\n",
      "[11] : train loss 0.003186, val loss drop 0.0060 to 0.0045\n",
      "[12] : train loss 0.002445, val loss drop 0.0045 to 0.0034\n",
      "[14] : train loss 0.002305, val loss drop 0.0034 to 0.0026\n",
      "[15] : train loss 0.001444, val loss drop 0.0026 to 0.0025\n",
      "[16] : train loss 0.001593, val loss drop 0.0025 to 0.0019\n",
      "[17] : train loss 0.001352, val loss drop 0.0019 to 0.0015\n",
      "[26] : train loss 0.000836, val loss drop 0.0015 to 0.0013\n",
      "[34] : train loss 0.001464, val loss drop 0.0013 to 0.0010\n",
      "[53] : train loss 0.000722, val loss drop 0.0010 to 0.0009\n",
      "[54] : train loss 0.000732, val loss drop 0.0009 to 0.0007\n",
      "[64] : train loss 0.000462, val loss drop 0.0007 to 0.0007\n",
      "[65] : train loss 0.000374, val loss drop 0.0007 to 0.0006\n",
      "fold 9\n",
      "[1] : train loss 2.673650, val loss drop 10000000.0000 to 1.9810\n",
      "[2] : train loss 0.878264, val loss drop 1.9810 to 0.2856\n",
      "[3] : train loss 0.135248, val loss drop 0.2856 to 0.0990\n",
      "[4] : train loss 0.045670, val loss drop 0.0990 to 0.0625\n",
      "[5] : train loss 0.022879, val loss drop 0.0625 to 0.0288\n",
      "[6] : train loss 0.012967, val loss drop 0.0288 to 0.0168\n",
      "[7] : train loss 0.010157, val loss drop 0.0168 to 0.0087\n",
      "[8] : train loss 0.005419, val loss drop 0.0087 to 0.0054\n",
      "[9] : train loss 0.003241, val loss drop 0.0054 to 0.0041\n",
      "[10] : train loss 0.002886, val loss drop 0.0041 to 0.0029\n",
      "[15] : train loss 0.002234, val loss drop 0.0029 to 0.0021\n",
      "[16] : train loss 0.001839, val loss drop 0.0021 to 0.0017\n",
      "[17] : train loss 0.001520, val loss drop 0.0017 to 0.0016\n",
      "[27] : train loss 0.001452, val loss drop 0.0016 to 0.0014\n",
      "[28] : train loss 0.001206, val loss drop 0.0014 to 0.0010\n",
      "[41] : train loss 0.001070, val loss drop 0.0010 to 0.0009\n",
      "[42] : train loss 0.001049, val loss drop 0.0009 to 0.0008\n",
      "[44] : train loss 0.001267, val loss drop 0.0008 to 0.0008\n",
      "[56] : train loss 0.000461, val loss drop 0.0008 to 0.0008\n",
      "[66] : train loss 0.000858, val loss drop 0.0008 to 0.0007\n",
      "[72] : train loss 0.000700, val loss drop 0.0007 to 0.0005\n",
      "[84] : train loss 0.000535, val loss drop 0.0005 to 0.0004\n",
      "fold 10\n",
      "[1] : train loss 2.531243, val loss drop 10000000.0000 to 1.1670\n",
      "[2] : train loss 0.736207, val loss drop 1.1670 to 0.3307\n",
      "[3] : train loss 0.159589, val loss drop 0.3307 to 0.1133\n",
      "[4] : train loss 0.055902, val loss drop 0.1133 to 0.0420\n",
      "[5] : train loss 0.025616, val loss drop 0.0420 to 0.0245\n",
      "[6] : train loss 0.014513, val loss drop 0.0245 to 0.0121\n",
      "[7] : train loss 0.008238, val loss drop 0.0121 to 0.0083\n",
      "[9] : train loss 0.004406, val loss drop 0.0083 to 0.0056\n",
      "[10] : train loss 0.003032, val loss drop 0.0056 to 0.0039\n",
      "[12] : train loss 0.002754, val loss drop 0.0039 to 0.0033\n",
      "[14] : train loss 0.002816, val loss drop 0.0033 to 0.0024\n",
      "[16] : train loss 0.001452, val loss drop 0.0024 to 0.0023\n",
      "[19] : train loss 0.001291, val loss drop 0.0023 to 0.0015\n",
      "[22] : train loss 0.000834, val loss drop 0.0015 to 0.0015\n",
      "[24] : train loss 0.000961, val loss drop 0.0015 to 0.0012\n",
      "[33] : train loss 0.000643, val loss drop 0.0012 to 0.0012\n",
      "[34] : train loss 0.000874, val loss drop 0.0012 to 0.0010\n",
      "[36] : train loss 0.000601, val loss drop 0.0010 to 0.0008\n",
      "[54] : train loss 0.000614, val loss drop 0.0008 to 0.0006\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 7.756458, val loss drop 10000000.0000 to 2.9085\n",
      "[2] : train loss 0.382560, val loss drop 2.9085 to 0.2460\n",
      "[3] : train loss 0.135207, val loss drop 0.2460 to 0.0653\n",
      "[4] : train loss 0.062416, val loss drop 0.0653 to 0.0639\n",
      "[5] : train loss 0.032859, val loss drop 0.0639 to 0.0184\n",
      "[6] : train loss 0.015477, val loss drop 0.0184 to 0.0127\n",
      "[7] : train loss 0.011364, val loss drop 0.0127 to 0.0064\n",
      "[13] : train loss 0.005109, val loss drop 0.0064 to 0.0042\n",
      "[17] : train loss 0.004440, val loss drop 0.0042 to 0.0039\n",
      "[18] : train loss 0.003634, val loss drop 0.0039 to 0.0035\n",
      "[24] : train loss 0.003100, val loss drop 0.0035 to 0.0024\n",
      "[29] : train loss 0.003864, val loss drop 0.0024 to 0.0022\n",
      "[40] : train loss 0.001794, val loss drop 0.0022 to 0.0019\n",
      "[44] : train loss 0.005217, val loss drop 0.0019 to 0.0017\n",
      "[48] : train loss 0.003617, val loss drop 0.0017 to 0.0017\n",
      "[53] : train loss 0.010765, val loss drop 0.0017 to 0.0015\n",
      "[57] : train loss 0.003529, val loss drop 0.0015 to 0.0015\n",
      "[63] : train loss 0.001153, val loss drop 0.0015 to 0.0013\n",
      "[65] : train loss 0.001049, val loss drop 0.0013 to 0.0013\n",
      "[68] : train loss 0.001459, val loss drop 0.0013 to 0.0009\n",
      "[83] : train loss 0.001534, val loss drop 0.0009 to 0.0006\n",
      "fold 2\n",
      "[1] : train loss 37.323544, val loss drop 10000000.0000 to 2.0691\n",
      "[2] : train loss 0.605267, val loss drop 2.0691 to 0.5998\n",
      "[3] : train loss 0.153422, val loss drop 0.5998 to 0.1369\n",
      "[4] : train loss 0.075985, val loss drop 0.1369 to 0.0859\n",
      "[6] : train loss 0.043197, val loss drop 0.0859 to 0.0346\n",
      "[7] : train loss 0.027547, val loss drop 0.0346 to 0.0267\n",
      "[9] : train loss 0.020700, val loss drop 0.0267 to 0.0255\n",
      "[10] : train loss 0.020413, val loss drop 0.0255 to 0.0192\n",
      "[12] : train loss 0.019624, val loss drop 0.0192 to 0.0151\n",
      "[14] : train loss 0.019134, val loss drop 0.0151 to 0.0128\n",
      "[17] : train loss 0.021580, val loss drop 0.0128 to 0.0119\n",
      "[20] : train loss 0.012078, val loss drop 0.0119 to 0.0109\n",
      "[24] : train loss 0.021569, val loss drop 0.0109 to 0.0097\n",
      "[25] : train loss 0.018214, val loss drop 0.0097 to 0.0078\n",
      "[33] : train loss 0.008193, val loss drop 0.0078 to 0.0073\n",
      "[36] : train loss 0.008344, val loss drop 0.0073 to 0.0051\n",
      "[55] : train loss 0.005727, val loss drop 0.0051 to 0.0044\n",
      "[62] : train loss 0.005746, val loss drop 0.0044 to 0.0040\n",
      "[66] : train loss 0.005493, val loss drop 0.0040 to 0.0039\n",
      "[68] : train loss 0.003967, val loss drop 0.0039 to 0.0030\n",
      "[81] : train loss 0.003236, val loss drop 0.0030 to 0.0029\n",
      "fold 3\n",
      "[1] : train loss 28.274870, val loss drop 10000000.0000 to 4.9341\n",
      "[2] : train loss 0.957620, val loss drop 4.9341 to 0.1478\n",
      "[4] : train loss 0.113439, val loss drop 0.1478 to 0.1068\n",
      "[5] : train loss 0.151404, val loss drop 0.1068 to 0.0424\n",
      "[7] : train loss 0.064830, val loss drop 0.0424 to 0.0349\n",
      "[8] : train loss 0.038979, val loss drop 0.0349 to 0.0200\n",
      "[9] : train loss 0.029824, val loss drop 0.0200 to 0.0199\n",
      "[11] : train loss 0.026882, val loss drop 0.0199 to 0.0156\n",
      "[21] : train loss 0.012488, val loss drop 0.0156 to 0.0112\n",
      "[23] : train loss 0.016279, val loss drop 0.0112 to 0.0106\n",
      "[25] : train loss 0.016857, val loss drop 0.0106 to 0.0075\n",
      "[65] : train loss 0.013041, val loss drop 0.0075 to 0.0043\n",
      "[91] : train loss 0.006317, val loss drop 0.0043 to 0.0031\n",
      "fold 4\n",
      "[1] : train loss 7.612723, val loss drop 10000000.0000 to 0.6022\n",
      "[2] : train loss 0.084408, val loss drop 0.6022 to 0.1134\n",
      "[3] : train loss 0.033184, val loss drop 0.1134 to 0.0306\n",
      "[4] : train loss 0.021052, val loss drop 0.0306 to 0.0164\n",
      "[5] : train loss 0.013602, val loss drop 0.0164 to 0.0128\n",
      "[6] : train loss 0.008633, val loss drop 0.0128 to 0.0061\n",
      "[8] : train loss 0.007250, val loss drop 0.0061 to 0.0053\n",
      "[9] : train loss 0.004351, val loss drop 0.0053 to 0.0036\n",
      "[10] : train loss 0.003826, val loss drop 0.0036 to 0.0032\n",
      "[15] : train loss 0.005069, val loss drop 0.0032 to 0.0022\n",
      "[21] : train loss 0.001763, val loss drop 0.0022 to 0.0021\n",
      "[25] : train loss 0.004712, val loss drop 0.0021 to 0.0019\n",
      "[27] : train loss 0.002028, val loss drop 0.0019 to 0.0018\n",
      "[31] : train loss 0.001872, val loss drop 0.0018 to 0.0012\n",
      "[38] : train loss 0.002764, val loss drop 0.0012 to 0.0011\n",
      "[41] : train loss 0.000706, val loss drop 0.0011 to 0.0011\n",
      "[43] : train loss 0.000598, val loss drop 0.0011 to 0.0011\n",
      "[45] : train loss 0.003048, val loss drop 0.0011 to 0.0010\n",
      "[61] : train loss 0.001748, val loss drop 0.0010 to 0.0009\n",
      "[68] : train loss 0.001683, val loss drop 0.0009 to 0.0009\n",
      "[70] : train loss 0.000628, val loss drop 0.0009 to 0.0008\n",
      "[73] : train loss 0.001593, val loss drop 0.0008 to 0.0008\n",
      "[74] : train loss 0.000830, val loss drop 0.0008 to 0.0007\n",
      "[75] : train loss 0.000605, val loss drop 0.0007 to 0.0006\n",
      "[87] : train loss 0.001108, val loss drop 0.0006 to 0.0006\n",
      "[96] : train loss 0.000937, val loss drop 0.0006 to 0.0005\n",
      "fold 5\n",
      "[1] : train loss 23.805341, val loss drop 10000000.0000 to 0.5857\n",
      "[2] : train loss 0.420656, val loss drop 0.5857 to 0.3811\n",
      "[3] : train loss 0.122726, val loss drop 0.3811 to 0.1212\n",
      "[4] : train loss 0.061203, val loss drop 0.1212 to 0.0390\n",
      "[5] : train loss 0.038956, val loss drop 0.0390 to 0.0357\n",
      "[6] : train loss 0.026139, val loss drop 0.0357 to 0.0171\n",
      "[9] : train loss 0.013978, val loss drop 0.0171 to 0.0114\n",
      "[12] : train loss 0.021082, val loss drop 0.0114 to 0.0109\n",
      "[15] : train loss 0.020674, val loss drop 0.0109 to 0.0074\n",
      "[19] : train loss 0.009593, val loss drop 0.0074 to 0.0069\n",
      "[28] : train loss 0.021447, val loss drop 0.0069 to 0.0065\n",
      "[31] : train loss 0.007430, val loss drop 0.0065 to 0.0057\n",
      "[32] : train loss 0.005594, val loss drop 0.0057 to 0.0052\n",
      "[33] : train loss 0.005072, val loss drop 0.0052 to 0.0050\n",
      "[49] : train loss 0.006422, val loss drop 0.0050 to 0.0041\n",
      "[50] : train loss 0.008841, val loss drop 0.0041 to 0.0037\n",
      "[55] : train loss 0.003064, val loss drop 0.0037 to 0.0033\n",
      "[94] : train loss 0.003688, val loss drop 0.0033 to 0.0032\n",
      "fold 6\n",
      "[1] : train loss 39.428267, val loss drop 10000000.0000 to 0.7071\n",
      "[2] : train loss 0.356323, val loss drop 0.7071 to 0.3110\n",
      "[3] : train loss 0.110254, val loss drop 0.3110 to 0.1231\n",
      "[4] : train loss 0.055039, val loss drop 0.1231 to 0.0373\n",
      "[5] : train loss 0.028822, val loss drop 0.0373 to 0.0264\n",
      "[6] : train loss 0.022869, val loss drop 0.0264 to 0.0227\n",
      "[7] : train loss 0.014565, val loss drop 0.0227 to 0.0127\n",
      "[8] : train loss 0.010528, val loss drop 0.0127 to 0.0115\n",
      "[11] : train loss 0.008801, val loss drop 0.0115 to 0.0086\n",
      "[12] : train loss 0.008117, val loss drop 0.0086 to 0.0066\n",
      "[14] : train loss 0.011165, val loss drop 0.0066 to 0.0057\n",
      "[19] : train loss 0.004611, val loss drop 0.0057 to 0.0043\n",
      "[33] : train loss 0.007916, val loss drop 0.0043 to 0.0035\n",
      "[35] : train loss 0.007215, val loss drop 0.0035 to 0.0033\n",
      "[39] : train loss 0.002150, val loss drop 0.0033 to 0.0032\n",
      "[41] : train loss 0.001591, val loss drop 0.0032 to 0.0029\n",
      "[42] : train loss 0.002982, val loss drop 0.0029 to 0.0024\n",
      "[54] : train loss 0.005410, val loss drop 0.0024 to 0.0024\n",
      "[63] : train loss 0.004534, val loss drop 0.0024 to 0.0022\n",
      "[78] : train loss 0.003452, val loss drop 0.0022 to 0.0021\n",
      "[80] : train loss 0.002311, val loss drop 0.0021 to 0.0020\n",
      "[94] : train loss 0.008498, val loss drop 0.0020 to 0.0017\n",
      "fold 7\n",
      "[1] : train loss 5.894296, val loss drop 10000000.0000 to 0.4194\n",
      "[2] : train loss 0.229870, val loss drop 0.4194 to 0.0935\n",
      "[3] : train loss 0.092238, val loss drop 0.0935 to 0.0514\n",
      "[4] : train loss 0.043723, val loss drop 0.0514 to 0.0506\n",
      "[5] : train loss 0.033907, val loss drop 0.0506 to 0.0166\n",
      "[7] : train loss 0.021631, val loss drop 0.0166 to 0.0098\n",
      "[10] : train loss 0.006068, val loss drop 0.0098 to 0.0091\n",
      "[12] : train loss 0.005576, val loss drop 0.0091 to 0.0054\n",
      "[14] : train loss 0.004840, val loss drop 0.0054 to 0.0040\n",
      "[15] : train loss 0.004918, val loss drop 0.0040 to 0.0038\n",
      "[22] : train loss 0.012651, val loss drop 0.0038 to 0.0037\n",
      "[23] : train loss 0.007087, val loss drop 0.0037 to 0.0031\n",
      "[25] : train loss 0.009369, val loss drop 0.0031 to 0.0028\n",
      "[27] : train loss 0.002897, val loss drop 0.0028 to 0.0022\n",
      "[39] : train loss 0.006015, val loss drop 0.0022 to 0.0021\n",
      "[41] : train loss 0.004045, val loss drop 0.0021 to 0.0017\n",
      "[43] : train loss 0.004300, val loss drop 0.0017 to 0.0017\n",
      "[56] : train loss 0.005109, val loss drop 0.0017 to 0.0015\n",
      "[61] : train loss 0.002749, val loss drop 0.0015 to 0.0015\n",
      "[70] : train loss 0.000855, val loss drop 0.0015 to 0.0012\n",
      "[93] : train loss 0.002448, val loss drop 0.0012 to 0.0009\n",
      "fold 8\n",
      "[1] : train loss 24.452251, val loss drop 10000000.0000 to 0.7320\n",
      "[2] : train loss 0.149285, val loss drop 0.7320 to 0.0586\n",
      "[4] : train loss 0.039483, val loss drop 0.0586 to 0.0250\n",
      "[5] : train loss 0.024223, val loss drop 0.0250 to 0.0131\n",
      "[7] : train loss 0.012588, val loss drop 0.0131 to 0.0125\n",
      "[8] : train loss 0.010849, val loss drop 0.0125 to 0.0107\n",
      "[9] : train loss 0.009899, val loss drop 0.0107 to 0.0092\n",
      "[10] : train loss 0.008010, val loss drop 0.0092 to 0.0084\n",
      "[11] : train loss 0.007197, val loss drop 0.0084 to 0.0070\n",
      "[17] : train loss 0.010761, val loss drop 0.0070 to 0.0045\n",
      "[33] : train loss 0.002813, val loss drop 0.0045 to 0.0037\n",
      "[35] : train loss 0.004386, val loss drop 0.0037 to 0.0035\n",
      "[36] : train loss 0.002118, val loss drop 0.0035 to 0.0028\n",
      "[40] : train loss 0.002606, val loss drop 0.0028 to 0.0023\n",
      "[57] : train loss 0.002071, val loss drop 0.0023 to 0.0022\n",
      "[61] : train loss 0.003435, val loss drop 0.0022 to 0.0018\n",
      "[62] : train loss 0.001999, val loss drop 0.0018 to 0.0014\n",
      "[74] : train loss 0.001334, val loss drop 0.0014 to 0.0012\n",
      "[76] : train loss 0.003325, val loss drop 0.0012 to 0.0012\n",
      "[81] : train loss 0.002441, val loss drop 0.0012 to 0.0011\n",
      "[95] : train loss 0.002068, val loss drop 0.0011 to 0.0009\n",
      "fold 9\n",
      "[1] : train loss 5.289181, val loss drop 10000000.0000 to 0.7511\n",
      "[2] : train loss 0.431687, val loss drop 0.7511 to 0.3559\n",
      "[3] : train loss 0.158664, val loss drop 0.3559 to 0.0314\n",
      "[4] : train loss 0.052110, val loss drop 0.0314 to 0.0298\n",
      "[5] : train loss 0.028860, val loss drop 0.0298 to 0.0197\n",
      "[6] : train loss 0.018604, val loss drop 0.0197 to 0.0192\n",
      "[7] : train loss 0.013221, val loss drop 0.0192 to 0.0110\n",
      "[12] : train loss 0.011482, val loss drop 0.0110 to 0.0071\n",
      "[13] : train loss 0.004479, val loss drop 0.0071 to 0.0059\n",
      "[15] : train loss 0.028790, val loss drop 0.0059 to 0.0051\n",
      "[28] : train loss 0.018885, val loss drop 0.0051 to 0.0050\n",
      "[31] : train loss 0.011567, val loss drop 0.0050 to 0.0041\n",
      "[32] : train loss 0.003112, val loss drop 0.0041 to 0.0037\n",
      "[33] : train loss 0.003100, val loss drop 0.0037 to 0.0034\n",
      "[35] : train loss 0.006261, val loss drop 0.0034 to 0.0031\n",
      "[39] : train loss 0.008532, val loss drop 0.0031 to 0.0023\n",
      "[53] : train loss 0.002651, val loss drop 0.0023 to 0.0017\n",
      "[54] : train loss 0.001710, val loss drop 0.0017 to 0.0012\n",
      "fold 10\n",
      "[1] : train loss 8.638377, val loss drop 10000000.0000 to 2.0573\n",
      "[2] : train loss 0.495480, val loss drop 2.0573 to 1.0484\n",
      "[3] : train loss 0.152122, val loss drop 1.0484 to 0.2675\n",
      "[4] : train loss 0.109865, val loss drop 0.2675 to 0.0403\n",
      "[5] : train loss 0.042428, val loss drop 0.0403 to 0.0362\n",
      "[6] : train loss 0.029841, val loss drop 0.0362 to 0.0185\n",
      "[7] : train loss 0.018582, val loss drop 0.0185 to 0.0145\n",
      "[8] : train loss 0.013718, val loss drop 0.0145 to 0.0104\n",
      "[21] : train loss 0.010432, val loss drop 0.0104 to 0.0065\n",
      "[22] : train loss 0.006035, val loss drop 0.0065 to 0.0060\n",
      "[31] : train loss 0.005337, val loss drop 0.0060 to 0.0056\n",
      "[32] : train loss 0.006465, val loss drop 0.0056 to 0.0055\n",
      "[35] : train loss 0.003901, val loss drop 0.0055 to 0.0047\n",
      "[37] : train loss 0.003447, val loss drop 0.0047 to 0.0041\n",
      "[39] : train loss 0.003396, val loss drop 0.0041 to 0.0030\n",
      "[50] : train loss 0.005790, val loss drop 0.0030 to 0.0027\n",
      "[64] : train loss 0.004613, val loss drop 0.0027 to 0.0018\n",
      "[69] : train loss 0.002032, val loss drop 0.0018 to 0.0016\n",
      "[86] : train loss 0.003616, val loss drop 0.0016 to 0.0013\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 55.147565, val loss drop 10000000.0000 to 31.5879\n",
      "[2] : train loss 10.843457, val loss drop 31.5879 to 10.0647\n",
      "[3] : train loss 2.973846, val loss drop 10.0647 to 8.9463\n",
      "[4] : train loss 1.652302, val loss drop 8.9463 to 2.5977\n",
      "[5] : train loss 1.038845, val loss drop 2.5977 to 1.1074\n",
      "[6] : train loss 0.860674, val loss drop 1.1074 to 0.7918\n",
      "[7] : train loss 0.571903, val loss drop 0.7918 to 0.7349\n",
      "[12] : train loss 0.338975, val loss drop 0.7349 to 0.2392\n",
      "[13] : train loss 0.281312, val loss drop 0.2392 to 0.2089\n",
      "[21] : train loss 0.133957, val loss drop 0.2089 to 0.1254\n",
      "[26] : train loss 0.333841, val loss drop 0.1254 to 0.1083\n",
      "[27] : train loss 0.342485, val loss drop 0.1083 to 0.0661\n",
      "[43] : train loss 0.075370, val loss drop 0.0661 to 0.0589\n",
      "[47] : train loss 0.177159, val loss drop 0.0589 to 0.0426\n",
      "[48] : train loss 0.185202, val loss drop 0.0426 to 0.0292\n",
      "[58] : train loss 0.084548, val loss drop 0.0292 to 0.0224\n",
      "fold 2\n",
      "[1] : train loss 37.388390, val loss drop 10000000.0000 to 11.1222\n",
      "[4] : train loss 0.953503, val loss drop 11.1222 to 1.7389\n",
      "[5] : train loss 0.693910, val loss drop 1.7389 to 0.2972\n",
      "[9] : train loss 0.411681, val loss drop 0.2972 to 0.2954\n",
      "[10] : train loss 0.297641, val loss drop 0.2954 to 0.2528\n",
      "[11] : train loss 0.202463, val loss drop 0.2528 to 0.2335\n",
      "[17] : train loss 0.391097, val loss drop 0.2335 to 0.1477\n",
      "[22] : train loss 0.187346, val loss drop 0.1477 to 0.1375\n",
      "[26] : train loss 0.190333, val loss drop 0.1375 to 0.0507\n",
      "[45] : train loss 0.115913, val loss drop 0.0507 to 0.0315\n",
      "[92] : train loss 0.123437, val loss drop 0.0315 to 0.0201\n",
      "fold 3\n",
      "[1] : train loss 37.367034, val loss drop 10000000.0000 to 5.7536\n",
      "[3] : train loss 1.602469, val loss drop 5.7536 to 4.4427\n",
      "[4] : train loss 0.931160, val loss drop 4.4427 to 1.6563\n",
      "[5] : train loss 0.617542, val loss drop 1.6563 to 0.3722\n",
      "[8] : train loss 0.449062, val loss drop 0.3722 to 0.3387\n",
      "[10] : train loss 0.227071, val loss drop 0.3387 to 0.1977\n",
      "[12] : train loss 0.281192, val loss drop 0.1977 to 0.1448\n",
      "[13] : train loss 0.376494, val loss drop 0.1448 to 0.0778\n",
      "[22] : train loss 0.142462, val loss drop 0.0778 to 0.0607\n",
      "[23] : train loss 0.144221, val loss drop 0.0607 to 0.0450\n",
      "[27] : train loss 0.185081, val loss drop 0.0450 to 0.0434\n",
      "[38] : train loss 0.283072, val loss drop 0.0434 to 0.0321\n",
      "[52] : train loss 0.170816, val loss drop 0.0321 to 0.0213\n",
      "[53] : train loss 0.047395, val loss drop 0.0213 to 0.0131\n",
      "fold 4\n",
      "[1] : train loss 29.130257, val loss drop 10000000.0000 to 5.3360\n",
      "[2] : train loss 3.460394, val loss drop 5.3360 to 1.5435\n",
      "[5] : train loss 0.637453, val loss drop 1.5435 to 0.9762\n",
      "[6] : train loss 0.364662, val loss drop 0.9762 to 0.3035\n",
      "[7] : train loss 0.215925, val loss drop 0.3035 to 0.2509\n",
      "[8] : train loss 0.245827, val loss drop 0.2509 to 0.1003\n",
      "[10] : train loss 0.450374, val loss drop 0.1003 to 0.0894\n",
      "[23] : train loss 0.166416, val loss drop 0.0894 to 0.0428\n",
      "[25] : train loss 0.311354, val loss drop 0.0428 to 0.0426\n",
      "[32] : train loss 0.240730, val loss drop 0.0426 to 0.0281\n",
      "[42] : train loss 0.138523, val loss drop 0.0281 to 0.0270\n",
      "[58] : train loss 0.236741, val loss drop 0.0270 to 0.0208\n",
      "fold 5\n",
      "[1] : train loss 35.788874, val loss drop 10000000.0000 to 8.4986\n",
      "[3] : train loss 1.816140, val loss drop 8.4986 to 7.5619\n",
      "[4] : train loss 0.969541, val loss drop 7.5619 to 1.7621\n",
      "[5] : train loss 0.743220, val loss drop 1.7621 to 0.7437\n",
      "[6] : train loss 0.674344, val loss drop 0.7437 to 0.4091\n",
      "[7] : train loss 0.389749, val loss drop 0.4091 to 0.3764\n",
      "[8] : train loss 0.367725, val loss drop 0.3764 to 0.2464\n",
      "[10] : train loss 0.404399, val loss drop 0.2464 to 0.1594\n",
      "[15] : train loss 0.312700, val loss drop 0.1594 to 0.1402\n",
      "[21] : train loss 0.236260, val loss drop 0.1402 to 0.0817\n",
      "[27] : train loss 0.230597, val loss drop 0.0817 to 0.0724\n",
      "[28] : train loss 0.339932, val loss drop 0.0724 to 0.0636\n",
      "[47] : train loss 0.095314, val loss drop 0.0636 to 0.0493\n",
      "[50] : train loss 0.138043, val loss drop 0.0493 to 0.0199\n",
      "[71] : train loss 0.121138, val loss drop 0.0199 to 0.0189\n",
      "[97] : train loss 0.070025, val loss drop 0.0189 to 0.0178\n",
      "fold 6\n",
      "[1] : train loss 35.670018, val loss drop 10000000.0000 to 22.6340\n",
      "[2] : train loss 3.039388, val loss drop 22.6340 to 15.9079\n",
      "[3] : train loss 1.521822, val loss drop 15.9079 to 11.9030\n",
      "[4] : train loss 1.057500, val loss drop 11.9030 to 2.1953\n",
      "[5] : train loss 0.676250, val loss drop 2.1953 to 0.4533\n",
      "[6] : train loss 0.627849, val loss drop 0.4533 to 0.3688\n",
      "[11] : train loss 0.381988, val loss drop 0.3688 to 0.2631\n",
      "[12] : train loss 0.377645, val loss drop 0.2631 to 0.1810\n",
      "[13] : train loss 0.230061, val loss drop 0.1810 to 0.1069\n",
      "[17] : train loss 0.303231, val loss drop 0.1069 to 0.0785\n",
      "[23] : train loss 0.196047, val loss drop 0.0785 to 0.0604\n",
      "[25] : train loss 0.217192, val loss drop 0.0604 to 0.0597\n",
      "[53] : train loss 0.195803, val loss drop 0.0597 to 0.0449\n",
      "[66] : train loss 0.268885, val loss drop 0.0449 to 0.0371\n",
      "[72] : train loss 0.125436, val loss drop 0.0371 to 0.0292\n",
      "[82] : train loss 0.160653, val loss drop 0.0292 to 0.0230\n",
      "[87] : train loss 0.097864, val loss drop 0.0230 to 0.0196\n",
      "fold 7\n",
      "[1] : train loss 30.588801, val loss drop 10000000.0000 to 19.3154\n",
      "[2] : train loss 3.660666, val loss drop 19.3154 to 3.8063\n",
      "[3] : train loss 1.319029, val loss drop 3.8063 to 2.2064\n",
      "[4] : train loss 0.787705, val loss drop 2.2064 to 1.7756\n",
      "[5] : train loss 1.015486, val loss drop 1.7756 to 0.4931\n",
      "[12] : train loss 0.351897, val loss drop 0.4931 to 0.2277\n",
      "[16] : train loss 0.168776, val loss drop 0.2277 to 0.2076\n",
      "[17] : train loss 0.136252, val loss drop 0.2076 to 0.0899\n",
      "[21] : train loss 0.152904, val loss drop 0.0899 to 0.0709\n",
      "[23] : train loss 0.163263, val loss drop 0.0709 to 0.0582\n",
      "[29] : train loss 0.165479, val loss drop 0.0582 to 0.0564\n",
      "[42] : train loss 0.108472, val loss drop 0.0564 to 0.0334\n",
      "[46] : train loss 0.101615, val loss drop 0.0334 to 0.0195\n",
      "[49] : train loss 0.131091, val loss drop 0.0195 to 0.0168\n",
      "fold 8\n",
      "[1] : train loss 25.827000, val loss drop 10000000.0000 to 3.3155\n",
      "[3] : train loss 1.299391, val loss drop 3.3155 to 1.0996\n",
      "[4] : train loss 0.704283, val loss drop 1.0996 to 0.5351\n",
      "[6] : train loss 0.303059, val loss drop 0.5351 to 0.2598\n",
      "[12] : train loss 0.259895, val loss drop 0.2598 to 0.1804\n",
      "[15] : train loss 0.193743, val loss drop 0.1804 to 0.1381\n",
      "[16] : train loss 0.265523, val loss drop 0.1381 to 0.0462\n",
      "[32] : train loss 0.150221, val loss drop 0.0462 to 0.0420\n",
      "[33] : train loss 0.094832, val loss drop 0.0420 to 0.0304\n",
      "[68] : train loss 0.102964, val loss drop 0.0304 to 0.0178\n",
      "[73] : train loss 0.134961, val loss drop 0.0178 to 0.0118\n",
      "fold 9\n",
      "[1] : train loss 40.264771, val loss drop 10000000.0000 to 10.0633\n",
      "[2] : train loss 5.374436, val loss drop 10.0633 to 3.2411\n",
      "[3] : train loss 2.212381, val loss drop 3.2411 to 2.4029\n",
      "[4] : train loss 0.921296, val loss drop 2.4029 to 2.0559\n",
      "[5] : train loss 0.703901, val loss drop 2.0559 to 0.8872\n",
      "[6] : train loss 0.652182, val loss drop 0.8872 to 0.6459\n",
      "[9] : train loss 0.308778, val loss drop 0.6459 to 0.5626\n",
      "[13] : train loss 0.222069, val loss drop 0.5626 to 0.2933\n",
      "[14] : train loss 0.174952, val loss drop 0.2933 to 0.1811\n",
      "[17] : train loss 0.304774, val loss drop 0.1811 to 0.1581\n",
      "[19] : train loss 0.225717, val loss drop 0.1581 to 0.1241\n",
      "[29] : train loss 0.482843, val loss drop 0.1241 to 0.0376\n",
      "[45] : train loss 0.114072, val loss drop 0.0376 to 0.0243\n",
      "[51] : train loss 0.145875, val loss drop 0.0243 to 0.0228\n",
      "[60] : train loss 0.073809, val loss drop 0.0228 to 0.0219\n",
      "fold 10\n",
      "[1] : train loss 28.357469, val loss drop 10000000.0000 to 8.2383\n",
      "[2] : train loss 3.580619, val loss drop 8.2383 to 1.9003\n",
      "[3] : train loss 1.155873, val loss drop 1.9003 to 1.8121\n",
      "[5] : train loss 0.726399, val loss drop 1.8121 to 0.3367\n",
      "[6] : train loss 0.359595, val loss drop 0.3367 to 0.2242\n",
      "[8] : train loss 0.851760, val loss drop 0.2242 to 0.1900\n",
      "[12] : train loss 0.379205, val loss drop 0.1900 to 0.1595\n",
      "[14] : train loss 0.217314, val loss drop 0.1595 to 0.1514\n",
      "[15] : train loss 0.173918, val loss drop 0.1514 to 0.1336\n",
      "[16] : train loss 0.212578, val loss drop 0.1336 to 0.0500\n",
      "[30] : train loss 0.076555, val loss drop 0.0500 to 0.0333\n",
      "[45] : train loss 0.076492, val loss drop 0.0333 to 0.0325\n",
      "[68] : train loss 0.212246, val loss drop 0.0325 to 0.0222\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './model/20200627-210501/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-263.674805</td>\n",
       "      <td>-41.601894</td>\n",
       "      <td>112.711327</td>\n",
       "      <td>0.442152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>316.673340</td>\n",
       "      <td>-283.934509</td>\n",
       "      <td>89.251762</td>\n",
       "      <td>0.477493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-236.339676</td>\n",
       "      <td>129.708801</td>\n",
       "      <td>30.965876</td>\n",
       "      <td>0.368679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>156.548706</td>\n",
       "      <td>273.805786</td>\n",
       "      <td>28.209118</td>\n",
       "      <td>0.394775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-168.113861</td>\n",
       "      <td>184.381714</td>\n",
       "      <td>132.927673</td>\n",
       "      <td>0.435188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -263.674805  -41.601894  112.711327  0.442152\n",
       "1  2801  316.673340 -283.934509   89.251762  0.477493\n",
       "2  2802 -236.339676  129.708801   30.965876  0.368679\n",
       "3  2803  156.548706  273.805786   28.209118  0.394775\n",
       "4  2804 -168.113861  184.381714  132.927673  0.435188"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, 'submit.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

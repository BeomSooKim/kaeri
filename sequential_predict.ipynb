{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.target[index]\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)\n",
    "        #return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sequential predict(XY-> V-> M)\n",
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "\n",
    "# make dataset\n",
    "    train_target = target[list(name)].values#.astype(np.float32)\n",
    "\n",
    "    # trainx, valx, trainy, valy = train_test_split(train_f, train_target, test_size = 0.2, shuffle = True, random_state = 38)\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        n_features = feature.shape[-1]\n",
    "        #fc = classifier([128, 64, 32, 16], input_size = 256*2*5, output_size = len(name))\n",
    "        #conv = conv_block([32, 64, 128, 256, 256, 256, 256], [1, 375, 5], (3, 1))\n",
    "        conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (3, 1))\n",
    "        fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        model = cnn_model(conv, fc)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            model.train()\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            model.eval()\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.560765, val loss drop 10000000.0000 to 1.9493\n",
      "[2] : train loss 1.150106, val loss drop 1.9493 to 0.6211\n",
      "[3] : train loss 0.426928, val loss drop 0.6211 to 0.1966\n",
      "[4] : train loss 0.105472, val loss drop 0.1966 to 0.0751\n",
      "[5] : train loss 0.033847, val loss drop 0.0751 to 0.0472\n",
      "[6] : train loss 0.023009, val loss drop 0.0472 to 0.0175\n",
      "[7] : train loss 0.011714, val loss drop 0.0175 to 0.0105\n",
      "[8] : train loss 0.007423, val loss drop 0.0105 to 0.0086\n",
      "[9] : train loss 0.004749, val loss drop 0.0086 to 0.0052\n",
      "[11] : train loss 0.002415, val loss drop 0.0052 to 0.0033\n",
      "[12] : train loss 0.002308, val loss drop 0.0033 to 0.0032\n",
      "[13] : train loss 0.002254, val loss drop 0.0032 to 0.0023\n",
      "[19] : train loss 0.001162, val loss drop 0.0023 to 0.0022\n",
      "[20] : train loss 0.000986, val loss drop 0.0022 to 0.0021\n",
      "[28] : train loss 0.000915, val loss drop 0.0021 to 0.0012\n",
      "[58] : train loss 0.000484, val loss drop 0.0012 to 0.0010\n",
      "[61] : train loss 0.000456, val loss drop 0.0010 to 0.0006\n",
      "[88] : train loss 0.000769, val loss drop 0.0006 to 0.0005\n",
      "fold 2\n",
      "[1] : train loss 2.578528, val loss drop 10000000.0000 to 1.2724\n",
      "[2] : train loss 0.730499, val loss drop 1.2724 to 0.1965\n",
      "[3] : train loss 0.144067, val loss drop 0.1965 to 0.1114\n",
      "[4] : train loss 0.049320, val loss drop 0.1114 to 0.1088\n",
      "[5] : train loss 0.021169, val loss drop 0.1088 to 0.0255\n",
      "[6] : train loss 0.011415, val loss drop 0.0255 to 0.0077\n",
      "[8] : train loss 0.004487, val loss drop 0.0077 to 0.0052\n",
      "[9] : train loss 0.003196, val loss drop 0.0052 to 0.0034\n",
      "[10] : train loss 0.002705, val loss drop 0.0034 to 0.0027\n",
      "[14] : train loss 0.001364, val loss drop 0.0027 to 0.0014\n",
      "[19] : train loss 0.001472, val loss drop 0.0014 to 0.0012\n",
      "[22] : train loss 0.000812, val loss drop 0.0012 to 0.0010\n",
      "[23] : train loss 0.000741, val loss drop 0.0010 to 0.0009\n",
      "[25] : train loss 0.000604, val loss drop 0.0009 to 0.0009\n",
      "[28] : train loss 0.000619, val loss drop 0.0009 to 0.0007\n",
      "[38] : train loss 0.000490, val loss drop 0.0007 to 0.0006\n",
      "[39] : train loss 0.000493, val loss drop 0.0006 to 0.0004\n",
      "[62] : train loss 0.000268, val loss drop 0.0004 to 0.0004\n",
      "[92] : train loss 0.000247, val loss drop 0.0004 to 0.0004\n",
      "[96] : train loss 0.000332, val loss drop 0.0004 to 0.0004\n",
      "fold 3\n",
      "[1] : train loss 2.273003, val loss drop 10000000.0000 to 1.0079\n",
      "[2] : train loss 0.477058, val loss drop 1.0079 to 0.2696\n",
      "[3] : train loss 0.126507, val loss drop 0.2696 to 0.0981\n",
      "[4] : train loss 0.041850, val loss drop 0.0981 to 0.0433\n",
      "[5] : train loss 0.017575, val loss drop 0.0433 to 0.0178\n",
      "[6] : train loss 0.009351, val loss drop 0.0178 to 0.0120\n",
      "[8] : train loss 0.004238, val loss drop 0.0120 to 0.0109\n",
      "[9] : train loss 0.004628, val loss drop 0.0109 to 0.0052\n",
      "[10] : train loss 0.003347, val loss drop 0.0052 to 0.0051\n",
      "[11] : train loss 0.002201, val loss drop 0.0051 to 0.0039\n",
      "[13] : train loss 0.001828, val loss drop 0.0039 to 0.0026\n",
      "[21] : train loss 0.001858, val loss drop 0.0026 to 0.0015\n",
      "[23] : train loss 0.000903, val loss drop 0.0015 to 0.0009\n",
      "[41] : train loss 0.001481, val loss drop 0.0009 to 0.0009\n",
      "[47] : train loss 0.000422, val loss drop 0.0009 to 0.0008\n",
      "[48] : train loss 0.000773, val loss drop 0.0008 to 0.0007\n",
      "[58] : train loss 0.000905, val loss drop 0.0007 to 0.0006\n",
      "[90] : train loss 0.000750, val loss drop 0.0006 to 0.0006\n",
      "[95] : train loss 0.000473, val loss drop 0.0006 to 0.0003\n",
      "fold 4\n",
      "[1] : train loss 2.561451, val loss drop 10000000.0000 to 1.4375\n",
      "[2] : train loss 0.820959, val loss drop 1.4375 to 0.3052\n",
      "[3] : train loss 0.165613, val loss drop 0.3052 to 0.0806\n",
      "[5] : train loss 0.021103, val loss drop 0.0806 to 0.0224\n",
      "[6] : train loss 0.011414, val loss drop 0.0224 to 0.0126\n",
      "[8] : train loss 0.005192, val loss drop 0.0126 to 0.0033\n",
      "[9] : train loss 0.003476, val loss drop 0.0033 to 0.0024\n",
      "[14] : train loss 0.001353, val loss drop 0.0024 to 0.0017\n",
      "[15] : train loss 0.001283, val loss drop 0.0017 to 0.0013\n",
      "[27] : train loss 0.000645, val loss drop 0.0013 to 0.0012\n",
      "[33] : train loss 0.000899, val loss drop 0.0012 to 0.0011\n",
      "[39] : train loss 0.000714, val loss drop 0.0011 to 0.0010\n",
      "[45] : train loss 0.001069, val loss drop 0.0010 to 0.0009\n",
      "[48] : train loss 0.000406, val loss drop 0.0009 to 0.0005\n",
      "[55] : train loss 0.000634, val loss drop 0.0005 to 0.0005\n",
      "[71] : train loss 0.001201, val loss drop 0.0005 to 0.0004\n",
      "[74] : train loss 0.000577, val loss drop 0.0004 to 0.0004\n",
      "[80] : train loss 0.000512, val loss drop 0.0004 to 0.0004\n",
      "fold 5\n",
      "[1] : train loss 2.717066, val loss drop 10000000.0000 to 1.7596\n",
      "[2] : train loss 1.404740, val loss drop 1.7596 to 0.6481\n",
      "[3] : train loss 0.381284, val loss drop 0.6481 to 0.1990\n",
      "[4] : train loss 0.082086, val loss drop 0.1990 to 0.0369\n",
      "[5] : train loss 0.028947, val loss drop 0.0369 to 0.0203\n",
      "[6] : train loss 0.014013, val loss drop 0.0203 to 0.0171\n",
      "[7] : train loss 0.008694, val loss drop 0.0171 to 0.0086\n",
      "[8] : train loss 0.005349, val loss drop 0.0086 to 0.0068\n",
      "[10] : train loss 0.003360, val loss drop 0.0068 to 0.0062\n",
      "[12] : train loss 0.002275, val loss drop 0.0062 to 0.0026\n",
      "[14] : train loss 0.001715, val loss drop 0.0026 to 0.0025\n",
      "[17] : train loss 0.000906, val loss drop 0.0025 to 0.0016\n",
      "[25] : train loss 0.001125, val loss drop 0.0016 to 0.0014\n",
      "[27] : train loss 0.000704, val loss drop 0.0014 to 0.0011\n",
      "[39] : train loss 0.001702, val loss drop 0.0011 to 0.0010\n",
      "[41] : train loss 0.000784, val loss drop 0.0010 to 0.0008\n",
      "[43] : train loss 0.000786, val loss drop 0.0008 to 0.0008\n",
      "[45] : train loss 0.000731, val loss drop 0.0008 to 0.0007\n",
      "[54] : train loss 0.000667, val loss drop 0.0007 to 0.0007\n",
      "[61] : train loss 0.000699, val loss drop 0.0007 to 0.0006\n",
      "[64] : train loss 0.000520, val loss drop 0.0006 to 0.0005\n",
      "[69] : train loss 0.000887, val loss drop 0.0005 to 0.0005\n",
      "[81] : train loss 0.000537, val loss drop 0.0005 to 0.0004\n",
      "fold 6\n",
      "[1] : train loss 2.497210, val loss drop 10000000.0000 to 1.1967\n",
      "[2] : train loss 0.630455, val loss drop 1.1967 to 0.3027\n",
      "[3] : train loss 0.134108, val loss drop 0.3027 to 0.1047\n",
      "[4] : train loss 0.043089, val loss drop 0.1047 to 0.0776\n",
      "[5] : train loss 0.018347, val loss drop 0.0776 to 0.0193\n",
      "[6] : train loss 0.010819, val loss drop 0.0193 to 0.0111\n",
      "[7] : train loss 0.006225, val loss drop 0.0111 to 0.0094\n",
      "[8] : train loss 0.003692, val loss drop 0.0094 to 0.0057\n",
      "[11] : train loss 0.002653, val loss drop 0.0057 to 0.0023\n",
      "[16] : train loss 0.001151, val loss drop 0.0023 to 0.0014\n",
      "[19] : train loss 0.000882, val loss drop 0.0014 to 0.0012\n",
      "[28] : train loss 0.001612, val loss drop 0.0012 to 0.0012\n",
      "[30] : train loss 0.000813, val loss drop 0.0012 to 0.0008\n",
      "[39] : train loss 0.000395, val loss drop 0.0008 to 0.0005\n",
      "[49] : train loss 0.001101, val loss drop 0.0005 to 0.0005\n",
      "[68] : train loss 0.000414, val loss drop 0.0005 to 0.0004\n",
      "fold 7\n",
      "[1] : train loss 2.457448, val loss drop 10000000.0000 to 1.3170\n",
      "[2] : train loss 0.662302, val loss drop 1.3170 to 0.2680\n",
      "[3] : train loss 0.164534, val loss drop 0.2680 to 0.0793\n",
      "[5] : train loss 0.027832, val loss drop 0.0793 to 0.0212\n",
      "[6] : train loss 0.017708, val loss drop 0.0212 to 0.0196\n",
      "[7] : train loss 0.009475, val loss drop 0.0196 to 0.0086\n",
      "[9] : train loss 0.005069, val loss drop 0.0086 to 0.0072\n",
      "[11] : train loss 0.003376, val loss drop 0.0072 to 0.0051\n",
      "[15] : train loss 0.001852, val loss drop 0.0051 to 0.0023\n",
      "[18] : train loss 0.002198, val loss drop 0.0023 to 0.0016\n",
      "[25] : train loss 0.001413, val loss drop 0.0016 to 0.0011\n",
      "[30] : train loss 0.000782, val loss drop 0.0011 to 0.0010\n",
      "[42] : train loss 0.000660, val loss drop 0.0010 to 0.0007\n",
      "[53] : train loss 0.000492, val loss drop 0.0007 to 0.0005\n",
      "[87] : train loss 0.000327, val loss drop 0.0005 to 0.0004\n",
      "fold 8\n",
      "[1] : train loss 2.169017, val loss drop 10000000.0000 to 0.8761\n",
      "[2] : train loss 0.469927, val loss drop 0.8761 to 0.2845\n",
      "[3] : train loss 0.121323, val loss drop 0.2845 to 0.0875\n",
      "[4] : train loss 0.045296, val loss drop 0.0875 to 0.0508\n",
      "[5] : train loss 0.019229, val loss drop 0.0508 to 0.0268\n",
      "[6] : train loss 0.010374, val loss drop 0.0268 to 0.0240\n",
      "[7] : train loss 0.005853, val loss drop 0.0240 to 0.0075\n",
      "[10] : train loss 0.002965, val loss drop 0.0075 to 0.0066\n",
      "[11] : train loss 0.002158, val loss drop 0.0066 to 0.0029\n",
      "[13] : train loss 0.001898, val loss drop 0.0029 to 0.0028\n",
      "[17] : train loss 0.001707, val loss drop 0.0028 to 0.0019\n",
      "[26] : train loss 0.001261, val loss drop 0.0019 to 0.0015\n",
      "[30] : train loss 0.001014, val loss drop 0.0015 to 0.0014\n",
      "[38] : train loss 0.000605, val loss drop 0.0014 to 0.0009\n",
      "[67] : train loss 0.000295, val loss drop 0.0009 to 0.0008\n",
      "[80] : train loss 0.000965, val loss drop 0.0008 to 0.0007\n",
      "[96] : train loss 0.000319, val loss drop 0.0007 to 0.0005\n",
      "fold 9\n",
      "[1] : train loss 2.693764, val loss drop 10000000.0000 to 1.4432\n",
      "[2] : train loss 0.913161, val loss drop 1.4432 to 0.2604\n",
      "[4] : train loss 0.058076, val loss drop 0.2604 to 0.1079\n",
      "[5] : train loss 0.028766, val loss drop 0.1079 to 0.0424\n",
      "[6] : train loss 0.015656, val loss drop 0.0424 to 0.0161\n",
      "[10] : train loss 0.003139, val loss drop 0.0161 to 0.0047\n",
      "[11] : train loss 0.002709, val loss drop 0.0047 to 0.0028\n",
      "[12] : train loss 0.002475, val loss drop 0.0028 to 0.0024\n",
      "[16] : train loss 0.002707, val loss drop 0.0024 to 0.0021\n",
      "[17] : train loss 0.001396, val loss drop 0.0021 to 0.0015\n",
      "[20] : train loss 0.000955, val loss drop 0.0015 to 0.0012\n",
      "[29] : train loss 0.001113, val loss drop 0.0012 to 0.0012\n",
      "[34] : train loss 0.000950, val loss drop 0.0012 to 0.0008\n",
      "[42] : train loss 0.000438, val loss drop 0.0008 to 0.0007\n",
      "[45] : train loss 0.000527, val loss drop 0.0007 to 0.0006\n",
      "[63] : train loss 0.000605, val loss drop 0.0006 to 0.0004\n",
      "fold 10\n",
      "[1] : train loss 2.438571, val loss drop 10000000.0000 to 1.4203\n",
      "[2] : train loss 0.734234, val loss drop 1.4203 to 0.3346\n",
      "[3] : train loss 0.144097, val loss drop 0.3346 to 0.0823\n",
      "[4] : train loss 0.045297, val loss drop 0.0823 to 0.0307\n",
      "[5] : train loss 0.019514, val loss drop 0.0307 to 0.0252\n",
      "[6] : train loss 0.010570, val loss drop 0.0252 to 0.0093\n",
      "[7] : train loss 0.005871, val loss drop 0.0093 to 0.0080\n",
      "[8] : train loss 0.004042, val loss drop 0.0080 to 0.0045\n",
      "[10] : train loss 0.002623, val loss drop 0.0045 to 0.0040\n",
      "[12] : train loss 0.001926, val loss drop 0.0040 to 0.0028\n",
      "[14] : train loss 0.001352, val loss drop 0.0028 to 0.0022\n",
      "[20] : train loss 0.001543, val loss drop 0.0022 to 0.0018\n",
      "[22] : train loss 0.000753, val loss drop 0.0018 to 0.0016\n",
      "[23] : train loss 0.000661, val loss drop 0.0016 to 0.0016\n",
      "[24] : train loss 0.000851, val loss drop 0.0016 to 0.0008\n",
      "[39] : train loss 0.000498, val loss drop 0.0008 to 0.0008\n",
      "[40] : train loss 0.000288, val loss drop 0.0008 to 0.0006\n",
      "[61] : train loss 0.000661, val loss drop 0.0006 to 0.0005\n",
      "[72] : train loss 0.000421, val loss drop 0.0005 to 0.0005\n",
      "[86] : train loss 0.000746, val loss drop 0.0005 to 0.0004\n",
      "[99] : train loss 0.000429, val loss drop 0.0004 to 0.0004\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 15.645872, val loss drop 10000000.0000 to 5.9779\n",
      "[2] : train loss 2.245281, val loss drop 5.9779 to 2.1940\n",
      "[3] : train loss 0.564295, val loss drop 2.1940 to 0.1580\n",
      "[4] : train loss 0.145717, val loss drop 0.1580 to 0.1429\n",
      "[5] : train loss 0.061145, val loss drop 0.1429 to 0.0397\n",
      "[6] : train loss 0.032252, val loss drop 0.0397 to 0.0290\n",
      "[7] : train loss 0.020974, val loss drop 0.0290 to 0.0174\n",
      "[10] : train loss 0.012447, val loss drop 0.0174 to 0.0142\n",
      "[11] : train loss 0.011588, val loss drop 0.0142 to 0.0138\n",
      "[14] : train loss 0.011766, val loss drop 0.0138 to 0.0108\n",
      "[22] : train loss 0.007213, val loss drop 0.0108 to 0.0087\n",
      "[23] : train loss 0.003726, val loss drop 0.0087 to 0.0065\n",
      "[26] : train loss 0.003954, val loss drop 0.0065 to 0.0061\n",
      "[32] : train loss 0.004627, val loss drop 0.0061 to 0.0058\n",
      "[33] : train loss 0.005190, val loss drop 0.0058 to 0.0045\n",
      "[47] : train loss 0.002061, val loss drop 0.0045 to 0.0039\n",
      "[58] : train loss 0.006324, val loss drop 0.0039 to 0.0033\n",
      "[68] : train loss 0.007602, val loss drop 0.0033 to 0.0030\n",
      "[91] : train loss 0.002053, val loss drop 0.0030 to 0.0027\n",
      "fold 2\n",
      "[1] : train loss 17.435384, val loss drop 10000000.0000 to 0.3408\n",
      "[2] : train loss 0.252802, val loss drop 0.3408 to 0.1335\n",
      "[3] : train loss 0.079061, val loss drop 0.1335 to 0.0567\n",
      "[4] : train loss 0.035502, val loss drop 0.0567 to 0.0249\n",
      "[5] : train loss 0.020533, val loss drop 0.0249 to 0.0181\n",
      "[6] : train loss 0.013355, val loss drop 0.0181 to 0.0106\n",
      "[9] : train loss 0.008702, val loss drop 0.0106 to 0.0089\n",
      "[15] : train loss 0.005668, val loss drop 0.0089 to 0.0055\n",
      "[22] : train loss 0.005173, val loss drop 0.0055 to 0.0049\n",
      "[25] : train loss 0.003209, val loss drop 0.0049 to 0.0035\n",
      "[33] : train loss 0.003922, val loss drop 0.0035 to 0.0024\n",
      "[41] : train loss 0.002383, val loss drop 0.0024 to 0.0021\n",
      "[54] : train loss 0.001546, val loss drop 0.0021 to 0.0018\n",
      "[63] : train loss 0.004389, val loss drop 0.0018 to 0.0016\n",
      "[74] : train loss 0.004568, val loss drop 0.0016 to 0.0011\n",
      "fold 3\n",
      "[1] : train loss 256.798415, val loss drop 10000000.0000 to 0.7724\n",
      "[2] : train loss 0.568156, val loss drop 0.7724 to 0.4954\n",
      "[3] : train loss 0.192227, val loss drop 0.4954 to 0.1067\n",
      "[4] : train loss 0.072320, val loss drop 0.1067 to 0.0544\n",
      "[5] : train loss 0.044954, val loss drop 0.0544 to 0.0391\n",
      "[6] : train loss 0.028289, val loss drop 0.0391 to 0.0283\n",
      "[7] : train loss 0.023725, val loss drop 0.0283 to 0.0208\n",
      "[10] : train loss 0.018854, val loss drop 0.0208 to 0.0184\n",
      "[14] : train loss 0.012939, val loss drop 0.0184 to 0.0143\n",
      "[17] : train loss 0.010441, val loss drop 0.0143 to 0.0116\n",
      "[20] : train loss 0.008830, val loss drop 0.0116 to 0.0108\n",
      "[27] : train loss 0.007336, val loss drop 0.0108 to 0.0107\n",
      "[28] : train loss 0.006387, val loss drop 0.0107 to 0.0085\n",
      "[29] : train loss 0.005240, val loss drop 0.0085 to 0.0085\n",
      "[36] : train loss 0.005755, val loss drop 0.0085 to 0.0077\n",
      "[38] : train loss 0.003782, val loss drop 0.0077 to 0.0068\n",
      "[50] : train loss 0.006590, val loss drop 0.0068 to 0.0063\n",
      "[54] : train loss 0.007447, val loss drop 0.0063 to 0.0053\n",
      "[67] : train loss 0.002583, val loss drop 0.0053 to 0.0036\n",
      "fold 4\n",
      "[1] : train loss 16.182902, val loss drop 10000000.0000 to 0.8067\n",
      "[2] : train loss 0.698576, val loss drop 0.8067 to 0.3193\n",
      "[3] : train loss 0.178082, val loss drop 0.3193 to 0.2083\n",
      "[4] : train loss 0.121524, val loss drop 0.2083 to 0.1056\n",
      "[5] : train loss 0.059132, val loss drop 0.1056 to 0.0500\n",
      "[6] : train loss 0.030955, val loss drop 0.0500 to 0.0314\n",
      "[7] : train loss 0.019621, val loss drop 0.0314 to 0.0182\n",
      "[8] : train loss 0.015717, val loss drop 0.0182 to 0.0156\n",
      "[9] : train loss 0.011621, val loss drop 0.0156 to 0.0143\n",
      "[10] : train loss 0.011689, val loss drop 0.0143 to 0.0104\n",
      "[12] : train loss 0.010753, val loss drop 0.0104 to 0.0083\n",
      "[15] : train loss 0.007755, val loss drop 0.0083 to 0.0066\n",
      "[21] : train loss 0.008068, val loss drop 0.0066 to 0.0063\n",
      "[26] : train loss 0.005710, val loss drop 0.0063 to 0.0059\n",
      "[29] : train loss 0.004224, val loss drop 0.0059 to 0.0047\n",
      "[33] : train loss 0.004999, val loss drop 0.0047 to 0.0046\n",
      "[36] : train loss 0.004152, val loss drop 0.0046 to 0.0044\n",
      "[44] : train loss 0.005710, val loss drop 0.0044 to 0.0041\n",
      "[48] : train loss 0.001838, val loss drop 0.0041 to 0.0033\n",
      "[78] : train loss 0.006281, val loss drop 0.0033 to 0.0031\n",
      "[80] : train loss 0.001404, val loss drop 0.0031 to 0.0028\n",
      "[86] : train loss 0.004626, val loss drop 0.0028 to 0.0027\n",
      "[94] : train loss 0.000884, val loss drop 0.0027 to 0.0024\n",
      "fold 5\n",
      "[1] : train loss 22.608295, val loss drop 10000000.0000 to 2.0219\n",
      "[2] : train loss 0.502182, val loss drop 2.0219 to 0.5454\n",
      "[3] : train loss 0.172052, val loss drop 0.5454 to 0.1499\n",
      "[4] : train loss 0.085241, val loss drop 0.1499 to 0.0977\n",
      "[5] : train loss 0.046575, val loss drop 0.0977 to 0.0422\n",
      "[6] : train loss 0.026377, val loss drop 0.0422 to 0.0220\n",
      "[7] : train loss 0.019897, val loss drop 0.0220 to 0.0157\n",
      "[9] : train loss 0.014829, val loss drop 0.0157 to 0.0133\n",
      "[11] : train loss 0.011583, val loss drop 0.0133 to 0.0116\n",
      "[12] : train loss 0.011392, val loss drop 0.0116 to 0.0099\n",
      "[13] : train loss 0.009944, val loss drop 0.0099 to 0.0094\n",
      "[19] : train loss 0.009517, val loss drop 0.0094 to 0.0087\n",
      "[20] : train loss 0.006766, val loss drop 0.0087 to 0.0071\n",
      "[27] : train loss 0.010459, val loss drop 0.0071 to 0.0068\n",
      "[31] : train loss 0.010341, val loss drop 0.0068 to 0.0054\n",
      "[40] : train loss 0.008492, val loss drop 0.0054 to 0.0046\n",
      "[46] : train loss 0.004730, val loss drop 0.0046 to 0.0037\n",
      "[71] : train loss 0.007768, val loss drop 0.0037 to 0.0036\n",
      "[75] : train loss 0.003322, val loss drop 0.0036 to 0.0034\n",
      "[86] : train loss 0.003049, val loss drop 0.0034 to 0.0032\n",
      "[88] : train loss 0.001762, val loss drop 0.0032 to 0.0030\n",
      "[91] : train loss 0.001848, val loss drop 0.0030 to 0.0027\n",
      "[97] : train loss 0.006651, val loss drop 0.0027 to 0.0024\n",
      "[100] : train loss 0.002129, val loss drop 0.0024 to 0.0024\n",
      "fold 6\n",
      "[1] : train loss 100.908081, val loss drop 10000000.0000 to 1.8039\n",
      "[2] : train loss 0.495744, val loss drop 1.8039 to 0.2841\n",
      "[3] : train loss 0.241310, val loss drop 0.2841 to 0.1381\n",
      "[4] : train loss 0.118675, val loss drop 0.1381 to 0.1131\n",
      "[5] : train loss 0.086277, val loss drop 0.1131 to 0.0813\n",
      "[6] : train loss 0.057778, val loss drop 0.0813 to 0.0575\n",
      "[8] : train loss 0.041802, val loss drop 0.0575 to 0.0420\n",
      "[9] : train loss 0.034818, val loss drop 0.0420 to 0.0313\n",
      "[11] : train loss 0.043842, val loss drop 0.0313 to 0.0309\n",
      "[16] : train loss 0.020166, val loss drop 0.0309 to 0.0291\n",
      "[17] : train loss 0.022042, val loss drop 0.0291 to 0.0171\n",
      "[29] : train loss 0.009415, val loss drop 0.0171 to 0.0166\n",
      "[33] : train loss 0.016225, val loss drop 0.0166 to 0.0130\n",
      "[39] : train loss 0.009362, val loss drop 0.0130 to 0.0100\n",
      "[42] : train loss 0.010798, val loss drop 0.0100 to 0.0090\n",
      "[59] : train loss 0.009145, val loss drop 0.0090 to 0.0089\n",
      "[63] : train loss 0.009909, val loss drop 0.0089 to 0.0087\n",
      "[70] : train loss 0.006488, val loss drop 0.0087 to 0.0077\n",
      "[75] : train loss 0.018637, val loss drop 0.0077 to 0.0062\n",
      "fold 7\n",
      "[1] : train loss 16.691569, val loss drop 10000000.0000 to 0.5455\n",
      "[2] : train loss 0.178138, val loss drop 0.5455 to 0.0370\n",
      "[4] : train loss 0.021155, val loss drop 0.0370 to 0.0162\n",
      "[5] : train loss 0.015507, val loss drop 0.0162 to 0.0156\n",
      "[6] : train loss 0.015393, val loss drop 0.0156 to 0.0109\n",
      "[7] : train loss 0.013567, val loss drop 0.0109 to 0.0101\n",
      "[8] : train loss 0.012665, val loss drop 0.0101 to 0.0091\n",
      "[10] : train loss 0.009667, val loss drop 0.0091 to 0.0056\n",
      "[11] : train loss 0.005025, val loss drop 0.0056 to 0.0050\n",
      "[21] : train loss 0.003970, val loss drop 0.0050 to 0.0048\n",
      "[22] : train loss 0.003958, val loss drop 0.0048 to 0.0036\n",
      "[34] : train loss 0.002782, val loss drop 0.0036 to 0.0034\n",
      "[37] : train loss 0.004127, val loss drop 0.0034 to 0.0029\n",
      "[38] : train loss 0.002307, val loss drop 0.0029 to 0.0021\n",
      "[49] : train loss 0.001833, val loss drop 0.0021 to 0.0019\n",
      "[61] : train loss 0.004236, val loss drop 0.0019 to 0.0017\n",
      "[66] : train loss 0.001986, val loss drop 0.0017 to 0.0016\n",
      "[68] : train loss 0.001934, val loss drop 0.0016 to 0.0014\n",
      "[73] : train loss 0.001800, val loss drop 0.0014 to 0.0011\n",
      "[85] : train loss 0.003994, val loss drop 0.0011 to 0.0010\n",
      "[96] : train loss 0.001269, val loss drop 0.0010 to 0.0009\n",
      "[100] : train loss 0.001267, val loss drop 0.0009 to 0.0008\n",
      "fold 8\n",
      "[1] : train loss 15.806626, val loss drop 10000000.0000 to 0.6858\n",
      "[3] : train loss 0.208988, val loss drop 0.6858 to 0.0716\n",
      "[5] : train loss 0.057014, val loss drop 0.0716 to 0.0449\n",
      "[6] : train loss 0.030479, val loss drop 0.0449 to 0.0368\n",
      "[7] : train loss 0.022676, val loss drop 0.0368 to 0.0239\n",
      "[8] : train loss 0.017172, val loss drop 0.0239 to 0.0226\n",
      "[12] : train loss 0.023182, val loss drop 0.0226 to 0.0175\n",
      "[13] : train loss 0.028695, val loss drop 0.0175 to 0.0140\n",
      "[14] : train loss 0.012038, val loss drop 0.0140 to 0.0134\n",
      "[16] : train loss 0.007622, val loss drop 0.0134 to 0.0123\n",
      "[17] : train loss 0.008810, val loss drop 0.0123 to 0.0122\n",
      "[19] : train loss 0.008182, val loss drop 0.0122 to 0.0106\n",
      "[26] : train loss 0.015624, val loss drop 0.0106 to 0.0097\n",
      "[38] : train loss 0.003699, val loss drop 0.0097 to 0.0093\n",
      "[40] : train loss 0.010492, val loss drop 0.0093 to 0.0087\n",
      "[42] : train loss 0.006039, val loss drop 0.0087 to 0.0081\n",
      "[43] : train loss 0.005846, val loss drop 0.0081 to 0.0070\n",
      "[48] : train loss 0.003775, val loss drop 0.0070 to 0.0063\n",
      "[65] : train loss 0.014514, val loss drop 0.0063 to 0.0056\n",
      "[76] : train loss 0.012347, val loss drop 0.0056 to 0.0053\n",
      "[90] : train loss 0.005293, val loss drop 0.0053 to 0.0051\n",
      "[94] : train loss 0.022537, val loss drop 0.0051 to 0.0048\n",
      "fold 9\n",
      "[1] : train loss 20.268288, val loss drop 10000000.0000 to 0.4073\n",
      "[3] : train loss 0.073660, val loss drop 0.4073 to 0.0329\n",
      "[4] : train loss 0.024914, val loss drop 0.0329 to 0.0301\n",
      "[5] : train loss 0.012833, val loss drop 0.0301 to 0.0151\n",
      "[6] : train loss 0.008978, val loss drop 0.0151 to 0.0083\n",
      "[7] : train loss 0.007039, val loss drop 0.0083 to 0.0053\n",
      "[11] : train loss 0.004417, val loss drop 0.0053 to 0.0042\n",
      "[13] : train loss 0.004654, val loss drop 0.0042 to 0.0039\n",
      "[14] : train loss 0.004379, val loss drop 0.0039 to 0.0029\n",
      "[17] : train loss 0.002660, val loss drop 0.0029 to 0.0020\n",
      "[18] : train loss 0.002620, val loss drop 0.0020 to 0.0019\n",
      "[28] : train loss 0.001674, val loss drop 0.0019 to 0.0018\n",
      "[29] : train loss 0.001735, val loss drop 0.0018 to 0.0017\n",
      "[30] : train loss 0.001607, val loss drop 0.0017 to 0.0016\n",
      "[31] : train loss 0.001665, val loss drop 0.0016 to 0.0015\n",
      "[35] : train loss 0.002221, val loss drop 0.0015 to 0.0015\n",
      "[37] : train loss 0.002893, val loss drop 0.0015 to 0.0013\n",
      "[46] : train loss 0.001164, val loss drop 0.0013 to 0.0011\n",
      "[50] : train loss 0.001633, val loss drop 0.0011 to 0.0010\n",
      "[51] : train loss 0.000851, val loss drop 0.0010 to 0.0009\n",
      "[57] : train loss 0.002602, val loss drop 0.0009 to 0.0009\n",
      "[62] : train loss 0.001528, val loss drop 0.0009 to 0.0008\n",
      "[67] : train loss 0.000951, val loss drop 0.0008 to 0.0007\n",
      "[73] : train loss 0.001747, val loss drop 0.0007 to 0.0007\n",
      "[79] : train loss 0.001134, val loss drop 0.0007 to 0.0007\n",
      "[81] : train loss 0.001612, val loss drop 0.0007 to 0.0006\n",
      "fold 10\n",
      "[1] : train loss 16.742530, val loss drop 10000000.0000 to 0.4836\n",
      "[2] : train loss 0.319761, val loss drop 0.4836 to 0.4189\n",
      "[3] : train loss 0.179518, val loss drop 0.4189 to 0.1176\n",
      "[4] : train loss 0.089216, val loss drop 0.1176 to 0.0440\n",
      "[6] : train loss 0.033650, val loss drop 0.0440 to 0.0266\n",
      "[7] : train loss 0.019198, val loss drop 0.0266 to 0.0151\n",
      "[10] : train loss 0.018712, val loss drop 0.0151 to 0.0132\n",
      "[11] : train loss 0.012604, val loss drop 0.0132 to 0.0131\n",
      "[12] : train loss 0.010305, val loss drop 0.0131 to 0.0118\n",
      "[14] : train loss 0.009980, val loss drop 0.0118 to 0.0100\n",
      "[19] : train loss 0.014231, val loss drop 0.0100 to 0.0072\n",
      "[29] : train loss 0.010609, val loss drop 0.0072 to 0.0062\n",
      "[32] : train loss 0.003787, val loss drop 0.0062 to 0.0051\n",
      "[40] : train loss 0.011766, val loss drop 0.0051 to 0.0051\n",
      "[50] : train loss 0.003615, val loss drop 0.0051 to 0.0047\n",
      "[51] : train loss 0.002502, val loss drop 0.0047 to 0.0035\n",
      "[52] : train loss 0.005584, val loss drop 0.0035 to 0.0035\n",
      "[71] : train loss 0.008609, val loss drop 0.0035 to 0.0030\n",
      "[87] : train loss 0.007422, val loss drop 0.0030 to 0.0026\n",
      "[92] : train loss 0.001703, val loss drop 0.0026 to 0.0024\n",
      "[93] : train loss 0.000876, val loss drop 0.0024 to 0.0024\n"
     ]
    }
   ],
   "source": [
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 36.495263, val loss drop 10000000.0000 to 7.0307\n",
      "[3] : train loss 1.723455, val loss drop 7.0307 to 3.3378\n",
      "[5] : train loss 0.679273, val loss drop 3.3378 to 1.9550\n",
      "[6] : train loss 0.808075, val loss drop 1.9550 to 0.3929\n",
      "[9] : train loss 0.278975, val loss drop 0.3929 to 0.2885\n",
      "[14] : train loss 0.344460, val loss drop 0.2885 to 0.1133\n",
      "[18] : train loss 0.244292, val loss drop 0.1133 to 0.1099\n",
      "[19] : train loss 0.223345, val loss drop 0.1099 to 0.0907\n",
      "[22] : train loss 0.373950, val loss drop 0.0907 to 0.0860\n",
      "[25] : train loss 0.127380, val loss drop 0.0860 to 0.0714\n",
      "[38] : train loss 0.179388, val loss drop 0.0714 to 0.0523\n",
      "[59] : train loss 0.103606, val loss drop 0.0523 to 0.0477\n",
      "[66] : train loss 0.464927, val loss drop 0.0477 to 0.0442\n",
      "[92] : train loss 0.044855, val loss drop 0.0442 to 0.0409\n",
      "[95] : train loss 0.110060, val loss drop 0.0409 to 0.0379\n",
      "[98] : train loss 0.080377, val loss drop 0.0379 to 0.0317\n",
      "[100] : train loss 0.085395, val loss drop 0.0317 to 0.0258\n",
      "fold 2\n",
      "[1] : train loss 37.067645, val loss drop 10000000.0000 to 12.8817\n",
      "[2] : train loss 5.038956, val loss drop 12.8817 to 9.7241\n",
      "[3] : train loss 2.069760, val loss drop 9.7241 to 2.1506\n",
      "[4] : train loss 0.937837, val loss drop 2.1506 to 1.1307\n",
      "[5] : train loss 0.651153, val loss drop 1.1307 to 0.4166\n",
      "[9] : train loss 0.445886, val loss drop 0.4166 to 0.3411\n",
      "[12] : train loss 0.297432, val loss drop 0.3411 to 0.1896\n",
      "[19] : train loss 0.226265, val loss drop 0.1896 to 0.1586\n",
      "[21] : train loss 0.284637, val loss drop 0.1586 to 0.1177\n",
      "[22] : train loss 0.172252, val loss drop 0.1177 to 0.0898\n",
      "[27] : train loss 0.138479, val loss drop 0.0898 to 0.0665\n",
      "[32] : train loss 0.202427, val loss drop 0.0665 to 0.0577\n",
      "[34] : train loss 0.125726, val loss drop 0.0577 to 0.0316\n",
      "[87] : train loss 0.198009, val loss drop 0.0316 to 0.0247\n",
      "[100] : train loss 0.094274, val loss drop 0.0247 to 0.0224\n",
      "fold 3\n",
      "[1] : train loss 52.379524, val loss drop 10000000.0000 to 57.3343\n",
      "[2] : train loss 8.899731, val loss drop 57.3343 to 24.6994\n",
      "[3] : train loss 2.709420, val loss drop 24.6994 to 13.3812\n",
      "[4] : train loss 1.353817, val loss drop 13.3812 to 0.9379\n",
      "[5] : train loss 1.176243, val loss drop 0.9379 to 0.4479\n",
      "[6] : train loss 0.675152, val loss drop 0.4479 to 0.4311\n",
      "[11] : train loss 0.282375, val loss drop 0.4311 to 0.2099\n",
      "[12] : train loss 0.259052, val loss drop 0.2099 to 0.1927\n",
      "[13] : train loss 0.260549, val loss drop 0.1927 to 0.1790\n",
      "[14] : train loss 0.173512, val loss drop 0.1790 to 0.1219\n",
      "[17] : train loss 0.294739, val loss drop 0.1219 to 0.0951\n",
      "[24] : train loss 0.170810, val loss drop 0.0951 to 0.0799\n",
      "[36] : train loss 0.196051, val loss drop 0.0799 to 0.0523\n",
      "[57] : train loss 0.275111, val loss drop 0.0523 to 0.0435\n",
      "[63] : train loss 0.143091, val loss drop 0.0435 to 0.0255\n",
      "[74] : train loss 0.170068, val loss drop 0.0255 to 0.0169\n",
      "[75] : train loss 0.065555, val loss drop 0.0169 to 0.0148\n",
      "fold 4\n",
      "[1] : train loss 27.222920, val loss drop 10000000.0000 to 4.6533\n",
      "[3] : train loss 0.880185, val loss drop 4.6533 to 2.6578\n",
      "[4] : train loss 0.738050, val loss drop 2.6578 to 0.4595\n",
      "[6] : train loss 0.648905, val loss drop 0.4595 to 0.2164\n",
      "[7] : train loss 0.292093, val loss drop 0.2164 to 0.1700\n",
      "[10] : train loss 0.228039, val loss drop 0.1700 to 0.1239\n",
      "[17] : train loss 0.321451, val loss drop 0.1239 to 0.1236\n",
      "[19] : train loss 0.206919, val loss drop 0.1236 to 0.0587\n",
      "[40] : train loss 0.400959, val loss drop 0.0587 to 0.0301\n",
      "[55] : train loss 0.118498, val loss drop 0.0301 to 0.0283\n",
      "[95] : train loss 0.065285, val loss drop 0.0283 to 0.0217\n",
      "fold 5\n",
      "[1] : train loss 25.848083, val loss drop 10000000.0000 to 2.8673\n",
      "[2] : train loss 2.680698, val loss drop 2.8673 to 2.1050\n",
      "[5] : train loss 0.540651, val loss drop 2.1050 to 0.6462\n",
      "[6] : train loss 0.343978, val loss drop 0.6462 to 0.2323\n",
      "[7] : train loss 0.249662, val loss drop 0.2323 to 0.2307\n",
      "[8] : train loss 0.198636, val loss drop 0.2307 to 0.2232\n",
      "[9] : train loss 0.192470, val loss drop 0.2232 to 0.1923\n",
      "[10] : train loss 0.374134, val loss drop 0.1923 to 0.1431\n",
      "[13] : train loss 0.323507, val loss drop 0.1431 to 0.1163\n",
      "[25] : train loss 0.188679, val loss drop 0.1163 to 0.0628\n",
      "[35] : train loss 0.146754, val loss drop 0.0628 to 0.0285\n",
      "[42] : train loss 0.133002, val loss drop 0.0285 to 0.0235\n",
      "[46] : train loss 0.186843, val loss drop 0.0235 to 0.0180\n",
      "fold 6\n",
      "[1] : train loss 29.283320, val loss drop 10000000.0000 to 39.1980\n",
      "[2] : train loss 3.284450, val loss drop 39.1980 to 5.9443\n",
      "[4] : train loss 0.749736, val loss drop 5.9443 to 0.6943\n",
      "[5] : train loss 0.731901, val loss drop 0.6943 to 0.2450\n",
      "[6] : train loss 0.578363, val loss drop 0.2450 to 0.1625\n",
      "[8] : train loss 0.223362, val loss drop 0.1625 to 0.1149\n",
      "[15] : train loss 0.204928, val loss drop 0.1149 to 0.0929\n",
      "[21] : train loss 0.361056, val loss drop 0.0929 to 0.0891\n",
      "[27] : train loss 0.172587, val loss drop 0.0891 to 0.0606\n",
      "[40] : train loss 0.063736, val loss drop 0.0606 to 0.0318\n",
      "[42] : train loss 0.301956, val loss drop 0.0318 to 0.0245\n",
      "[54] : train loss 0.234201, val loss drop 0.0245 to 0.0176\n",
      "[73] : train loss 0.090738, val loss drop 0.0176 to 0.0136\n",
      "fold 7\n",
      "[1] : train loss 29.675398, val loss drop 10000000.0000 to 3.2797\n",
      "[4] : train loss 0.771877, val loss drop 3.2797 to 0.4419\n",
      "[8] : train loss 0.640089, val loss drop 0.4419 to 0.1883\n",
      "[10] : train loss 0.369017, val loss drop 0.1883 to 0.1578\n",
      "[12] : train loss 0.280228, val loss drop 0.1578 to 0.1213\n",
      "[16] : train loss 0.334652, val loss drop 0.1213 to 0.1027\n",
      "[26] : train loss 0.232028, val loss drop 0.1027 to 0.0464\n",
      "[33] : train loss 0.124074, val loss drop 0.0464 to 0.0458\n",
      "[36] : train loss 0.128403, val loss drop 0.0458 to 0.0415\n",
      "[50] : train loss 0.302819, val loss drop 0.0415 to 0.0302\n",
      "[58] : train loss 0.248929, val loss drop 0.0302 to 0.0212\n",
      "fold 8\n",
      "[1] : train loss 37.045291, val loss drop 10000000.0000 to 9.8716\n",
      "[3] : train loss 1.298805, val loss drop 9.8716 to 6.4111\n",
      "[4] : train loss 0.802581, val loss drop 6.4111 to 0.8539\n",
      "[5] : train loss 0.439343, val loss drop 0.8539 to 0.5215\n",
      "[6] : train loss 0.307471, val loss drop 0.5215 to 0.3418\n",
      "[8] : train loss 0.398873, val loss drop 0.3418 to 0.2944\n",
      "[9] : train loss 0.242642, val loss drop 0.2944 to 0.0957\n",
      "[19] : train loss 0.212872, val loss drop 0.0957 to 0.0782\n",
      "[23] : train loss 0.254964, val loss drop 0.0782 to 0.0503\n",
      "[29] : train loss 0.205260, val loss drop 0.0503 to 0.0471\n",
      "[58] : train loss 0.066995, val loss drop 0.0471 to 0.0388\n",
      "[59] : train loss 0.178734, val loss drop 0.0388 to 0.0361\n",
      "[79] : train loss 0.111586, val loss drop 0.0361 to 0.0244\n",
      "[80] : train loss 0.099896, val loss drop 0.0244 to 0.0141\n",
      "fold 9\n",
      "[1] : train loss 34.977878, val loss drop 10000000.0000 to 8.0779\n",
      "[4] : train loss 0.679361, val loss drop 8.0779 to 3.3021\n",
      "[5] : train loss 0.534767, val loss drop 3.3021 to 1.4034\n",
      "[6] : train loss 0.522366, val loss drop 1.4034 to 0.7454\n",
      "[7] : train loss 0.412561, val loss drop 0.7454 to 0.1961\n",
      "[8] : train loss 0.484461, val loss drop 0.1961 to 0.1428\n",
      "[11] : train loss 0.233276, val loss drop 0.1428 to 0.0985\n",
      "[19] : train loss 0.211310, val loss drop 0.0985 to 0.0717\n",
      "[20] : train loss 0.141742, val loss drop 0.0717 to 0.0553\n",
      "[31] : train loss 0.130264, val loss drop 0.0553 to 0.0434\n",
      "[32] : train loss 0.092364, val loss drop 0.0434 to 0.0254\n",
      "[53] : train loss 0.088604, val loss drop 0.0254 to 0.0229\n",
      "[80] : train loss 0.141063, val loss drop 0.0229 to 0.0179\n",
      "fold 10\n",
      "[1] : train loss 49.114920, val loss drop 10000000.0000 to 16.1564\n",
      "[2] : train loss 5.069691, val loss drop 16.1564 to 11.7978\n",
      "[3] : train loss 1.736874, val loss drop 11.7978 to 4.6914\n",
      "[4] : train loss 0.857451, val loss drop 4.6914 to 0.9146\n",
      "[5] : train loss 0.577899, val loss drop 0.9146 to 0.6634\n",
      "[6] : train loss 0.576783, val loss drop 0.6634 to 0.3347\n",
      "[7] : train loss 0.366571, val loss drop 0.3347 to 0.2690\n",
      "[8] : train loss 0.395366, val loss drop 0.2690 to 0.2149\n",
      "[15] : train loss 0.247304, val loss drop 0.2149 to 0.1467\n",
      "[19] : train loss 0.493073, val loss drop 0.1467 to 0.1277\n",
      "[21] : train loss 0.342588, val loss drop 0.1277 to 0.0961\n",
      "[32] : train loss 0.125125, val loss drop 0.0961 to 0.0518\n",
      "[36] : train loss 0.188352, val loss drop 0.0518 to 0.0366\n",
      "[41] : train loss 0.107448, val loss drop 0.0366 to 0.0307\n",
      "[56] : train loss 0.152596, val loss drop 0.0307 to 0.0276\n",
      "[58] : train loss 0.303861, val loss drop 0.0276 to 0.0197\n",
      "[80] : train loss 0.089980, val loss drop 0.0197 to 0.0106\n"
     ]
    }
   ],
   "source": [
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "# define model\n",
    "model = cnn_model(conv, fc)\n",
    "pred_array = []\n",
    "for i in range(1, nfold + 1):\n",
    "    model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "    model = model.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict = model(test_f.cuda())\n",
    "    pred_array.append(predict.detach().cpu().numpy())\n",
    "result = np.mean(pred_array, axis = 0)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "# define model\n",
    "model = cnn_model(conv, fc)\n",
    "pred_array = []\n",
    "for i in range(1, nfold + 1):\n",
    "    model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "    model = model.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict = model(test_f_add.cuda())\n",
    "    pred_array.append(predict.detach().cpu().numpy())\n",
    "result = np.mean(pred_array, axis = 0)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, n_features], (3, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*3*n_features, output_size = len(name))\n",
    "# define model\n",
    "model = cnn_model(conv, fc)\n",
    "pred_array = []\n",
    "for i in range(1, nfold + 1):\n",
    "    model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "    model = model.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predict = model(test_f_add.cuda())\n",
    "    pred_array.append(predict.detach().cpu().numpy())\n",
    "result = np.mean(pred_array, axis = 0)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-260.121490</td>\n",
       "      <td>-42.052998</td>\n",
       "      <td>113.206131</td>\n",
       "      <td>0.450548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>315.630432</td>\n",
       "      <td>-280.351257</td>\n",
       "      <td>89.356171</td>\n",
       "      <td>0.481469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-237.003372</td>\n",
       "      <td>129.876831</td>\n",
       "      <td>29.687794</td>\n",
       "      <td>0.395682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>159.593109</td>\n",
       "      <td>274.795746</td>\n",
       "      <td>27.364025</td>\n",
       "      <td>0.409057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-169.503418</td>\n",
       "      <td>187.204330</td>\n",
       "      <td>132.531036</td>\n",
       "      <td>0.452389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -260.121490  -42.052998  113.206131  0.450548\n",
       "1  2801  315.630432 -280.351257   89.356171  0.481469\n",
       "2  2802 -237.003372  129.876831   29.687794  0.395682\n",
       "3  2803  159.593109  274.795746   27.364025  0.409057\n",
       "4  2804 -169.503418  187.204330  132.531036  0.452389"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, 'submit.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.target[index]\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs, stride = (2, 1))\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        #self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        #return self.pool(x)\n",
    "        return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sequential predict(XY-> V-> M)\n",
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "\n",
    "# make dataset\n",
    "    train_target = target[list(name)].values#.astype(np.float32)\n",
    "\n",
    "    # trainx, valx, trainy, valy = train_test_split(train_f, train_target, test_size = 0.2, shuffle = True, random_state = 38)\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        n_features = feature.shape[-1]\n",
    "        #fc = classifier([128, 64, 32, 16], input_size = 256*2*5, output_size = len(name))\n",
    "        #conv = conv_block([32, 64, 128, 256, 256, 256, 256], [1, 375, 5], (3, 1))\n",
    "        conv = conv_block([16, 32, 64, 128, 256, 256, 256], [1, 375, n_features], (3, 1))\n",
    "        fc = classifier([128, 64, 32, 16], input_size = 256*1*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        model = cnn_model(conv, fc)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            model.train()\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            model.eval()\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.947474, val loss drop 10000000.0000 to 2.2058\n",
      "[2] : train loss 1.811406, val loss drop 2.2058 to 0.9758\n",
      "[3] : train loss 0.585628, val loss drop 0.9758 to 0.5168\n",
      "[5] : train loss 0.036457, val loss drop 0.5168 to 0.0501\n",
      "[7] : train loss 0.009243, val loss drop 0.0501 to 0.0085\n",
      "[10] : train loss 0.005460, val loss drop 0.0085 to 0.0052\n",
      "[11] : train loss 0.003609, val loss drop 0.0052 to 0.0031\n",
      "[18] : train loss 0.002623, val loss drop 0.0031 to 0.0029\n",
      "[20] : train loss 0.001639, val loss drop 0.0029 to 0.0013\n",
      "[49] : train loss 0.001369, val loss drop 0.0013 to 0.0010\n",
      "[54] : train loss 0.000932, val loss drop 0.0010 to 0.0007\n",
      "[69] : train loss 0.001058, val loss drop 0.0007 to 0.0007\n",
      "[72] : train loss 0.001392, val loss drop 0.0007 to 0.0006\n",
      "[83] : train loss 0.000539, val loss drop 0.0006 to 0.0004\n",
      "fold 2\n",
      "[1] : train loss 3.054003, val loss drop 10000000.0000 to 2.3390\n",
      "[2] : train loss 2.062557, val loss drop 2.3390 to 1.0744\n",
      "[3] : train loss 0.670414, val loss drop 1.0744 to 0.1539\n",
      "[6] : train loss 0.013202, val loss drop 0.1539 to 0.0256\n",
      "[7] : train loss 0.007706, val loss drop 0.0256 to 0.0079\n",
      "[11] : train loss 0.002841, val loss drop 0.0079 to 0.0049\n",
      "[13] : train loss 0.001702, val loss drop 0.0049 to 0.0041\n",
      "[14] : train loss 0.001893, val loss drop 0.0041 to 0.0034\n",
      "[18] : train loss 0.001710, val loss drop 0.0034 to 0.0020\n",
      "[20] : train loss 0.001685, val loss drop 0.0020 to 0.0016\n",
      "[23] : train loss 0.002744, val loss drop 0.0016 to 0.0011\n",
      "[33] : train loss 0.001191, val loss drop 0.0011 to 0.0011\n",
      "[39] : train loss 0.001237, val loss drop 0.0011 to 0.0006\n",
      "[40] : train loss 0.000725, val loss drop 0.0006 to 0.0006\n",
      "[57] : train loss 0.000692, val loss drop 0.0006 to 0.0004\n",
      "[85] : train loss 0.000668, val loss drop 0.0004 to 0.0003\n",
      "fold 3\n",
      "[1] : train loss 2.986514, val loss drop 10000000.0000 to 2.1435\n",
      "[2] : train loss 1.866353, val loss drop 2.1435 to 0.7384\n",
      "[3] : train loss 0.548469, val loss drop 0.7384 to 0.2290\n",
      "[6] : train loss 0.012170, val loss drop 0.2290 to 0.0164\n",
      "[7] : train loss 0.007800, val loss drop 0.0164 to 0.0087\n",
      "[8] : train loss 0.006680, val loss drop 0.0087 to 0.0083\n",
      "[10] : train loss 0.005990, val loss drop 0.0083 to 0.0079\n",
      "[12] : train loss 0.002533, val loss drop 0.0079 to 0.0054\n",
      "[14] : train loss 0.002529, val loss drop 0.0054 to 0.0039\n",
      "[15] : train loss 0.001967, val loss drop 0.0039 to 0.0035\n",
      "[16] : train loss 0.001835, val loss drop 0.0035 to 0.0024\n",
      "[17] : train loss 0.001951, val loss drop 0.0024 to 0.0017\n",
      "[18] : train loss 0.001692, val loss drop 0.0017 to 0.0015\n",
      "[27] : train loss 0.001718, val loss drop 0.0015 to 0.0011\n",
      "[28] : train loss 0.001078, val loss drop 0.0011 to 0.0008\n",
      "[44] : train loss 0.001306, val loss drop 0.0008 to 0.0006\n",
      "[57] : train loss 0.000987, val loss drop 0.0006 to 0.0005\n",
      "[72] : train loss 0.000617, val loss drop 0.0005 to 0.0004\n",
      "[79] : train loss 0.000562, val loss drop 0.0004 to 0.0003\n",
      "[100] : train loss 0.000621, val loss drop 0.0003 to 0.0003\n",
      "fold 4\n",
      "[1] : train loss 2.979946, val loss drop 10000000.0000 to 1.9154\n",
      "[2] : train loss 1.715114, val loss drop 1.9154 to 0.4811\n",
      "[3] : train loss 0.310169, val loss drop 0.4811 to 0.1574\n",
      "[6] : train loss 0.010812, val loss drop 0.1574 to 0.0418\n",
      "[7] : train loss 0.007314, val loss drop 0.0418 to 0.0097\n",
      "[8] : train loss 0.004371, val loss drop 0.0097 to 0.0038\n",
      "[11] : train loss 0.005282, val loss drop 0.0038 to 0.0030\n",
      "[16] : train loss 0.002496, val loss drop 0.0030 to 0.0021\n",
      "[19] : train loss 0.001791, val loss drop 0.0021 to 0.0020\n",
      "[20] : train loss 0.001811, val loss drop 0.0020 to 0.0018\n",
      "[21] : train loss 0.001250, val loss drop 0.0018 to 0.0010\n",
      "[26] : train loss 0.001633, val loss drop 0.0010 to 0.0009\n",
      "[45] : train loss 0.001078, val loss drop 0.0009 to 0.0007\n",
      "[82] : train loss 0.001262, val loss drop 0.0007 to 0.0006\n",
      "[86] : train loss 0.000448, val loss drop 0.0006 to 0.0005\n",
      "fold 5\n",
      "[1] : train loss 3.054507, val loss drop 10000000.0000 to 2.3626\n",
      "[2] : train loss 2.317401, val loss drop 2.3626 to 1.7273\n",
      "[3] : train loss 1.390093, val loss drop 1.7273 to 0.6314\n",
      "[4] : train loss 0.533788, val loss drop 0.6314 to 0.2607\n",
      "[6] : train loss 0.027584, val loss drop 0.2607 to 0.1610\n",
      "[7] : train loss 0.013629, val loss drop 0.1610 to 0.0191\n",
      "[8] : train loss 0.008937, val loss drop 0.0191 to 0.0081\n",
      "[12] : train loss 0.004617, val loss drop 0.0081 to 0.0038\n",
      "[14] : train loss 0.002233, val loss drop 0.0038 to 0.0023\n",
      "[17] : train loss 0.002522, val loss drop 0.0023 to 0.0018\n",
      "[30] : train loss 0.001756, val loss drop 0.0018 to 0.0011\n",
      "[36] : train loss 0.001518, val loss drop 0.0011 to 0.0011\n",
      "[48] : train loss 0.001244, val loss drop 0.0011 to 0.0009\n",
      "[49] : train loss 0.001030, val loss drop 0.0009 to 0.0008\n",
      "[60] : train loss 0.001175, val loss drop 0.0008 to 0.0006\n",
      "[62] : train loss 0.001169, val loss drop 0.0006 to 0.0006\n",
      "[67] : train loss 0.001337, val loss drop 0.0006 to 0.0005\n",
      "[70] : train loss 0.001184, val loss drop 0.0005 to 0.0004\n",
      "fold 6\n",
      "[1] : train loss 3.046708, val loss drop 10000000.0000 to 2.1432\n",
      "[2] : train loss 2.094276, val loss drop 2.1432 to 1.1975\n",
      "[3] : train loss 0.845272, val loss drop 1.1975 to 0.5107\n",
      "[4] : train loss 0.217193, val loss drop 0.5107 to 0.3626\n",
      "[5] : train loss 0.041895, val loss drop 0.3626 to 0.1711\n",
      "[6] : train loss 0.015252, val loss drop 0.1711 to 0.0666\n",
      "[8] : train loss 0.007125, val loss drop 0.0666 to 0.0154\n",
      "[9] : train loss 0.007736, val loss drop 0.0154 to 0.0048\n",
      "[11] : train loss 0.004495, val loss drop 0.0048 to 0.0040\n",
      "[12] : train loss 0.002216, val loss drop 0.0040 to 0.0030\n",
      "[13] : train loss 0.002174, val loss drop 0.0030 to 0.0020\n",
      "[17] : train loss 0.002128, val loss drop 0.0020 to 0.0018\n",
      "[30] : train loss 0.002038, val loss drop 0.0018 to 0.0008\n",
      "[41] : train loss 0.001185, val loss drop 0.0008 to 0.0006\n",
      "[43] : train loss 0.001754, val loss drop 0.0006 to 0.0005\n",
      "[66] : train loss 0.001057, val loss drop 0.0005 to 0.0005\n",
      "fold 7\n",
      "[1] : train loss 2.922018, val loss drop 10000000.0000 to 2.0806\n",
      "[2] : train loss 1.805806, val loss drop 2.0806 to 0.7926\n",
      "[3] : train loss 0.552583, val loss drop 0.7926 to 0.2416\n",
      "[5] : train loss 0.029481, val loss drop 0.2416 to 0.2065\n",
      "[6] : train loss 0.014760, val loss drop 0.2065 to 0.0080\n",
      "[8] : train loss 0.006597, val loss drop 0.0080 to 0.0057\n",
      "[9] : train loss 0.005399, val loss drop 0.0057 to 0.0051\n",
      "[10] : train loss 0.004568, val loss drop 0.0051 to 0.0043\n",
      "[13] : train loss 0.003106, val loss drop 0.0043 to 0.0033\n",
      "[14] : train loss 0.002357, val loss drop 0.0033 to 0.0029\n",
      "[16] : train loss 0.002101, val loss drop 0.0029 to 0.0019\n",
      "[17] : train loss 0.001693, val loss drop 0.0019 to 0.0014\n",
      "[19] : train loss 0.001753, val loss drop 0.0014 to 0.0013\n",
      "[28] : train loss 0.001599, val loss drop 0.0013 to 0.0012\n",
      "[30] : train loss 0.001024, val loss drop 0.0012 to 0.0010\n",
      "[31] : train loss 0.001620, val loss drop 0.0010 to 0.0009\n",
      "[43] : train loss 0.002470, val loss drop 0.0009 to 0.0008\n",
      "[51] : train loss 0.001433, val loss drop 0.0008 to 0.0008\n",
      "[53] : train loss 0.000919, val loss drop 0.0008 to 0.0006\n",
      "[77] : train loss 0.000875, val loss drop 0.0006 to 0.0004\n",
      "[84] : train loss 0.001047, val loss drop 0.0004 to 0.0003\n",
      "fold 8\n",
      "[1] : train loss 2.957690, val loss drop 10000000.0000 to 2.1953\n",
      "[2] : train loss 1.905740, val loss drop 2.1953 to 0.8315\n",
      "[3] : train loss 0.746038, val loss drop 0.8315 to 0.2067\n",
      "[6] : train loss 0.012441, val loss drop 0.2067 to 0.0538\n",
      "[8] : train loss 0.005104, val loss drop 0.0538 to 0.0116\n",
      "[9] : train loss 0.005370, val loss drop 0.0116 to 0.0044\n",
      "[11] : train loss 0.007415, val loss drop 0.0044 to 0.0034\n",
      "[12] : train loss 0.003572, val loss drop 0.0034 to 0.0033\n",
      "[14] : train loss 0.002860, val loss drop 0.0033 to 0.0016\n",
      "[18] : train loss 0.003681, val loss drop 0.0016 to 0.0015\n",
      "[30] : train loss 0.001673, val loss drop 0.0015 to 0.0010\n",
      "[44] : train loss 0.002138, val loss drop 0.0010 to 0.0010\n",
      "[48] : train loss 0.001377, val loss drop 0.0010 to 0.0008\n",
      "[56] : train loss 0.001773, val loss drop 0.0008 to 0.0008\n",
      "[69] : train loss 0.000955, val loss drop 0.0008 to 0.0005\n",
      "[75] : train loss 0.001140, val loss drop 0.0005 to 0.0005\n",
      "[89] : train loss 0.001027, val loss drop 0.0005 to 0.0005\n",
      "fold 9\n",
      "[1] : train loss 2.990971, val loss drop 10000000.0000 to 2.4275\n",
      "[2] : train loss 1.823552, val loss drop 2.4275 to 0.7730\n",
      "[3] : train loss 0.517990, val loss drop 0.7730 to 0.1935\n",
      "[5] : train loss 0.031629, val loss drop 0.1935 to 0.1193\n",
      "[6] : train loss 0.015782, val loss drop 0.1193 to 0.0170\n",
      "[8] : train loss 0.006730, val loss drop 0.0170 to 0.0059\n",
      "[9] : train loss 0.004299, val loss drop 0.0059 to 0.0055\n",
      "[14] : train loss 0.004849, val loss drop 0.0055 to 0.0052\n",
      "[16] : train loss 0.001958, val loss drop 0.0052 to 0.0048\n",
      "[17] : train loss 0.002578, val loss drop 0.0048 to 0.0024\n",
      "[19] : train loss 0.003164, val loss drop 0.0024 to 0.0019\n",
      "[22] : train loss 0.001797, val loss drop 0.0019 to 0.0015\n",
      "[23] : train loss 0.001594, val loss drop 0.0015 to 0.0013\n",
      "[26] : train loss 0.001715, val loss drop 0.0013 to 0.0011\n",
      "[27] : train loss 0.001390, val loss drop 0.0011 to 0.0010\n",
      "[38] : train loss 0.001045, val loss drop 0.0010 to 0.0007\n",
      "[50] : train loss 0.001787, val loss drop 0.0007 to 0.0004\n",
      "[95] : train loss 0.000365, val loss drop 0.0004 to 0.0003\n",
      "fold 10\n",
      "[1] : train loss 3.039653, val loss drop 10000000.0000 to 2.4274\n",
      "[2] : train loss 2.064689, val loss drop 2.4274 to 0.9220\n",
      "[3] : train loss 0.814917, val loss drop 0.9220 to 0.2943\n",
      "[4] : train loss 0.165767, val loss drop 0.2943 to 0.2579\n",
      "[6] : train loss 0.022237, val loss drop 0.2579 to 0.0800\n",
      "[7] : train loss 0.011615, val loss drop 0.0800 to 0.0381\n",
      "[8] : train loss 0.009773, val loss drop 0.0381 to 0.0060\n",
      "[11] : train loss 0.005131, val loss drop 0.0060 to 0.0049\n",
      "[12] : train loss 0.002801, val loss drop 0.0049 to 0.0033\n",
      "[15] : train loss 0.003474, val loss drop 0.0033 to 0.0016\n",
      "[17] : train loss 0.001772, val loss drop 0.0016 to 0.0013\n",
      "[34] : train loss 0.001450, val loss drop 0.0013 to 0.0009\n",
      "[44] : train loss 0.000785, val loss drop 0.0009 to 0.0005\n",
      "[91] : train loss 0.000997, val loss drop 0.0005 to 0.0005\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss = kfold_train('XY',train_f, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feature = train_t[['X','Y']].values.reshape((2800,1 , 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 4.264157, val loss drop 10000000.0000 to 0.5841\n",
      "[2] : train loss 0.133741, val loss drop 0.5841 to 0.0779\n",
      "[3] : train loss 0.052556, val loss drop 0.0779 to 0.0429\n",
      "[4] : train loss 0.028958, val loss drop 0.0429 to 0.0267\n",
      "[5] : train loss 0.018431, val loss drop 0.0267 to 0.0230\n",
      "[6] : train loss 0.013264, val loss drop 0.0230 to 0.0138\n",
      "[9] : train loss 0.010436, val loss drop 0.0138 to 0.0102\n",
      "[10] : train loss 0.008032, val loss drop 0.0102 to 0.0095\n",
      "[13] : train loss 0.007572, val loss drop 0.0095 to 0.0085\n",
      "[16] : train loss 0.004096, val loss drop 0.0085 to 0.0051\n",
      "[21] : train loss 0.003869, val loss drop 0.0051 to 0.0036\n",
      "[36] : train loss 0.003180, val loss drop 0.0036 to 0.0031\n",
      "[41] : train loss 0.001347, val loss drop 0.0031 to 0.0031\n",
      "[42] : train loss 0.001513, val loss drop 0.0031 to 0.0022\n",
      "[43] : train loss 0.002462, val loss drop 0.0022 to 0.0019\n",
      "[56] : train loss 0.001790, val loss drop 0.0019 to 0.0015\n",
      "[73] : train loss 0.002017, val loss drop 0.0015 to 0.0015\n",
      "[76] : train loss 0.001105, val loss drop 0.0015 to 0.0015\n",
      "[88] : train loss 0.002453, val loss drop 0.0015 to 0.0014\n",
      "[89] : train loss 0.000947, val loss drop 0.0014 to 0.0013\n",
      "[91] : train loss 0.001944, val loss drop 0.0013 to 0.0012\n",
      "fold 2\n",
      "[1] : train loss 4.164650, val loss drop 10000000.0000 to 0.6842\n",
      "[2] : train loss 0.301630, val loss drop 0.6842 to 0.2335\n",
      "[3] : train loss 0.092679, val loss drop 0.2335 to 0.0955\n",
      "[4] : train loss 0.044865, val loss drop 0.0955 to 0.0495\n",
      "[5] : train loss 0.025373, val loss drop 0.0495 to 0.0253\n",
      "[6] : train loss 0.017948, val loss drop 0.0253 to 0.0136\n",
      "[8] : train loss 0.010442, val loss drop 0.0136 to 0.0135\n",
      "[9] : train loss 0.008511, val loss drop 0.0135 to 0.0106\n",
      "[11] : train loss 0.006229, val loss drop 0.0106 to 0.0084\n",
      "[14] : train loss 0.004967, val loss drop 0.0084 to 0.0074\n",
      "[18] : train loss 0.003754, val loss drop 0.0074 to 0.0058\n",
      "[24] : train loss 0.005914, val loss drop 0.0058 to 0.0053\n",
      "[33] : train loss 0.002285, val loss drop 0.0053 to 0.0050\n",
      "[36] : train loss 0.002719, val loss drop 0.0050 to 0.0041\n",
      "[40] : train loss 0.001870, val loss drop 0.0041 to 0.0034\n",
      "[46] : train loss 0.001071, val loss drop 0.0034 to 0.0028\n",
      "[63] : train loss 0.001423, val loss drop 0.0028 to 0.0024\n",
      "[66] : train loss 0.001132, val loss drop 0.0024 to 0.0022\n",
      "[71] : train loss 0.001080, val loss drop 0.0022 to 0.0019\n",
      "[93] : train loss 0.001030, val loss drop 0.0019 to 0.0018\n",
      "fold 3\n",
      "[1] : train loss 1.823880, val loss drop 10000000.0000 to 0.1573\n",
      "[2] : train loss 0.078983, val loss drop 0.1573 to 0.0985\n",
      "[3] : train loss 0.028993, val loss drop 0.0985 to 0.0282\n",
      "[4] : train loss 0.015365, val loss drop 0.0282 to 0.0211\n",
      "[5] : train loss 0.010739, val loss drop 0.0211 to 0.0088\n",
      "[7] : train loss 0.005876, val loss drop 0.0088 to 0.0060\n",
      "[8] : train loss 0.005440, val loss drop 0.0060 to 0.0052\n",
      "[11] : train loss 0.005106, val loss drop 0.0052 to 0.0045\n",
      "[13] : train loss 0.004118, val loss drop 0.0045 to 0.0035\n",
      "[22] : train loss 0.002410, val loss drop 0.0035 to 0.0024\n",
      "[26] : train loss 0.002764, val loss drop 0.0024 to 0.0019\n",
      "[38] : train loss 0.002658, val loss drop 0.0019 to 0.0019\n",
      "[44] : train loss 0.002540, val loss drop 0.0019 to 0.0011\n",
      "[50] : train loss 0.000767, val loss drop 0.0011 to 0.0010\n",
      "[59] : train loss 0.000998, val loss drop 0.0010 to 0.0009\n",
      "[68] : train loss 0.001484, val loss drop 0.0009 to 0.0008\n",
      "[95] : train loss 0.002463, val loss drop 0.0008 to 0.0007\n",
      "fold 4\n",
      "[1] : train loss 5.216972, val loss drop 10000000.0000 to 0.4634\n",
      "[2] : train loss 0.214080, val loss drop 0.4634 to 0.1101\n",
      "[3] : train loss 0.072170, val loss drop 0.1101 to 0.0335\n",
      "[6] : train loss 0.021011, val loss drop 0.0335 to 0.0205\n",
      "[7] : train loss 0.014476, val loss drop 0.0205 to 0.0128\n",
      "[10] : train loss 0.007381, val loss drop 0.0128 to 0.0100\n",
      "[14] : train loss 0.007516, val loss drop 0.0100 to 0.0093\n",
      "[15] : train loss 0.005901, val loss drop 0.0093 to 0.0060\n",
      "[16] : train loss 0.005840, val loss drop 0.0060 to 0.0059\n",
      "[19] : train loss 0.003960, val loss drop 0.0059 to 0.0053\n",
      "[20] : train loss 0.003694, val loss drop 0.0053 to 0.0053\n",
      "[21] : train loss 0.003494, val loss drop 0.0053 to 0.0042\n",
      "[25] : train loss 0.009265, val loss drop 0.0042 to 0.0034\n",
      "[36] : train loss 0.005204, val loss drop 0.0034 to 0.0030\n",
      "[37] : train loss 0.001836, val loss drop 0.0030 to 0.0024\n",
      "[38] : train loss 0.003001, val loss drop 0.0024 to 0.0024\n",
      "[44] : train loss 0.004089, val loss drop 0.0024 to 0.0022\n",
      "[68] : train loss 0.003526, val loss drop 0.0022 to 0.0021\n",
      "[71] : train loss 0.002178, val loss drop 0.0021 to 0.0020\n",
      "[73] : train loss 0.001902, val loss drop 0.0020 to 0.0015\n",
      "[85] : train loss 0.004935, val loss drop 0.0015 to 0.0012\n",
      "[98] : train loss 0.001368, val loss drop 0.0012 to 0.0012\n",
      "fold 5\n",
      "[1] : train loss 4.388874, val loss drop 10000000.0000 to 0.2362\n",
      "[2] : train loss 0.110752, val loss drop 0.2362 to 0.0700\n",
      "[3] : train loss 0.042851, val loss drop 0.0700 to 0.0313\n",
      "[4] : train loss 0.023324, val loss drop 0.0313 to 0.0185\n",
      "[6] : train loss 0.010637, val loss drop 0.0185 to 0.0126\n"
     ]
    }
   ],
   "source": [
    "# train V using XY\n",
    "loss = kfold_train('V',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "for name in ['XY','M','V']:\n",
    "    fc = classifier([128, 64, 32, 16], input_size = 512*3*5, output_size = len(name))\n",
    "    conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, 5], (3, 1))\n",
    "    # define model\n",
    "    model = cnn_model(conv, fc)\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold + 1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predict = model(test_f.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    submission[list(name)] = np.mean(pred_array, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-261.139008</td>\n",
       "      <td>-39.881229</td>\n",
       "      <td>111.966736</td>\n",
       "      <td>0.434986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>316.171234</td>\n",
       "      <td>-286.514954</td>\n",
       "      <td>90.317627</td>\n",
       "      <td>0.421709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-233.366653</td>\n",
       "      <td>128.657593</td>\n",
       "      <td>28.895294</td>\n",
       "      <td>0.357899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>160.693039</td>\n",
       "      <td>276.158539</td>\n",
       "      <td>27.653961</td>\n",
       "      <td>0.372838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-170.325470</td>\n",
       "      <td>187.950928</td>\n",
       "      <td>133.650543</td>\n",
       "      <td>0.478190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -261.139008  -39.881229  111.966736  0.434986\n",
       "1  2801  316.171234 -286.514954   90.317627  0.421709\n",
       "2  2802 -233.366653  128.657593   28.895294  0.357899\n",
       "3  2803  160.693039  276.158539   27.653961  0.372838\n",
       "4  2804 -170.325470  187.950928  133.650543  0.478190"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, 'submit.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 10)\n",
    "y = torch.randn(1, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

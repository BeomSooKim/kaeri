{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cnn architecture 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "def fix_seed(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)\n",
    "        #return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_cnn, self).__init__()\n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size = (4,1), stride = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size = (2,1)),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size = (4,1), stride = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size = (2,1)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size = (4,1), stride = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size = (2,1)),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size = (4,1), stride = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size = (2,1)),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size = (4,1), stride = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size = (2,1)),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size = (4,1), stride = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size = (2,1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_fc(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(custom_fc, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis = 2):\n",
    "    mu = np.expand_dims(x.mean(axis = 2), axis = axis)\n",
    "    sd = np.expand_dims(x.std(axis = 2), axis = axis)\n",
    "\n",
    "    normalized = (x - mu) / sd\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "# concatenate normalized data\n",
    "train_norm = normalize(train_f)\n",
    "test_norm = normalize(test_f)\n",
    "\n",
    "train_f = np.concatenate((train_f, train_norm), axis = 1)\n",
    "test_f = np.concatenate((test_f, test_norm), axis = 1)\n",
    "\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        #conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "        #fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        #model = cnn_model(conv, fc)\n",
    "        fe = custom_cnn()\n",
    "        fc = custom_fc(512*2*n_features, len(name))\n",
    "        model = nn.Sequential(fe, fc)\n",
    "        #model = get_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 3.031957, val loss drop 10000000.0000 to 2.4187\n",
      "[2] : train loss 1.866044, val loss drop 2.4187 to 1.1020\n",
      "[3] : train loss 0.724624, val loss drop 1.1020 to 0.3799\n",
      "[4] : train loss 0.201127, val loss drop 0.3799 to 0.1924\n",
      "[5] : train loss 0.090820, val loss drop 0.1924 to 0.0858\n",
      "[6] : train loss 0.038448, val loss drop 0.0858 to 0.0414\n",
      "[7] : train loss 0.020213, val loss drop 0.0414 to 0.0284\n",
      "[8] : train loss 0.014702, val loss drop 0.0284 to 0.0156\n",
      "[9] : train loss 0.009817, val loss drop 0.0156 to 0.0099\n",
      "[10] : train loss 0.007532, val loss drop 0.0099 to 0.0091\n",
      "[11] : train loss 0.006131, val loss drop 0.0091 to 0.0069\n",
      "[13] : train loss 0.004351, val loss drop 0.0069 to 0.0053\n",
      "[14] : train loss 0.003650, val loss drop 0.0053 to 0.0043\n",
      "[16] : train loss 0.002574, val loss drop 0.0043 to 0.0036\n",
      "[17] : train loss 0.001955, val loss drop 0.0036 to 0.0035\n",
      "[21] : train loss 0.001834, val loss drop 0.0035 to 0.0030\n",
      "[22] : train loss 0.001855, val loss drop 0.0030 to 0.0024\n",
      "[28] : train loss 0.001628, val loss drop 0.0024 to 0.0020\n",
      "[31] : train loss 0.001282, val loss drop 0.0020 to 0.0015\n",
      "[39] : train loss 0.000699, val loss drop 0.0015 to 0.0014\n",
      "[45] : train loss 0.001266, val loss drop 0.0014 to 0.0012\n",
      "[54] : train loss 0.000707, val loss drop 0.0012 to 0.0012\n",
      "[56] : train loss 0.000541, val loss drop 0.0012 to 0.0011\n",
      "[59] : train loss 0.001737, val loss drop 0.0011 to 0.0010\n",
      "[73] : train loss 0.000660, val loss drop 0.0010 to 0.0009\n",
      "[75] : train loss 0.000570, val loss drop 0.0009 to 0.0008\n",
      "[81] : train loss 0.000494, val loss drop 0.0008 to 0.0008\n",
      "[96] : train loss 0.000403, val loss drop 0.0008 to 0.0007\n",
      "fold 2\n",
      "[1] : train loss 3.017915, val loss drop 10000000.0000 to 2.2589\n",
      "[2] : train loss 1.591100, val loss drop 2.2589 to 0.7513\n",
      "[3] : train loss 0.344546, val loss drop 0.7513 to 0.1830\n",
      "[4] : train loss 0.110135, val loss drop 0.1830 to 0.1013\n",
      "[5] : train loss 0.046420, val loss drop 0.1013 to 0.0963\n",
      "[6] : train loss 0.024267, val loss drop 0.0963 to 0.0647\n",
      "[7] : train loss 0.014521, val loss drop 0.0647 to 0.0250\n",
      "[8] : train loss 0.008994, val loss drop 0.0250 to 0.0134\n",
      "[9] : train loss 0.006157, val loss drop 0.0134 to 0.0069\n",
      "[10] : train loss 0.004385, val loss drop 0.0069 to 0.0051\n",
      "[11] : train loss 0.003376, val loss drop 0.0051 to 0.0045\n",
      "[12] : train loss 0.002864, val loss drop 0.0045 to 0.0042\n",
      "[13] : train loss 0.002696, val loss drop 0.0042 to 0.0042\n",
      "[15] : train loss 0.002361, val loss drop 0.0042 to 0.0031\n",
      "[16] : train loss 0.001632, val loss drop 0.0031 to 0.0028\n",
      "[18] : train loss 0.001493, val loss drop 0.0028 to 0.0027\n",
      "[20] : train loss 0.001857, val loss drop 0.0027 to 0.0022\n",
      "[22] : train loss 0.001578, val loss drop 0.0022 to 0.0021\n",
      "[24] : train loss 0.001057, val loss drop 0.0021 to 0.0018\n",
      "[28] : train loss 0.001317, val loss drop 0.0018 to 0.0017\n",
      "[31] : train loss 0.000975, val loss drop 0.0017 to 0.0015\n",
      "[39] : train loss 0.000570, val loss drop 0.0015 to 0.0014\n",
      "[40] : train loss 0.000805, val loss drop 0.0014 to 0.0013\n",
      "[41] : train loss 0.000510, val loss drop 0.0013 to 0.0010\n",
      "[64] : train loss 0.000484, val loss drop 0.0010 to 0.0010\n",
      "[67] : train loss 0.000534, val loss drop 0.0010 to 0.0008\n",
      "[68] : train loss 0.000571, val loss drop 0.0008 to 0.0007\n",
      "[69] : train loss 0.000758, val loss drop 0.0007 to 0.0007\n",
      "[90] : train loss 0.000547, val loss drop 0.0007 to 0.0006\n",
      "[91] : train loss 0.000588, val loss drop 0.0006 to 0.0006\n",
      "fold 3\n",
      "[1] : train loss 2.979189, val loss drop 10000000.0000 to 2.2386\n",
      "[2] : train loss 1.571263, val loss drop 2.2386 to 0.8143\n",
      "[3] : train loss 0.365781, val loss drop 0.8143 to 0.3284\n",
      "[4] : train loss 0.128895, val loss drop 0.3284 to 0.1777\n",
      "[5] : train loss 0.050591, val loss drop 0.1777 to 0.0975\n",
      "[6] : train loss 0.024281, val loss drop 0.0975 to 0.0576\n",
      "[7] : train loss 0.014465, val loss drop 0.0576 to 0.0271\n",
      "[8] : train loss 0.009694, val loss drop 0.0271 to 0.0203\n",
      "[9] : train loss 0.007447, val loss drop 0.0203 to 0.0147\n",
      "[10] : train loss 0.005665, val loss drop 0.0147 to 0.0083\n",
      "[11] : train loss 0.004763, val loss drop 0.0083 to 0.0068\n",
      "[13] : train loss 0.004241, val loss drop 0.0068 to 0.0066\n",
      "[14] : train loss 0.002847, val loss drop 0.0066 to 0.0040\n",
      "[17] : train loss 0.001827, val loss drop 0.0040 to 0.0035\n",
      "[22] : train loss 0.002084, val loss drop 0.0035 to 0.0034\n",
      "[23] : train loss 0.002000, val loss drop 0.0034 to 0.0027\n",
      "[24] : train loss 0.001511, val loss drop 0.0027 to 0.0021\n",
      "[26] : train loss 0.001100, val loss drop 0.0021 to 0.0020\n",
      "[31] : train loss 0.001041, val loss drop 0.0020 to 0.0018\n",
      "[32] : train loss 0.001076, val loss drop 0.0018 to 0.0018\n",
      "[38] : train loss 0.000926, val loss drop 0.0018 to 0.0017\n",
      "[39] : train loss 0.000888, val loss drop 0.0017 to 0.0013\n",
      "[54] : train loss 0.000673, val loss drop 0.0013 to 0.0012\n",
      "[73] : train loss 0.000396, val loss drop 0.0012 to 0.0008\n",
      "[86] : train loss 0.000544, val loss drop 0.0008 to 0.0008\n",
      "[99] : train loss 0.000249, val loss drop 0.0008 to 0.0007\n",
      "fold 4\n",
      "[1] : train loss 3.037381, val loss drop 10000000.0000 to 2.0961\n",
      "[2] : train loss 1.738093, val loss drop 2.0961 to 0.8999\n",
      "[3] : train loss 0.582851, val loss drop 0.8999 to 0.3270\n",
      "[4] : train loss 0.168403, val loss drop 0.3270 to 0.1312\n",
      "[6] : train loss 0.034113, val loss drop 0.1312 to 0.0457\n",
      "[7] : train loss 0.018377, val loss drop 0.0457 to 0.0240\n",
      "[8] : train loss 0.011182, val loss drop 0.0240 to 0.0211\n",
      "[9] : train loss 0.007667, val loss drop 0.0211 to 0.0087\n",
      "[10] : train loss 0.005618, val loss drop 0.0087 to 0.0075\n",
      "[11] : train loss 0.004109, val loss drop 0.0075 to 0.0055\n",
      "[13] : train loss 0.003818, val loss drop 0.0055 to 0.0047\n",
      "[14] : train loss 0.003185, val loss drop 0.0047 to 0.0035\n",
      "[16] : train loss 0.002270, val loss drop 0.0035 to 0.0029\n",
      "[19] : train loss 0.002323, val loss drop 0.0029 to 0.0026\n",
      "[20] : train loss 0.001268, val loss drop 0.0026 to 0.0024\n",
      "[22] : train loss 0.001335, val loss drop 0.0024 to 0.0022\n",
      "[26] : train loss 0.001867, val loss drop 0.0022 to 0.0020\n",
      "[35] : train loss 0.001153, val loss drop 0.0020 to 0.0018\n",
      "[36] : train loss 0.000794, val loss drop 0.0018 to 0.0012\n",
      "[44] : train loss 0.000765, val loss drop 0.0012 to 0.0010\n",
      "[51] : train loss 0.000530, val loss drop 0.0010 to 0.0008\n",
      "[71] : train loss 0.000543, val loss drop 0.0008 to 0.0006\n",
      "[86] : train loss 0.000254, val loss drop 0.0006 to 0.0006\n",
      "fold 5\n",
      "[1] : train loss 2.993268, val loss drop 10000000.0000 to 2.4242\n",
      "[2] : train loss 1.709642, val loss drop 2.4242 to 1.0850\n",
      "[3] : train loss 0.566783, val loss drop 1.0850 to 0.2973\n",
      "[4] : train loss 0.152009, val loss drop 0.2973 to 0.1066\n",
      "[5] : train loss 0.061999, val loss drop 0.1066 to 0.0516\n",
      "[6] : train loss 0.029219, val loss drop 0.0516 to 0.0414\n",
      "[7] : train loss 0.015430, val loss drop 0.0414 to 0.0260\n",
      "[8] : train loss 0.008988, val loss drop 0.0260 to 0.0096\n",
      "[9] : train loss 0.006219, val loss drop 0.0096 to 0.0070\n",
      "[11] : train loss 0.004631, val loss drop 0.0070 to 0.0064\n",
      "[12] : train loss 0.002982, val loss drop 0.0064 to 0.0044\n",
      "[14] : train loss 0.002315, val loss drop 0.0044 to 0.0030\n",
      "[15] : train loss 0.001792, val loss drop 0.0030 to 0.0026\n",
      "[21] : train loss 0.001406, val loss drop 0.0026 to 0.0025\n",
      "[22] : train loss 0.001216, val loss drop 0.0025 to 0.0020\n",
      "[23] : train loss 0.001364, val loss drop 0.0020 to 0.0019\n",
      "[25] : train loss 0.000926, val loss drop 0.0019 to 0.0016\n",
      "[27] : train loss 0.000660, val loss drop 0.0016 to 0.0014\n",
      "[30] : train loss 0.000630, val loss drop 0.0014 to 0.0012\n",
      "[34] : train loss 0.000639, val loss drop 0.0012 to 0.0011\n",
      "[40] : train loss 0.000655, val loss drop 0.0011 to 0.0010\n",
      "[51] : train loss 0.000703, val loss drop 0.0010 to 0.0009\n",
      "[53] : train loss 0.000729, val loss drop 0.0009 to 0.0009\n",
      "[68] : train loss 0.000277, val loss drop 0.0009 to 0.0008\n",
      "[69] : train loss 0.000317, val loss drop 0.0008 to 0.0007\n",
      "[96] : train loss 0.000391, val loss drop 0.0007 to 0.0006\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 2.827650, val loss drop 10000000.0000 to 5.7157\n",
      "[2] : train loss 0.339924, val loss drop 5.7157 to 0.3626\n",
      "[3] : train loss 0.110811, val loss drop 0.3626 to 0.1151\n",
      "[4] : train loss 0.052105, val loss drop 0.1151 to 0.0342\n",
      "[5] : train loss 0.030621, val loss drop 0.0342 to 0.0096\n",
      "[9] : train loss 0.009671, val loss drop 0.0096 to 0.0077\n",
      "[10] : train loss 0.006583, val loss drop 0.0077 to 0.0044\n",
      "[11] : train loss 0.004027, val loss drop 0.0044 to 0.0030\n",
      "[12] : train loss 0.002959, val loss drop 0.0030 to 0.0028\n",
      "[16] : train loss 0.001953, val loss drop 0.0028 to 0.0023\n",
      "[19] : train loss 0.004081, val loss drop 0.0023 to 0.0020\n",
      "[21] : train loss 0.002048, val loss drop 0.0020 to 0.0019\n",
      "[22] : train loss 0.001399, val loss drop 0.0019 to 0.0018\n",
      "[23] : train loss 0.001375, val loss drop 0.0018 to 0.0017\n",
      "[24] : train loss 0.001667, val loss drop 0.0017 to 0.0015\n",
      "[25] : train loss 0.002955, val loss drop 0.0015 to 0.0015\n",
      "[38] : train loss 0.001701, val loss drop 0.0015 to 0.0011\n",
      "[43] : train loss 0.002017, val loss drop 0.0011 to 0.0009\n",
      "[65] : train loss 0.001191, val loss drop 0.0009 to 0.0008\n",
      "[85] : train loss 0.000672, val loss drop 0.0008 to 0.0006\n",
      "[100] : train loss 0.001542, val loss drop 0.0006 to 0.0006\n",
      "fold 2\n",
      "[1] : train loss 32.811921, val loss drop 10000000.0000 to 0.2685\n",
      "[2] : train loss 0.251651, val loss drop 0.2685 to 0.0517\n",
      "[3] : train loss 0.104984, val loss drop 0.0517 to 0.0513\n",
      "[4] : train loss 0.054904, val loss drop 0.0513 to 0.0155\n",
      "[5] : train loss 0.024960, val loss drop 0.0155 to 0.0139\n",
      "[6] : train loss 0.014307, val loss drop 0.0139 to 0.0118\n",
      "[7] : train loss 0.010528, val loss drop 0.0118 to 0.0079\n",
      "[9] : train loss 0.008383, val loss drop 0.0079 to 0.0067\n",
      "[10] : train loss 0.005883, val loss drop 0.0067 to 0.0063\n",
      "[11] : train loss 0.005975, val loss drop 0.0063 to 0.0054\n",
      "[12] : train loss 0.005086, val loss drop 0.0054 to 0.0046\n",
      "[16] : train loss 0.003711, val loss drop 0.0046 to 0.0040\n",
      "[18] : train loss 0.003471, val loss drop 0.0040 to 0.0035\n",
      "[21] : train loss 0.003119, val loss drop 0.0035 to 0.0029\n",
      "[23] : train loss 0.002634, val loss drop 0.0029 to 0.0029\n",
      "[32] : train loss 0.002141, val loss drop 0.0029 to 0.0022\n",
      "[36] : train loss 0.002420, val loss drop 0.0022 to 0.0018\n",
      "[43] : train loss 0.002839, val loss drop 0.0018 to 0.0016\n",
      "[52] : train loss 0.004662, val loss drop 0.0016 to 0.0013\n",
      "[57] : train loss 0.001691, val loss drop 0.0013 to 0.0011\n",
      "[82] : train loss 0.002344, val loss drop 0.0011 to 0.0010\n",
      "[86] : train loss 0.001775, val loss drop 0.0010 to 0.0010\n",
      "fold 3\n",
      "[1] : train loss 6.145094, val loss drop 10000000.0000 to 0.3095\n",
      "[3] : train loss 0.066970, val loss drop 0.3095 to 0.0382\n",
      "[4] : train loss 0.025240, val loss drop 0.0382 to 0.0276\n",
      "[5] : train loss 0.015440, val loss drop 0.0276 to 0.0167\n",
      "[6] : train loss 0.011408, val loss drop 0.0167 to 0.0119\n",
      "[7] : train loss 0.008662, val loss drop 0.0119 to 0.0115\n",
      "[8] : train loss 0.009959, val loss drop 0.0115 to 0.0063\n",
      "[10] : train loss 0.005333, val loss drop 0.0063 to 0.0055\n",
      "[12] : train loss 0.006829, val loss drop 0.0055 to 0.0048\n",
      "[14] : train loss 0.005691, val loss drop 0.0048 to 0.0047\n",
      "[16] : train loss 0.004105, val loss drop 0.0047 to 0.0033\n",
      "[21] : train loss 0.003654, val loss drop 0.0033 to 0.0027\n",
      "[42] : train loss 0.007390, val loss drop 0.0027 to 0.0024\n",
      "[47] : train loss 0.002587, val loss drop 0.0024 to 0.0021\n",
      "[51] : train loss 0.001690, val loss drop 0.0021 to 0.0014\n",
      "fold 4\n",
      "[1] : train loss 26.414959, val loss drop 10000000.0000 to 1.3247\n",
      "[2] : train loss 0.286576, val loss drop 1.3247 to 0.7873\n",
      "[3] : train loss 0.114214, val loss drop 0.7873 to 0.1673\n",
      "[4] : train loss 0.051776, val loss drop 0.1673 to 0.0243\n",
      "[5] : train loss 0.024986, val loss drop 0.0243 to 0.0169\n",
      "[7] : train loss 0.016451, val loss drop 0.0169 to 0.0114\n",
      "[8] : train loss 0.009606, val loss drop 0.0114 to 0.0078\n",
      "[10] : train loss 0.007031, val loss drop 0.0078 to 0.0062\n",
      "[11] : train loss 0.006677, val loss drop 0.0062 to 0.0054\n",
      "[13] : train loss 0.005041, val loss drop 0.0054 to 0.0048\n",
      "[15] : train loss 0.005306, val loss drop 0.0048 to 0.0046\n",
      "[17] : train loss 0.003798, val loss drop 0.0046 to 0.0036\n",
      "[21] : train loss 0.002774, val loss drop 0.0036 to 0.0032\n",
      "[24] : train loss 0.002629, val loss drop 0.0032 to 0.0028\n",
      "[28] : train loss 0.002018, val loss drop 0.0028 to 0.0026\n",
      "[29] : train loss 0.002657, val loss drop 0.0026 to 0.0022\n",
      "[37] : train loss 0.002778, val loss drop 0.0022 to 0.0022\n",
      "[40] : train loss 0.002968, val loss drop 0.0022 to 0.0022\n",
      "[41] : train loss 0.002129, val loss drop 0.0022 to 0.0016\n",
      "[50] : train loss 0.004958, val loss drop 0.0016 to 0.0016\n",
      "[51] : train loss 0.002234, val loss drop 0.0016 to 0.0014\n",
      "[63] : train loss 0.002941, val loss drop 0.0014 to 0.0012\n",
      "[66] : train loss 0.001816, val loss drop 0.0012 to 0.0011\n",
      "[68] : train loss 0.001157, val loss drop 0.0011 to 0.0011\n",
      "[75] : train loss 0.002058, val loss drop 0.0011 to 0.0011\n",
      "fold 5\n",
      "[1] : train loss 2.201853, val loss drop 10000000.0000 to 6.2392\n",
      "[2] : train loss 0.316773, val loss drop 6.2392 to 0.2605\n",
      "[3] : train loss 0.138749, val loss drop 0.2605 to 0.0542\n",
      "[4] : train loss 0.057246, val loss drop 0.0542 to 0.0386\n",
      "[6] : train loss 0.024488, val loss drop 0.0386 to 0.0182\n",
      "[7] : train loss 0.009402, val loss drop 0.0182 to 0.0122\n",
      "[8] : train loss 0.009630, val loss drop 0.0122 to 0.0102\n",
      "[9] : train loss 0.006785, val loss drop 0.0102 to 0.0074\n",
      "[10] : train loss 0.007141, val loss drop 0.0074 to 0.0047\n",
      "[12] : train loss 0.005592, val loss drop 0.0047 to 0.0047\n",
      "[20] : train loss 0.005447, val loss drop 0.0047 to 0.0035\n",
      "[21] : train loss 0.007889, val loss drop 0.0035 to 0.0035\n",
      "[30] : train loss 0.016715, val loss drop 0.0035 to 0.0033\n",
      "[38] : train loss 0.005168, val loss drop 0.0033 to 0.0028\n",
      "[53] : train loss 0.005894, val loss drop 0.0028 to 0.0022\n",
      "[70] : train loss 0.002564, val loss drop 0.0022 to 0.0021\n",
      "[80] : train loss 0.003173, val loss drop 0.0021 to 0.0015\n",
      "[96] : train loss 0.002523, val loss drop 0.0015 to 0.0014\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 42.327029, val loss drop 10000000.0000 to 12.1459\n",
      "[3] : train loss 1.609204, val loss drop 12.1459 to 3.2359\n",
      "[4] : train loss 1.062548, val loss drop 3.2359 to 3.1751\n",
      "[5] : train loss 0.729052, val loss drop 3.1751 to 0.6840\n",
      "[6] : train loss 0.440276, val loss drop 0.6840 to 0.3417\n",
      "[7] : train loss 0.372138, val loss drop 0.3417 to 0.2559\n",
      "[10] : train loss 0.432131, val loss drop 0.2559 to 0.2064\n",
      "[11] : train loss 0.361478, val loss drop 0.2064 to 0.1712\n",
      "[18] : train loss 0.535697, val loss drop 0.1712 to 0.1192\n",
      "[24] : train loss 0.229534, val loss drop 0.1192 to 0.0892\n",
      "[39] : train loss 0.228258, val loss drop 0.0892 to 0.0720\n",
      "[44] : train loss 0.103372, val loss drop 0.0720 to 0.0503\n",
      "[58] : train loss 0.130658, val loss drop 0.0503 to 0.0416\n",
      "[80] : train loss 0.078015, val loss drop 0.0416 to 0.0370\n",
      "[100] : train loss 0.097459, val loss drop 0.0370 to 0.0261\n",
      "fold 2\n",
      "[1] : train loss 38.939474, val loss drop 10000000.0000 to 9.1608\n",
      "[3] : train loss 2.160990, val loss drop 9.1608 to 3.2637\n",
      "[4] : train loss 1.172834, val loss drop 3.2637 to 1.0574\n",
      "[5] : train loss 0.860559, val loss drop 1.0574 to 0.5463\n",
      "[8] : train loss 0.397014, val loss drop 0.5463 to 0.2261\n",
      "[15] : train loss 0.440477, val loss drop 0.2261 to 0.1083\n",
      "[17] : train loss 0.187720, val loss drop 0.1083 to 0.0953\n",
      "[20] : train loss 0.302839, val loss drop 0.0953 to 0.0904\n",
      "[28] : train loss 0.229331, val loss drop 0.0904 to 0.0532\n",
      "[30] : train loss 0.331686, val loss drop 0.0532 to 0.0488\n",
      "[34] : train loss 0.111742, val loss drop 0.0488 to 0.0486\n",
      "[60] : train loss 0.204039, val loss drop 0.0486 to 0.0463\n",
      "[72] : train loss 0.111702, val loss drop 0.0463 to 0.0453\n",
      "[83] : train loss 0.296524, val loss drop 0.0453 to 0.0386\n",
      "[93] : train loss 0.176765, val loss drop 0.0386 to 0.0238\n",
      "[95] : train loss 0.082060, val loss drop 0.0238 to 0.0211\n",
      "[97] : train loss 0.063910, val loss drop 0.0211 to 0.0173\n",
      "fold 3\n",
      "[1] : train loss 33.889837, val loss drop 10000000.0000 to 13.5836\n",
      "[2] : train loss 4.245796, val loss drop 13.5836 to 6.3066\n",
      "[3] : train loss 1.445036, val loss drop 6.3066 to 1.5109\n",
      "[4] : train loss 1.024261, val loss drop 1.5109 to 1.4352\n",
      "[5] : train loss 0.662178, val loss drop 1.4352 to 0.8399\n",
      "[6] : train loss 0.524306, val loss drop 0.8399 to 0.3218\n",
      "[7] : train loss 0.541323, val loss drop 0.3218 to 0.2399\n",
      "[9] : train loss 0.329415, val loss drop 0.2399 to 0.2024\n",
      "[13] : train loss 0.417125, val loss drop 0.2024 to 0.1912\n",
      "[28] : train loss 0.111357, val loss drop 0.1912 to 0.1893\n",
      "[29] : train loss 0.438091, val loss drop 0.1893 to 0.1417\n",
      "[30] : train loss 0.429386, val loss drop 0.1417 to 0.0829\n",
      "[37] : train loss 0.143979, val loss drop 0.0829 to 0.0582\n",
      "[43] : train loss 0.256194, val loss drop 0.0582 to 0.0398\n",
      "[46] : train loss 0.104952, val loss drop 0.0398 to 0.0275\n",
      "[91] : train loss 0.072817, val loss drop 0.0275 to 0.0273\n",
      "fold 4\n",
      "[1] : train loss 35.405820, val loss drop 10000000.0000 to 12.8515\n",
      "[2] : train loss 5.996608, val loss drop 12.8515 to 2.3355\n",
      "[3] : train loss 2.193652, val loss drop 2.3355 to 1.5497\n",
      "[4] : train loss 1.175200, val loss drop 1.5497 to 1.0733\n",
      "[5] : train loss 0.710072, val loss drop 1.0733 to 0.4732\n",
      "[9] : train loss 0.814717, val loss drop 0.4732 to 0.4185\n",
      "[10] : train loss 0.272095, val loss drop 0.4185 to 0.2311\n",
      "[13] : train loss 0.325842, val loss drop 0.2311 to 0.1916\n",
      "[15] : train loss 0.325000, val loss drop 0.1916 to 0.1474\n",
      "[18] : train loss 0.470675, val loss drop 0.1474 to 0.1094\n",
      "[19] : train loss 0.255548, val loss drop 0.1094 to 0.0884\n",
      "[50] : train loss 0.505586, val loss drop 0.0884 to 0.0499\n",
      "[63] : train loss 0.096127, val loss drop 0.0499 to 0.0269\n",
      "[93] : train loss 0.113165, val loss drop 0.0269 to 0.0204\n",
      "[94] : train loss 0.104558, val loss drop 0.0204 to 0.0186\n",
      "fold 5\n",
      "[1] : train loss 36.223154, val loss drop 10000000.0000 to 18.1591\n",
      "[3] : train loss 2.033426, val loss drop 18.1591 to 1.3522\n",
      "[4] : train loss 1.014880, val loss drop 1.3522 to 0.5650\n",
      "[5] : train loss 0.642850, val loss drop 0.5650 to 0.4106\n",
      "[6] : train loss 0.618016, val loss drop 0.4106 to 0.3378\n",
      "[7] : train loss 0.358381, val loss drop 0.3378 to 0.2044\n",
      "[9] : train loss 0.599746, val loss drop 0.2044 to 0.1842\n",
      "[11] : train loss 0.445022, val loss drop 0.1842 to 0.1188\n",
      "[15] : train loss 0.389207, val loss drop 0.1188 to 0.0696\n",
      "[32] : train loss 0.237954, val loss drop 0.0696 to 0.0490\n",
      "[40] : train loss 0.109066, val loss drop 0.0490 to 0.0341\n",
      "[52] : train loss 0.144626, val loss drop 0.0341 to 0.0196\n",
      "[98] : train loss 0.202810, val loss drop 0.0196 to 0.0162\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}\n",
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xy': 0.000625143100911778,\n",
       " 'v': 0.0010803749855117096,\n",
       " 'm': 0.02109942916598308}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_model # leaky relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xy': 0.000880345263100234,\n",
       " 'v': 0.0009727095718393787,\n",
       " 'm': 0.01579595682751895}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_model #elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xy': 0.0006840781165308819,\n",
       " 'v': 0.0010359705444652266,\n",
       " 'm': 0.023326632654456333}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_model # relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xy : leakyrelu > relu > elu\n",
    "- v : elu > relu > leakyrelu\n",
    "- m : elu > leakyrelu > relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "#conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "#fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "#model = cnn_model(conv, fc)\n",
    "fe = custom_cnn()\n",
    "fc = custom_fc(512*4*4, len(name))\n",
    "model = nn.Sequential(fe, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "#conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "#fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "#model = cnn_model(conv, fc)\n",
    "\n",
    "fe = custom_cnn()\n",
    "fc = custom_fc(512*4*4, len(name))\n",
    "model = nn.Sequential(fe, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "#conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "#fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "#model = cnn_model(conv, fc)\n",
    "\n",
    "fe = custom_cnn()\n",
    "fc = custom_fc(512*4*4, len(name))\n",
    "model = nn.Sequential(fe, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, '{}.csv'.format(save_path.split('/')[-1])), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

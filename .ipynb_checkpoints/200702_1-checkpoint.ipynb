{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- s1, s2, s3, s4 max min값을 fc layer에서 concat(fc + bn) : m이 제일 안나옴;;\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "def fix_seed(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_model = nn.Sequential(\n",
    "    nn.Conv2d(2, 32, (1, 4)),\n",
    "    nn.ELU(),\n",
    "    nn.MaxPool2d((2, 1)),\n",
    "    nn.Conv2d(32, 64, (4, 1)),\n",
    "    nn.ELU(),\n",
    "    nn.MaxPool2d((2, 1)),\n",
    "    nn.Conv2d(64, 128, (4, 1)),\n",
    "    nn.ELU(),\n",
    "    nn.MaxPool2d((2, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        \n",
    "        max_s = batchX[0,:,1:5].max(axis = 0)\n",
    "        min_s =  batchX[0,:,1:5].min(axis = 0)\n",
    "        batchX_add = np.concatenate((max_s, min_s), axis = -1)\n",
    "        \n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchX_add, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, x_add, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        x_add = x_add.cuda()\n",
    "        \n",
    "        pred = model(x, x_add)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, x_add, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            x_add = x_add.cuda()\n",
    "            \n",
    "            pred = model(x, x_add)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cnn_v2(nn.Module):\n",
    "    def __init__(self, n_feature, add_feature, out_len):\n",
    "        super(custom_cnn_v2, self).__init__()\n",
    "        self.conv_kernel = (4, 1)\n",
    "        self.pool_kernel = (2, 1)\n",
    "        \n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*2*n_feature + add_feature, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, out_len)\n",
    "        )\n",
    "        \n",
    "        self.add_embed = nn.Sequential(\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm1d(8)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, x_add):\n",
    "        fe_out = self.fe(x)\n",
    "        fe_out = fe_out.view(fe_out.size(0), -1)\n",
    "        fe_out = torch.cat((fe_out, self.add_embed(x_add)), axis = -1)\n",
    "        return self.fc(fe_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cnn(nn.Module):\n",
    "    def __init__(self, n_feature, out_len):\n",
    "        super(custom_cnn, self).__init__()\n",
    "        self.conv_kernel = (4, 1)\n",
    "        self.pool_kernel = (2, 1)\n",
    "        \n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*2*n_feature, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, out_len)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.fe(x)\n",
    "        return self.fc(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis = 2):\n",
    "    mu = np.expand_dims(x.mean(axis = 2), axis = axis)\n",
    "    sd = np.expand_dims(x.std(axis = 2), axis = axis)\n",
    "\n",
    "    normalized = (x - mu) / sd\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "# concatenate normalized data\n",
    "train_norm = normalize(train_f)\n",
    "test_norm = normalize(test_f)\n",
    "\n",
    "train_f = np.concatenate((train_f, train_norm), axis = 1)\n",
    "test_f = np.concatenate((test_f, test_norm), axis = 1)\n",
    "\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        model = custom_cnn_v2(n_features, 8, len(name))\n",
    "        #model = get_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 3.163335, val loss drop 10000000.0000 to 3.2723\n",
      "[2] : train loss 3.009837, val loss drop 3.2723 to 3.1972\n",
      "[3] : train loss 2.861636, val loss drop 3.1972 to 2.9796\n",
      "[4] : train loss 2.668565, val loss drop 2.9796 to 2.7357\n",
      "[5] : train loss 2.432578, val loss drop 2.7357 to 2.1950\n",
      "[6] : train loss 2.152043, val loss drop 2.1950 to 2.1166\n",
      "[7] : train loss 1.835612, val loss drop 2.1166 to 1.5799\n",
      "[8] : train loss 1.495743, val loss drop 1.5799 to 1.3194\n",
      "[9] : train loss 1.149792, val loss drop 1.3194 to 0.9982\n",
      "[10] : train loss 0.822798, val loss drop 0.9982 to 0.5456\n",
      "[11] : train loss 0.517393, val loss drop 0.5456 to 0.2661\n",
      "[12] : train loss 0.276473, val loss drop 0.2661 to 0.1186\n",
      "[13] : train loss 0.119750, val loss drop 0.1186 to 0.0277\n",
      "[14] : train loss 0.038910, val loss drop 0.0277 to 0.0098\n",
      "[16] : train loss 0.016821, val loss drop 0.0098 to 0.0065\n",
      "[19] : train loss 0.013987, val loss drop 0.0065 to 0.0065\n",
      "[20] : train loss 0.009370, val loss drop 0.0065 to 0.0049\n",
      "[22] : train loss 0.019236, val loss drop 0.0049 to 0.0041\n",
      "[23] : train loss 0.012027, val loss drop 0.0041 to 0.0034\n",
      "[28] : train loss 0.012551, val loss drop 0.0034 to 0.0033\n",
      "[34] : train loss 0.010745, val loss drop 0.0033 to 0.0028\n",
      "[39] : train loss 0.009654, val loss drop 0.0028 to 0.0021\n",
      "[49] : train loss 0.011368, val loss drop 0.0021 to 0.0016\n",
      "[61] : train loss 0.013119, val loss drop 0.0016 to 0.0015\n",
      "[77] : train loss 0.006180, val loss drop 0.0015 to 0.0013\n",
      "[78] : train loss 0.010900, val loss drop 0.0013 to 0.0012\n",
      "[92] : train loss 0.011245, val loss drop 0.0012 to 0.0010\n",
      "fold 2\n",
      "[1] : train loss 3.173270, val loss drop 10000000.0000 to 2.7545\n",
      "[2] : train loss 3.001072, val loss drop 2.7545 to 2.6261\n",
      "[3] : train loss 2.836062, val loss drop 2.6261 to 2.2992\n",
      "[5] : train loss 2.390951, val loss drop 2.2992 to 1.9686\n",
      "[6] : train loss 2.109912, val loss drop 1.9686 to 1.9292\n",
      "[7] : train loss 1.795606, val loss drop 1.9292 to 1.4473\n",
      "[8] : train loss 1.469318, val loss drop 1.4473 to 1.3459\n",
      "[9] : train loss 1.170079, val loss drop 1.3459 to 0.8686\n",
      "[10] : train loss 0.922289, val loss drop 0.8686 to 0.6914\n",
      "[11] : train loss 0.710521, val loss drop 0.6914 to 0.5781\n",
      "[12] : train loss 0.541036, val loss drop 0.5781 to 0.4697\n",
      "[13] : train loss 0.378676, val loss drop 0.4697 to 0.1952\n",
      "[14] : train loss 0.231940, val loss drop 0.1952 to 0.1250\n",
      "[15] : train loss 0.122222, val loss drop 0.1250 to 0.0529\n",
      "[16] : train loss 0.047560, val loss drop 0.0529 to 0.0173\n",
      "[17] : train loss 0.019191, val loss drop 0.0173 to 0.0096\n",
      "[19] : train loss 0.020962, val loss drop 0.0096 to 0.0095\n",
      "[20] : train loss 0.018065, val loss drop 0.0095 to 0.0046\n",
      "[22] : train loss 0.010027, val loss drop 0.0046 to 0.0031\n",
      "[27] : train loss 0.011671, val loss drop 0.0031 to 0.0024\n",
      "[38] : train loss 0.014817, val loss drop 0.0024 to 0.0020\n",
      "[52] : train loss 0.010793, val loss drop 0.0020 to 0.0018\n",
      "[55] : train loss 0.006178, val loss drop 0.0018 to 0.0015\n",
      "[71] : train loss 0.010636, val loss drop 0.0015 to 0.0015\n",
      "[78] : train loss 0.011251, val loss drop 0.0015 to 0.0013\n",
      "[90] : train loss 0.008232, val loss drop 0.0013 to 0.0009\n",
      "[100] : train loss 0.004704, val loss drop 0.0009 to 0.0009\n",
      "fold 3\n",
      "[1] : train loss 3.164498, val loss drop 10000000.0000 to 2.7491\n",
      "[3] : train loss 2.847420, val loss drop 2.7491 to 2.6091\n",
      "[4] : train loss 2.664040, val loss drop 2.6091 to 2.4537\n",
      "[5] : train loss 2.435967, val loss drop 2.4537 to 2.2379\n",
      "[6] : train loss 2.171581, val loss drop 2.2379 to 1.7384\n",
      "[7] : train loss 1.884098, val loss drop 1.7384 to 1.6860\n",
      "[8] : train loss 1.588071, val loss drop 1.6860 to 1.3754\n",
      "[9] : train loss 1.289989, val loss drop 1.3754 to 1.0257\n",
      "[10] : train loss 0.991290, val loss drop 1.0257 to 0.6519\n",
      "[11] : train loss 0.710861, val loss drop 0.6519 to 0.4454\n",
      "[12] : train loss 0.466617, val loss drop 0.4454 to 0.2646\n",
      "[13] : train loss 0.274890, val loss drop 0.2646 to 0.1712\n",
      "[14] : train loss 0.144675, val loss drop 0.1712 to 0.0686\n",
      "[15] : train loss 0.070122, val loss drop 0.0686 to 0.0321\n",
      "[16] : train loss 0.039578, val loss drop 0.0321 to 0.0277\n",
      "[17] : train loss 0.027471, val loss drop 0.0277 to 0.0230\n",
      "[18] : train loss 0.023472, val loss drop 0.0230 to 0.0157\n",
      "[20] : train loss 0.020864, val loss drop 0.0157 to 0.0075\n",
      "[21] : train loss 0.013766, val loss drop 0.0075 to 0.0067\n",
      "[22] : train loss 0.012972, val loss drop 0.0067 to 0.0042\n",
      "[24] : train loss 0.010504, val loss drop 0.0042 to 0.0042\n",
      "[28] : train loss 0.019184, val loss drop 0.0042 to 0.0027\n",
      "[36] : train loss 0.013936, val loss drop 0.0027 to 0.0026\n",
      "[41] : train loss 0.011817, val loss drop 0.0026 to 0.0024\n",
      "[47] : train loss 0.017222, val loss drop 0.0024 to 0.0019\n",
      "[55] : train loss 0.009438, val loss drop 0.0019 to 0.0018\n",
      "[78] : train loss 0.007417, val loss drop 0.0018 to 0.0017\n",
      "[81] : train loss 0.011403, val loss drop 0.0017 to 0.0015\n",
      "[93] : train loss 0.004794, val loss drop 0.0015 to 0.0014\n",
      "[100] : train loss 0.006107, val loss drop 0.0014 to 0.0011\n",
      "fold 4\n",
      "[1] : train loss 3.192295, val loss drop 10000000.0000 to 2.9388\n",
      "[2] : train loss 3.032582, val loss drop 2.9388 to 2.8179\n",
      "[3] : train loss 2.888838, val loss drop 2.8179 to 2.6905\n",
      "[4] : train loss 2.712472, val loss drop 2.6905 to 2.4994\n",
      "[5] : train loss 2.488510, val loss drop 2.4994 to 2.1056\n",
      "[6] : train loss 2.210797, val loss drop 2.1056 to 2.0289\n",
      "[7] : train loss 1.884333, val loss drop 2.0289 to 1.6845\n",
      "[8] : train loss 1.530745, val loss drop 1.6845 to 1.3427\n",
      "[9] : train loss 1.160425, val loss drop 1.3427 to 0.9330\n",
      "[10] : train loss 0.803279, val loss drop 0.9330 to 0.5237\n",
      "[11] : train loss 0.482112, val loss drop 0.5237 to 0.2940\n",
      "[12] : train loss 0.236455, val loss drop 0.2940 to 0.1182\n",
      "[13] : train loss 0.085162, val loss drop 0.1182 to 0.0278\n",
      "[14] : train loss 0.028569, val loss drop 0.0278 to 0.0116\n",
      "[17] : train loss 0.015147, val loss drop 0.0116 to 0.0064\n",
      "[21] : train loss 0.014488, val loss drop 0.0064 to 0.0053\n",
      "[26] : train loss 0.012603, val loss drop 0.0053 to 0.0043\n",
      "[28] : train loss 0.012282, val loss drop 0.0043 to 0.0030\n",
      "[29] : train loss 0.014355, val loss drop 0.0030 to 0.0029\n",
      "[30] : train loss 0.008856, val loss drop 0.0029 to 0.0029\n",
      "[32] : train loss 0.012503, val loss drop 0.0029 to 0.0024\n",
      "[38] : train loss 0.006898, val loss drop 0.0024 to 0.0024\n",
      "[39] : train loss 0.010259, val loss drop 0.0024 to 0.0022\n",
      "[43] : train loss 0.011522, val loss drop 0.0022 to 0.0020\n",
      "[47] : train loss 0.011660, val loss drop 0.0020 to 0.0018\n",
      "[55] : train loss 0.012909, val loss drop 0.0018 to 0.0017\n",
      "[70] : train loss 0.005345, val loss drop 0.0017 to 0.0013\n",
      "[78] : train loss 0.010322, val loss drop 0.0013 to 0.0010\n",
      "fold 5\n",
      "[1] : train loss 3.196741, val loss drop 10000000.0000 to 2.9292\n",
      "[2] : train loss 3.043977, val loss drop 2.9292 to 2.9287\n",
      "[3] : train loss 2.906385, val loss drop 2.9287 to 2.5683\n",
      "[5] : train loss 2.521176, val loss drop 2.5683 to 2.5181\n",
      "[6] : train loss 2.270871, val loss drop 2.5181 to 2.1651\n",
      "[7] : train loss 1.994924, val loss drop 2.1651 to 1.9277\n",
      "[8] : train loss 1.697703, val loss drop 1.9277 to 1.3879\n",
      "[9] : train loss 1.377196, val loss drop 1.3879 to 1.2386\n",
      "[10] : train loss 1.050291, val loss drop 1.2386 to 0.9322\n",
      "[11] : train loss 0.730802, val loss drop 0.9322 to 0.4314\n",
      "[12] : train loss 0.448608, val loss drop 0.4314 to 0.3092\n",
      "[13] : train loss 0.239107, val loss drop 0.3092 to 0.1179\n",
      "[14] : train loss 0.106503, val loss drop 0.1179 to 0.0295\n",
      "[15] : train loss 0.037829, val loss drop 0.0295 to 0.0112\n",
      "[16] : train loss 0.015155, val loss drop 0.0112 to 0.0110\n",
      "[18] : train loss 0.012545, val loss drop 0.0110 to 0.0085\n",
      "[19] : train loss 0.015119, val loss drop 0.0085 to 0.0076\n",
      "[20] : train loss 0.016968, val loss drop 0.0076 to 0.0056\n",
      "[23] : train loss 0.016697, val loss drop 0.0056 to 0.0048\n",
      "[24] : train loss 0.012209, val loss drop 0.0048 to 0.0045\n",
      "[27] : train loss 0.016698, val loss drop 0.0045 to 0.0044\n",
      "[29] : train loss 0.008735, val loss drop 0.0044 to 0.0028\n",
      "[30] : train loss 0.008975, val loss drop 0.0028 to 0.0026\n",
      "[31] : train loss 0.011983, val loss drop 0.0026 to 0.0026\n",
      "[33] : train loss 0.011238, val loss drop 0.0026 to 0.0026\n",
      "[43] : train loss 0.010100, val loss drop 0.0026 to 0.0019\n",
      "[60] : train loss 0.006411, val loss drop 0.0019 to 0.0019\n",
      "[70] : train loss 0.009863, val loss drop 0.0019 to 0.0017\n",
      "[71] : train loss 0.008869, val loss drop 0.0017 to 0.0016\n",
      "[73] : train loss 0.008858, val loss drop 0.0016 to 0.0016\n",
      "[75] : train loss 0.009614, val loss drop 0.0016 to 0.0012\n",
      "[91] : train loss 0.006611, val loss drop 0.0012 to 0.0011\n",
      "fold 6\n",
      "[1] : train loss 3.162818, val loss drop 10000000.0000 to 2.9209\n",
      "[2] : train loss 3.003227, val loss drop 2.9209 to 2.6577\n",
      "[4] : train loss 2.662727, val loss drop 2.6577 to 2.5310\n",
      "[5] : train loss 2.419816, val loss drop 2.5310 to 2.3065\n",
      "[6] : train loss 2.137745, val loss drop 2.3065 to 2.0114\n",
      "[7] : train loss 1.831267, val loss drop 2.0114 to 1.8534\n",
      "[8] : train loss 1.519397, val loss drop 1.8534 to 1.3243\n",
      "[9] : train loss 1.232939, val loss drop 1.3243 to 1.0036\n",
      "[10] : train loss 0.988809, val loss drop 1.0036 to 0.8526\n",
      "[11] : train loss 0.798930, val loss drop 0.8526 to 0.6687\n",
      "[12] : train loss 0.630664, val loss drop 0.6687 to 0.5106\n",
      "[13] : train loss 0.481192, val loss drop 0.5106 to 0.2468\n",
      "[15] : train loss 0.270906, val loss drop 0.2468 to 0.1882\n",
      "[16] : train loss 0.193423, val loss drop 0.1882 to 0.1230\n",
      "[17] : train loss 0.120510, val loss drop 0.1230 to 0.0556\n",
      "[18] : train loss 0.071122, val loss drop 0.0556 to 0.0352\n",
      "[19] : train loss 0.043913, val loss drop 0.0352 to 0.0208\n",
      "[20] : train loss 0.027854, val loss drop 0.0208 to 0.0156\n",
      "[23] : train loss 0.021516, val loss drop 0.0156 to 0.0130\n",
      "[24] : train loss 0.017053, val loss drop 0.0130 to 0.0074\n",
      "[25] : train loss 0.014530, val loss drop 0.0074 to 0.0057\n",
      "[27] : train loss 0.016587, val loss drop 0.0057 to 0.0041\n",
      "[28] : train loss 0.021614, val loss drop 0.0041 to 0.0030\n",
      "[34] : train loss 0.010261, val loss drop 0.0030 to 0.0018\n",
      "[82] : train loss 0.005793, val loss drop 0.0018 to 0.0017\n",
      "[85] : train loss 0.010181, val loss drop 0.0017 to 0.0012\n",
      "fold 7\n",
      "[1] : train loss 3.182431, val loss drop 10000000.0000 to 2.5517\n",
      "[3] : train loss 2.858587, val loss drop 2.5517 to 2.3527\n",
      "[5] : train loss 2.410581, val loss drop 2.3527 to 2.0593\n",
      "[6] : train loss 2.118212, val loss drop 2.0593 to 1.8301\n",
      "[7] : train loss 1.803085, val loss drop 1.8301 to 1.4185\n",
      "[8] : train loss 1.471114, val loss drop 1.4185 to 1.0956\n",
      "[9] : train loss 1.138240, val loss drop 1.0956 to 0.9529\n",
      "[10] : train loss 0.815463, val loss drop 0.9529 to 0.5660\n",
      "[11] : train loss 0.517304, val loss drop 0.5660 to 0.2664\n",
      "[12] : train loss 0.278255, val loss drop 0.2664 to 0.0868\n",
      "[13] : train loss 0.111583, val loss drop 0.0868 to 0.0177\n",
      "[15] : train loss 0.026703, val loss drop 0.0177 to 0.0139\n",
      "[17] : train loss 0.018950, val loss drop 0.0139 to 0.0063\n",
      "[18] : train loss 0.017054, val loss drop 0.0063 to 0.0049\n",
      "[21] : train loss 0.013232, val loss drop 0.0049 to 0.0033\n",
      "[26] : train loss 0.008695, val loss drop 0.0033 to 0.0028\n",
      "[30] : train loss 0.018653, val loss drop 0.0028 to 0.0025\n",
      "[31] : train loss 0.011625, val loss drop 0.0025 to 0.0022\n",
      "[32] : train loss 0.010611, val loss drop 0.0022 to 0.0019\n",
      "[44] : train loss 0.010242, val loss drop 0.0019 to 0.0016\n",
      "[67] : train loss 0.009278, val loss drop 0.0016 to 0.0014\n",
      "[74] : train loss 0.014283, val loss drop 0.0014 to 0.0013\n",
      "fold 8\n",
      "[1] : train loss 3.164623, val loss drop 10000000.0000 to 3.0711\n",
      "[2] : train loss 2.990579, val loss drop 3.0711 to 2.8057\n",
      "[3] : train loss 2.825505, val loss drop 2.8057 to 2.7025\n",
      "[5] : train loss 2.383355, val loss drop 2.7025 to 2.1477\n",
      "[6] : train loss 2.084515, val loss drop 2.1477 to 1.9348\n",
      "[7] : train loss 1.734524, val loss drop 1.9348 to 1.5387\n",
      "[8] : train loss 1.353611, val loss drop 1.5387 to 1.0312\n",
      "[9] : train loss 0.961251, val loss drop 1.0312 to 0.7867\n",
      "[10] : train loss 0.596518, val loss drop 0.7867 to 0.3759\n",
      "[11] : train loss 0.309470, val loss drop 0.3759 to 0.1583\n",
      "[12] : train loss 0.125750, val loss drop 0.1583 to 0.0348\n",
      "[13] : train loss 0.046137, val loss drop 0.0348 to 0.0124\n",
      "[16] : train loss 0.023126, val loss drop 0.0124 to 0.0068\n",
      "[17] : train loss 0.017299, val loss drop 0.0068 to 0.0045\n",
      "[19] : train loss 0.015508, val loss drop 0.0045 to 0.0041\n",
      "[22] : train loss 0.011799, val loss drop 0.0041 to 0.0038\n",
      "[24] : train loss 0.010428, val loss drop 0.0038 to 0.0033\n",
      "[26] : train loss 0.014643, val loss drop 0.0033 to 0.0028\n",
      "[36] : train loss 0.008444, val loss drop 0.0028 to 0.0019\n",
      "[52] : train loss 0.006530, val loss drop 0.0019 to 0.0016\n",
      "[60] : train loss 0.010289, val loss drop 0.0016 to 0.0015\n",
      "[95] : train loss 0.005304, val loss drop 0.0015 to 0.0012\n",
      "fold 9\n",
      "[1] : train loss 3.163533, val loss drop 10000000.0000 to 3.3694\n",
      "[2] : train loss 3.010530, val loss drop 3.3694 to 2.7248\n",
      "[5] : train loss 2.460710, val loss drop 2.7248 to 2.2726\n",
      "[6] : train loss 2.189219, val loss drop 2.2726 to 1.8902\n",
      "[7] : train loss 1.875398, val loss drop 1.8902 to 1.5179\n",
      "[9] : train loss 1.203731, val loss drop 1.5179 to 1.1009\n",
      "[10] : train loss 0.888526, val loss drop 1.1009 to 0.8080\n",
      "[11] : train loss 0.625231, val loss drop 0.8080 to 0.3688\n",
      "[12] : train loss 0.396654, val loss drop 0.3688 to 0.3131\n",
      "[13] : train loss 0.228618, val loss drop 0.3131 to 0.1053\n",
      "[14] : train loss 0.103973, val loss drop 0.1053 to 0.0660\n",
      "[15] : train loss 0.047073, val loss drop 0.0660 to 0.0145\n",
      "[17] : train loss 0.016936, val loss drop 0.0145 to 0.0125\n",
      "[18] : train loss 0.012660, val loss drop 0.0125 to 0.0053\n",
      "[19] : train loss 0.011206, val loss drop 0.0053 to 0.0045\n",
      "[20] : train loss 0.013596, val loss drop 0.0045 to 0.0039\n",
      "[22] : train loss 0.011440, val loss drop 0.0039 to 0.0034\n",
      "[26] : train loss 0.011465, val loss drop 0.0034 to 0.0027\n",
      "[35] : train loss 0.015353, val loss drop 0.0027 to 0.0022\n",
      "[47] : train loss 0.011596, val loss drop 0.0022 to 0.0019\n",
      "[48] : train loss 0.009411, val loss drop 0.0019 to 0.0019\n",
      "[54] : train loss 0.011607, val loss drop 0.0019 to 0.0014\n",
      "[59] : train loss 0.011275, val loss drop 0.0014 to 0.0013\n",
      "[62] : train loss 0.012789, val loss drop 0.0013 to 0.0012\n",
      "[67] : train loss 0.008148, val loss drop 0.0012 to 0.0011\n",
      "[99] : train loss 0.006706, val loss drop 0.0011 to 0.0010\n",
      "fold 10\n",
      "[1] : train loss 3.138298, val loss drop 10000000.0000 to 3.2833\n",
      "[2] : train loss 2.942031, val loss drop 3.2833 to 2.6298\n",
      "[4] : train loss 2.547848, val loss drop 2.6298 to 2.4075\n",
      "[5] : train loss 2.287076, val loss drop 2.4075 to 2.2761\n",
      "[6] : train loss 1.968660, val loss drop 2.2761 to 1.6710\n",
      "[7] : train loss 1.605914, val loss drop 1.6710 to 1.2123\n",
      "[8] : train loss 1.216007, val loss drop 1.2123 to 1.0584\n",
      "[9] : train loss 0.832060, val loss drop 1.0584 to 0.5971\n",
      "[10] : train loss 0.487390, val loss drop 0.5971 to 0.2919\n",
      "[11] : train loss 0.229673, val loss drop 0.2919 to 0.0774\n",
      "[12] : train loss 0.079614, val loss drop 0.0774 to 0.0242\n",
      "[13] : train loss 0.027799, val loss drop 0.0242 to 0.0081\n",
      "[17] : train loss 0.019946, val loss drop 0.0081 to 0.0058\n",
      "[19] : train loss 0.013208, val loss drop 0.0058 to 0.0046\n",
      "[20] : train loss 0.020563, val loss drop 0.0046 to 0.0036\n",
      "[23] : train loss 0.010312, val loss drop 0.0036 to 0.0035\n",
      "[25] : train loss 0.013985, val loss drop 0.0035 to 0.0021\n",
      "[35] : train loss 0.012810, val loss drop 0.0021 to 0.0017\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 21.949489, val loss drop 10000000.0000 to 4.8959\n",
      "[2] : train loss 0.298162, val loss drop 4.8959 to 1.1873\n",
      "[3] : train loss 0.089572, val loss drop 1.1873 to 0.2192\n",
      "[4] : train loss 0.046526, val loss drop 0.2192 to 0.0473\n",
      "[5] : train loss 0.027298, val loss drop 0.0473 to 0.0453\n",
      "[6] : train loss 0.019768, val loss drop 0.0453 to 0.0152\n",
      "[8] : train loss 0.012644, val loss drop 0.0152 to 0.0111\n",
      "[9] : train loss 0.010034, val loss drop 0.0111 to 0.0106\n",
      "[10] : train loss 0.008740, val loss drop 0.0106 to 0.0075\n",
      "[11] : train loss 0.007334, val loss drop 0.0075 to 0.0063\n",
      "[15] : train loss 0.006528, val loss drop 0.0063 to 0.0047\n",
      "[18] : train loss 0.005437, val loss drop 0.0047 to 0.0037\n",
      "[20] : train loss 0.004607, val loss drop 0.0037 to 0.0030\n",
      "[21] : train loss 0.004356, val loss drop 0.0030 to 0.0029\n",
      "[30] : train loss 0.004503, val loss drop 0.0029 to 0.0029\n",
      "[32] : train loss 0.003671, val loss drop 0.0029 to 0.0026\n",
      "[33] : train loss 0.003330, val loss drop 0.0026 to 0.0023\n",
      "[39] : train loss 0.002921, val loss drop 0.0023 to 0.0022\n",
      "[43] : train loss 0.004431, val loss drop 0.0022 to 0.0022\n",
      "[48] : train loss 0.004007, val loss drop 0.0022 to 0.0014\n",
      "[51] : train loss 0.002406, val loss drop 0.0014 to 0.0011\n",
      "[63] : train loss 0.001634, val loss drop 0.0011 to 0.0011\n",
      "[65] : train loss 0.004311, val loss drop 0.0011 to 0.0010\n",
      "[74] : train loss 0.001494, val loss drop 0.0010 to 0.0009\n",
      "fold 2\n",
      "[1] : train loss 8.201780, val loss drop 10000000.0000 to 16.8443\n",
      "[2] : train loss 0.141630, val loss drop 16.8443 to 0.4894\n",
      "[3] : train loss 0.061509, val loss drop 0.4894 to 0.0724\n",
      "[4] : train loss 0.030519, val loss drop 0.0724 to 0.0292\n",
      "[5] : train loss 0.019288, val loss drop 0.0292 to 0.0174\n",
      "[6] : train loss 0.013525, val loss drop 0.0174 to 0.0133\n",
      "[7] : train loss 0.008537, val loss drop 0.0133 to 0.0047\n",
      "[13] : train loss 0.004150, val loss drop 0.0047 to 0.0020\n",
      "[14] : train loss 0.003964, val loss drop 0.0020 to 0.0018\n",
      "[23] : train loss 0.004044, val loss drop 0.0018 to 0.0018\n",
      "[24] : train loss 0.003803, val loss drop 0.0018 to 0.0011\n",
      "[28] : train loss 0.002933, val loss drop 0.0011 to 0.0010\n",
      "[35] : train loss 0.002916, val loss drop 0.0010 to 0.0009\n",
      "[39] : train loss 0.003610, val loss drop 0.0009 to 0.0009\n",
      "[49] : train loss 0.002503, val loss drop 0.0009 to 0.0006\n",
      "[84] : train loss 0.001935, val loss drop 0.0006 to 0.0005\n",
      "[97] : train loss 0.003995, val loss drop 0.0005 to 0.0004\n",
      "fold 3\n",
      "[1] : train loss 19.944421, val loss drop 10000000.0000 to 12.0663\n",
      "[2] : train loss 0.247729, val loss drop 12.0663 to 0.5170\n",
      "[3] : train loss 0.088726, val loss drop 0.5170 to 0.2440\n",
      "[4] : train loss 0.035206, val loss drop 0.2440 to 0.0449\n",
      "[5] : train loss 0.018151, val loss drop 0.0449 to 0.0220\n",
      "[6] : train loss 0.014100, val loss drop 0.0220 to 0.0115\n",
      "[9] : train loss 0.011294, val loss drop 0.0115 to 0.0054\n",
      "[10] : train loss 0.007393, val loss drop 0.0054 to 0.0039\n",
      "[13] : train loss 0.006501, val loss drop 0.0039 to 0.0031\n",
      "[14] : train loss 0.006041, val loss drop 0.0031 to 0.0028\n",
      "[17] : train loss 0.004751, val loss drop 0.0028 to 0.0022\n",
      "[18] : train loss 0.004699, val loss drop 0.0022 to 0.0020\n",
      "[24] : train loss 0.003511, val loss drop 0.0020 to 0.0015\n",
      "[29] : train loss 0.003245, val loss drop 0.0015 to 0.0012\n",
      "[44] : train loss 0.002297, val loss drop 0.0012 to 0.0010\n",
      "[59] : train loss 0.001624, val loss drop 0.0010 to 0.0009\n",
      "[80] : train loss 0.001443, val loss drop 0.0009 to 0.0009\n",
      "[91] : train loss 0.001592, val loss drop 0.0009 to 0.0006\n",
      "[95] : train loss 0.002166, val loss drop 0.0006 to 0.0005\n",
      "fold 4\n",
      "[1] : train loss 18.384781, val loss drop 10000000.0000 to 2.3816\n",
      "[3] : train loss 0.066778, val loss drop 2.3816 to 0.1960\n",
      "[4] : train loss 0.027431, val loss drop 0.1960 to 0.0292\n",
      "[5] : train loss 0.015161, val loss drop 0.0292 to 0.0265\n",
      "[6] : train loss 0.010682, val loss drop 0.0265 to 0.0069\n",
      "[9] : train loss 0.007212, val loss drop 0.0069 to 0.0062\n",
      "[10] : train loss 0.005966, val loss drop 0.0062 to 0.0045\n",
      "[11] : train loss 0.004760, val loss drop 0.0045 to 0.0032\n",
      "[13] : train loss 0.006548, val loss drop 0.0032 to 0.0030\n",
      "[14] : train loss 0.003646, val loss drop 0.0030 to 0.0029\n",
      "[16] : train loss 0.003693, val loss drop 0.0029 to 0.0020\n",
      "[19] : train loss 0.002688, val loss drop 0.0020 to 0.0018\n",
      "[26] : train loss 0.003384, val loss drop 0.0018 to 0.0017\n",
      "[32] : train loss 0.003141, val loss drop 0.0017 to 0.0015\n",
      "[40] : train loss 0.002389, val loss drop 0.0015 to 0.0013\n",
      "[43] : train loss 0.001532, val loss drop 0.0013 to 0.0011\n",
      "[44] : train loss 0.001569, val loss drop 0.0011 to 0.0008\n",
      "[56] : train loss 0.001642, val loss drop 0.0008 to 0.0008\n",
      "[98] : train loss 0.001698, val loss drop 0.0008 to 0.0007\n",
      "fold 5\n",
      "[1] : train loss 10.700235, val loss drop 10000000.0000 to 13.2557\n",
      "[2] : train loss 0.095310, val loss drop 13.2557 to 1.3091\n",
      "[3] : train loss 0.029796, val loss drop 1.3091 to 0.2562\n",
      "[4] : train loss 0.020033, val loss drop 0.2562 to 0.0452\n",
      "[5] : train loss 0.011210, val loss drop 0.0452 to 0.0170\n",
      "[6] : train loss 0.008081, val loss drop 0.0170 to 0.0076\n",
      "[7] : train loss 0.006461, val loss drop 0.0076 to 0.0060\n",
      "[8] : train loss 0.005124, val loss drop 0.0060 to 0.0049\n",
      "[9] : train loss 0.005412, val loss drop 0.0049 to 0.0040\n",
      "[11] : train loss 0.005121, val loss drop 0.0040 to 0.0034\n",
      "[13] : train loss 0.003645, val loss drop 0.0034 to 0.0026\n",
      "[18] : train loss 0.004113, val loss drop 0.0026 to 0.0023\n",
      "[23] : train loss 0.003307, val loss drop 0.0023 to 0.0022\n",
      "[30] : train loss 0.003117, val loss drop 0.0022 to 0.0021\n",
      "[31] : train loss 0.002007, val loss drop 0.0021 to 0.0016\n",
      "[34] : train loss 0.001961, val loss drop 0.0016 to 0.0013\n",
      "[36] : train loss 0.002843, val loss drop 0.0013 to 0.0012\n",
      "[46] : train loss 0.001630, val loss drop 0.0012 to 0.0009\n",
      "[66] : train loss 0.001819, val loss drop 0.0009 to 0.0007\n",
      "[78] : train loss 0.001807, val loss drop 0.0007 to 0.0007\n",
      "[93] : train loss 0.000972, val loss drop 0.0007 to 0.0006\n",
      "fold 6\n",
      "[1] : train loss 18.624722, val loss drop 10000000.0000 to 12.1380\n",
      "[2] : train loss 0.165055, val loss drop 12.1380 to 8.0685\n",
      "[3] : train loss 0.055810, val loss drop 8.0685 to 0.7155\n",
      "[4] : train loss 0.027264, val loss drop 0.7155 to 0.2021\n",
      "[5] : train loss 0.014900, val loss drop 0.2021 to 0.0326\n",
      "[6] : train loss 0.011091, val loss drop 0.0326 to 0.0226\n",
      "[7] : train loss 0.009094, val loss drop 0.0226 to 0.0092\n",
      "[8] : train loss 0.007537, val loss drop 0.0092 to 0.0067\n",
      "[9] : train loss 0.006851, val loss drop 0.0067 to 0.0066\n",
      "[10] : train loss 0.005896, val loss drop 0.0066 to 0.0065\n",
      "[11] : train loss 0.004837, val loss drop 0.0065 to 0.0054\n",
      "[15] : train loss 0.004513, val loss drop 0.0054 to 0.0037\n",
      "[18] : train loss 0.003614, val loss drop 0.0037 to 0.0033\n",
      "[20] : train loss 0.003746, val loss drop 0.0033 to 0.0023\n",
      "[24] : train loss 0.003541, val loss drop 0.0023 to 0.0019\n",
      "[37] : train loss 0.002869, val loss drop 0.0019 to 0.0013\n",
      "[55] : train loss 0.002559, val loss drop 0.0013 to 0.0010\n",
      "[58] : train loss 0.002346, val loss drop 0.0010 to 0.0009\n",
      "[73] : train loss 0.003003, val loss drop 0.0009 to 0.0007\n",
      "[95] : train loss 0.001237, val loss drop 0.0007 to 0.0006\n",
      "fold 7\n",
      "[1] : train loss 21.308286, val loss drop 10000000.0000 to 2.3176\n",
      "[2] : train loss 0.193474, val loss drop 2.3176 to 0.5613\n",
      "[3] : train loss 0.067605, val loss drop 0.5613 to 0.0621\n",
      "[4] : train loss 0.039709, val loss drop 0.0621 to 0.0318\n",
      "[6] : train loss 0.016444, val loss drop 0.0318 to 0.0180\n",
      "[7] : train loss 0.012363, val loss drop 0.0180 to 0.0132\n",
      "[8] : train loss 0.010092, val loss drop 0.0132 to 0.0115\n",
      "[9] : train loss 0.009657, val loss drop 0.0115 to 0.0066\n",
      "[11] : train loss 0.007117, val loss drop 0.0066 to 0.0063\n",
      "[13] : train loss 0.006800, val loss drop 0.0063 to 0.0055\n",
      "[15] : train loss 0.005387, val loss drop 0.0055 to 0.0040\n",
      "[18] : train loss 0.004605, val loss drop 0.0040 to 0.0038\n",
      "[20] : train loss 0.003928, val loss drop 0.0038 to 0.0032\n",
      "[26] : train loss 0.005881, val loss drop 0.0032 to 0.0031\n",
      "[27] : train loss 0.003950, val loss drop 0.0031 to 0.0027\n",
      "[28] : train loss 0.002545, val loss drop 0.0027 to 0.0024\n",
      "[32] : train loss 0.003267, val loss drop 0.0024 to 0.0022\n",
      "[39] : train loss 0.002092, val loss drop 0.0022 to 0.0021\n",
      "[40] : train loss 0.002576, val loss drop 0.0021 to 0.0018\n",
      "[48] : train loss 0.003196, val loss drop 0.0018 to 0.0014\n",
      "[56] : train loss 0.002593, val loss drop 0.0014 to 0.0014\n",
      "[66] : train loss 0.001813, val loss drop 0.0014 to 0.0013\n",
      "[74] : train loss 0.004105, val loss drop 0.0013 to 0.0013\n",
      "[75] : train loss 0.001793, val loss drop 0.0013 to 0.0011\n",
      "[88] : train loss 0.001624, val loss drop 0.0011 to 0.0009\n",
      "fold 8\n",
      "[1] : train loss 17.675814, val loss drop 10000000.0000 to 2.9290\n",
      "[2] : train loss 0.161338, val loss drop 2.9290 to 0.2618\n",
      "[3] : train loss 0.076733, val loss drop 0.2618 to 0.0539\n",
      "[4] : train loss 0.035188, val loss drop 0.0539 to 0.0285\n",
      "[5] : train loss 0.017238, val loss drop 0.0285 to 0.0139\n",
      "[6] : train loss 0.012119, val loss drop 0.0139 to 0.0088\n",
      "[7] : train loss 0.009111, val loss drop 0.0088 to 0.0084\n",
      "[8] : train loss 0.007762, val loss drop 0.0084 to 0.0083\n",
      "[9] : train loss 0.007299, val loss drop 0.0083 to 0.0063\n",
      "[11] : train loss 0.006006, val loss drop 0.0063 to 0.0051\n",
      "[13] : train loss 0.005445, val loss drop 0.0051 to 0.0048\n",
      "[14] : train loss 0.004500, val loss drop 0.0048 to 0.0047\n",
      "[16] : train loss 0.004014, val loss drop 0.0047 to 0.0044\n",
      "[18] : train loss 0.003938, val loss drop 0.0044 to 0.0032\n",
      "[19] : train loss 0.003146, val loss drop 0.0032 to 0.0024\n",
      "[26] : train loss 0.002093, val loss drop 0.0024 to 0.0019\n",
      "[29] : train loss 0.001787, val loss drop 0.0019 to 0.0016\n",
      "[31] : train loss 0.001819, val loss drop 0.0016 to 0.0014\n",
      "[34] : train loss 0.001568, val loss drop 0.0014 to 0.0012\n",
      "[42] : train loss 0.001189, val loss drop 0.0012 to 0.0012\n",
      "[43] : train loss 0.001435, val loss drop 0.0012 to 0.0009\n",
      "[59] : train loss 0.001168, val loss drop 0.0009 to 0.0009\n",
      "[60] : train loss 0.001265, val loss drop 0.0009 to 0.0006\n",
      "[80] : train loss 0.001407, val loss drop 0.0006 to 0.0006\n",
      "[93] : train loss 0.001822, val loss drop 0.0006 to 0.0005\n",
      "fold 9\n",
      "[1] : train loss 4.211610, val loss drop 10000000.0000 to 9.1651\n",
      "[2] : train loss 0.096699, val loss drop 9.1651 to 0.2757\n",
      "[3] : train loss 0.037279, val loss drop 0.2757 to 0.1185\n",
      "[4] : train loss 0.019927, val loss drop 0.1185 to 0.0173\n",
      "[5] : train loss 0.010350, val loss drop 0.0173 to 0.0077\n",
      "[6] : train loss 0.007402, val loss drop 0.0077 to 0.0074\n",
      "[7] : train loss 0.006047, val loss drop 0.0074 to 0.0046\n",
      "[8] : train loss 0.004527, val loss drop 0.0046 to 0.0042\n",
      "[9] : train loss 0.004013, val loss drop 0.0042 to 0.0036\n",
      "[14] : train loss 0.003438, val loss drop 0.0036 to 0.0020\n",
      "[18] : train loss 0.002486, val loss drop 0.0020 to 0.0017\n",
      "[19] : train loss 0.001961, val loss drop 0.0017 to 0.0014\n",
      "[29] : train loss 0.001781, val loss drop 0.0014 to 0.0014\n",
      "[30] : train loss 0.002450, val loss drop 0.0014 to 0.0012\n",
      "[32] : train loss 0.002549, val loss drop 0.0012 to 0.0008\n",
      "[43] : train loss 0.003432, val loss drop 0.0008 to 0.0007\n",
      "[61] : train loss 0.001127, val loss drop 0.0007 to 0.0006\n",
      "[85] : train loss 0.007019, val loss drop 0.0006 to 0.0006\n",
      "[98] : train loss 0.001140, val loss drop 0.0006 to 0.0005\n",
      "fold 10\n",
      "[1] : train loss 4.093010, val loss drop 10000000.0000 to 1.6818\n",
      "[2] : train loss 0.128568, val loss drop 1.6818 to 0.3785\n",
      "[3] : train loss 0.042838, val loss drop 0.3785 to 0.0688\n",
      "[4] : train loss 0.020388, val loss drop 0.0688 to 0.0114\n",
      "[5] : train loss 0.012410, val loss drop 0.0114 to 0.0098\n",
      "[6] : train loss 0.011481, val loss drop 0.0098 to 0.0069\n",
      "[7] : train loss 0.010539, val loss drop 0.0069 to 0.0041\n",
      "[9] : train loss 0.008546, val loss drop 0.0041 to 0.0040\n",
      "[10] : train loss 0.008664, val loss drop 0.0040 to 0.0036\n",
      "[11] : train loss 0.005458, val loss drop 0.0036 to 0.0031\n",
      "[14] : train loss 0.004532, val loss drop 0.0031 to 0.0029\n",
      "[17] : train loss 0.006573, val loss drop 0.0029 to 0.0029\n",
      "[18] : train loss 0.004587, val loss drop 0.0029 to 0.0022\n",
      "[21] : train loss 0.001955, val loss drop 0.0022 to 0.0015\n",
      "[35] : train loss 0.012749, val loss drop 0.0015 to 0.0011\n",
      "[37] : train loss 0.014170, val loss drop 0.0011 to 0.0009\n",
      "[50] : train loss 0.001931, val loss drop 0.0009 to 0.0007\n",
      "[52] : train loss 0.006430, val loss drop 0.0007 to 0.0006\n",
      "[87] : train loss 0.003486, val loss drop 0.0006 to 0.0005\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 92.108534, val loss drop 10000000.0000 to 65.9074\n",
      "[5] : train loss 48.082489, val loss drop 65.9074 to 50.7943\n",
      "[6] : train loss 33.378146, val loss drop 50.7943 to 32.9175\n",
      "[7] : train loss 19.118137, val loss drop 32.9175 to 24.7330\n",
      "[8] : train loss 8.688341, val loss drop 24.7330 to 6.5551\n",
      "[9] : train loss 3.533169, val loss drop 6.5551 to 2.1816\n",
      "[10] : train loss 2.216246, val loss drop 2.1816 to 1.4830\n",
      "[14] : train loss 1.389720, val loss drop 1.4830 to 1.0970\n",
      "[15] : train loss 1.389614, val loss drop 1.0970 to 0.7544\n",
      "[17] : train loss 0.899555, val loss drop 0.7544 to 0.6051\n",
      "[26] : train loss 1.034605, val loss drop 0.6051 to 0.4461\n",
      "[27] : train loss 0.609966, val loss drop 0.4461 to 0.3608\n",
      "[33] : train loss 0.706781, val loss drop 0.3608 to 0.3414\n",
      "[43] : train loss 0.645900, val loss drop 0.3414 to 0.2178\n",
      "[44] : train loss 0.492218, val loss drop 0.2178 to 0.2114\n",
      "[51] : train loss 0.801813, val loss drop 0.2114 to 0.1550\n",
      "[59] : train loss 0.281225, val loss drop 0.1550 to 0.1190\n",
      "[64] : train loss 0.535740, val loss drop 0.1190 to 0.1120\n",
      "fold 2\n",
      "[1] : train loss 91.686317, val loss drop 10000000.0000 to 66.8369\n",
      "[3] : train loss 72.774932, val loss drop 66.8369 to 61.8361\n",
      "[4] : train loss 60.571388, val loss drop 61.8361 to 51.0954\n",
      "[5] : train loss 46.359150, val loss drop 51.0954 to 41.9387\n",
      "[6] : train loss 32.057439, val loss drop 41.9387 to 20.1282\n",
      "[7] : train loss 17.807284, val loss drop 20.1282 to 19.8548\n",
      "[8] : train loss 7.932310, val loss drop 19.8548 to 4.0522\n",
      "[10] : train loss 1.797488, val loss drop 4.0522 to 3.4003\n",
      "[11] : train loss 1.343362, val loss drop 3.4003 to 0.8858\n",
      "[13] : train loss 0.759616, val loss drop 0.8858 to 0.4888\n",
      "[17] : train loss 1.021922, val loss drop 0.4888 to 0.3537\n",
      "[29] : train loss 0.635642, val loss drop 0.3537 to 0.3155\n",
      "[35] : train loss 0.492389, val loss drop 0.3155 to 0.3027\n",
      "[37] : train loss 0.553948, val loss drop 0.3027 to 0.2096\n",
      "[44] : train loss 0.390855, val loss drop 0.2096 to 0.1369\n",
      "[51] : train loss 0.330010, val loss drop 0.1369 to 0.1354\n",
      "[57] : train loss 0.299095, val loss drop 0.1354 to 0.1300\n",
      "[70] : train loss 0.388774, val loss drop 0.1300 to 0.1128\n",
      "[72] : train loss 0.367557, val loss drop 0.1128 to 0.0686\n",
      "[98] : train loss 0.187209, val loss drop 0.0686 to 0.0607\n",
      "fold 3\n",
      "[1] : train loss 88.436550, val loss drop 10000000.0000 to 64.3371\n",
      "[3] : train loss 65.511738, val loss drop 64.3371 to 47.2299\n",
      "[4] : train loss 50.968642, val loss drop 47.2299 to 38.1086\n",
      "[5] : train loss 34.386230, val loss drop 38.1086 to 33.3305\n",
      "[6] : train loss 18.999579, val loss drop 33.3305 to 20.9263\n",
      "[7] : train loss 9.667686, val loss drop 20.9263 to 5.8758\n",
      "[8] : train loss 5.525802, val loss drop 5.8758 to 4.7144\n",
      "[10] : train loss 1.696706, val loss drop 4.7144 to 2.5274\n",
      "[11] : train loss 1.854103, val loss drop 2.5274 to 1.3490\n",
      "[15] : train loss 1.106515, val loss drop 1.3490 to 1.1902\n",
      "[19] : train loss 0.713684, val loss drop 1.1902 to 0.6536\n",
      "[20] : train loss 0.616139, val loss drop 0.6536 to 0.6137\n",
      "[23] : train loss 0.471384, val loss drop 0.6137 to 0.6126\n",
      "[28] : train loss 0.474076, val loss drop 0.6126 to 0.2651\n",
      "[47] : train loss 0.313527, val loss drop 0.2651 to 0.1571\n",
      "[61] : train loss 0.287609, val loss drop 0.1571 to 0.0989\n",
      "[78] : train loss 0.214060, val loss drop 0.0989 to 0.0931\n",
      "[90] : train loss 0.213541, val loss drop 0.0931 to 0.0829\n",
      "fold 4\n",
      "[1] : train loss 92.886408, val loss drop 10000000.0000 to 88.4294\n",
      "[2] : train loss 83.790869, val loss drop 88.4294 to 70.3364\n",
      "[3] : train loss 75.235053, val loss drop 70.3364 to 63.7411\n",
      "[4] : train loss 64.738657, val loss drop 63.7411 to 45.8734\n",
      "[5] : train loss 52.412593, val loss drop 45.8734 to 37.1726\n",
      "[6] : train loss 39.200812, val loss drop 37.1726 to 25.8396\n",
      "[8] : train loss 17.582994, val loss drop 25.8396 to 20.2024\n",
      "[9] : train loss 10.520877, val loss drop 20.2024 to 8.2115\n",
      "[13] : train loss 2.827147, val loss drop 8.2115 to 3.6614\n",
      "[14] : train loss 1.510310, val loss drop 3.6614 to 1.5875\n",
      "[20] : train loss 0.586081, val loss drop 1.5875 to 0.2638\n",
      "[49] : train loss 0.385456, val loss drop 0.2638 to 0.1702\n",
      "[59] : train loss 0.350891, val loss drop 0.1702 to 0.1622\n",
      "[68] : train loss 0.538520, val loss drop 0.1622 to 0.1122\n",
      "[84] : train loss 0.308302, val loss drop 0.1122 to 0.0696\n",
      "fold 5\n",
      "[1] : train loss 89.777573, val loss drop 10000000.0000 to 67.7027\n",
      "[3] : train loss 69.696492, val loss drop 67.7027 to 65.8050\n",
      "[4] : train loss 57.481460, val loss drop 65.8050 to 51.0836\n",
      "[5] : train loss 43.478751, val loss drop 51.0836 to 37.3398\n",
      "[6] : train loss 29.218481, val loss drop 37.3398 to 15.9053\n",
      "[7] : train loss 16.196113, val loss drop 15.9053 to 8.0266\n",
      "[8] : train loss 6.877937, val loss drop 8.0266 to 5.0120\n",
      "[9] : train loss 3.143491, val loss drop 5.0120 to 2.5059\n",
      "[11] : train loss 1.964351, val loss drop 2.5059 to 2.4028\n",
      "[12] : train loss 1.428579, val loss drop 2.4028 to 1.9070\n",
      "[13] : train loss 1.066937, val loss drop 1.9070 to 1.2395\n",
      "[15] : train loss 0.883795, val loss drop 1.2395 to 1.1663\n",
      "[17] : train loss 0.550963, val loss drop 1.1663 to 0.3348\n",
      "[26] : train loss 0.515652, val loss drop 0.3348 to 0.2899\n",
      "[29] : train loss 0.405063, val loss drop 0.2899 to 0.2685\n",
      "[36] : train loss 0.547052, val loss drop 0.2685 to 0.2300\n",
      "[38] : train loss 0.388062, val loss drop 0.2300 to 0.1498\n",
      "[58] : train loss 0.323591, val loss drop 0.1498 to 0.0602\n",
      "[62] : train loss 0.329517, val loss drop 0.0602 to 0.0595\n",
      "[64] : train loss 0.333219, val loss drop 0.0595 to 0.0491\n",
      "fold 6\n",
      "[1] : train loss 92.837458, val loss drop 10000000.0000 to 86.0329\n",
      "[2] : train loss 81.960415, val loss drop 86.0329 to 73.3430\n",
      "[3] : train loss 71.622259, val loss drop 73.3430 to 63.7745\n",
      "[4] : train loss 59.559770, val loss drop 63.7745 to 55.7451\n",
      "[5] : train loss 45.313721, val loss drop 55.7451 to 37.8890\n",
      "[6] : train loss 30.815870, val loss drop 37.8890 to 27.9815\n",
      "[7] : train loss 19.228984, val loss drop 27.9815 to 15.4387\n",
      "[9] : train loss 7.017084, val loss drop 15.4387 to 6.0370\n",
      "[10] : train loss 4.144390, val loss drop 6.0370 to 5.4885\n",
      "[11] : train loss 2.458318, val loss drop 5.4885 to 2.0591\n",
      "[14] : train loss 1.343525, val loss drop 2.0591 to 1.9488\n",
      "[16] : train loss 0.923100, val loss drop 1.9488 to 1.7274\n",
      "[17] : train loss 0.799923, val loss drop 1.7274 to 0.6021\n",
      "[19] : train loss 0.531436, val loss drop 0.6021 to 0.3044\n",
      "[29] : train loss 0.646508, val loss drop 0.3044 to 0.2462\n",
      "[72] : train loss 0.265331, val loss drop 0.2462 to 0.2148\n",
      "[86] : train loss 0.303174, val loss drop 0.2148 to 0.1340\n",
      "fold 7\n",
      "[1] : train loss 90.008424, val loss drop 10000000.0000 to 59.1058\n",
      "[5] : train loss 36.591738, val loss drop 59.1058 to 48.2733\n",
      "[6] : train loss 21.389679, val loss drop 48.2733 to 17.3239\n",
      "[7] : train loss 9.503909, val loss drop 17.3239 to 12.7071\n",
      "[8] : train loss 5.666005, val loss drop 12.7071 to 5.8924\n",
      "[9] : train loss 2.584686, val loss drop 5.8924 to 2.7920\n",
      "[10] : train loss 1.509529, val loss drop 2.7920 to 1.6444\n",
      "[11] : train loss 1.297163, val loss drop 1.6444 to 0.6479\n",
      "[13] : train loss 0.845005, val loss drop 0.6479 to 0.3773\n",
      "[17] : train loss 1.214102, val loss drop 0.3773 to 0.2491\n",
      "[36] : train loss 0.315347, val loss drop 0.2491 to 0.1855\n",
      "[37] : train loss 0.767381, val loss drop 0.1855 to 0.1189\n",
      "fold 8\n",
      "[1] : train loss 92.204594, val loss drop 10000000.0000 to 60.9285\n",
      "[3] : train loss 72.513715, val loss drop 60.9285 to 58.6790\n",
      "[4] : train loss 58.946320, val loss drop 58.6790 to 48.4854\n",
      "[5] : train loss 42.586731, val loss drop 48.4854 to 32.5218\n",
      "[6] : train loss 26.691595, val loss drop 32.5218 to 14.3895\n",
      "[7] : train loss 13.069364, val loss drop 14.3895 to 12.9601\n",
      "[8] : train loss 7.681215, val loss drop 12.9601 to 9.2549\n",
      "[9] : train loss 3.837656, val loss drop 9.2549 to 7.2111\n",
      "[10] : train loss 2.116588, val loss drop 7.2111 to 3.2281\n",
      "[12] : train loss 1.201590, val loss drop 3.2281 to 1.5502\n",
      "[13] : train loss 1.358311, val loss drop 1.5502 to 0.5916\n",
      "[17] : train loss 0.587424, val loss drop 0.5916 to 0.5636\n",
      "[23] : train loss 0.627583, val loss drop 0.5636 to 0.3269\n",
      "[56] : train loss 0.525931, val loss drop 0.3269 to 0.2295\n",
      "[62] : train loss 0.387009, val loss drop 0.2295 to 0.1618\n",
      "[66] : train loss 0.294887, val loss drop 0.1618 to 0.1114\n",
      "[91] : train loss 0.226434, val loss drop 0.1114 to 0.0959\n",
      "fold 9\n",
      "[1] : train loss 94.382208, val loss drop 10000000.0000 to 87.0390\n",
      "[2] : train loss 83.997457, val loss drop 87.0390 to 74.8918\n",
      "[3] : train loss 74.924050, val loss drop 74.8918 to 61.9890\n",
      "[4] : train loss 63.315180, val loss drop 61.9890 to 54.1695\n",
      "[5] : train loss 49.623047, val loss drop 54.1695 to 24.5131\n",
      "[7] : train loss 20.800188, val loss drop 24.5131 to 11.2278\n",
      "[8] : train loss 12.051769, val loss drop 11.2278 to 9.3184\n",
      "[9] : train loss 8.219408, val loss drop 9.3184 to 7.6914\n",
      "[10] : train loss 5.527042, val loss drop 7.6914 to 5.5687\n",
      "[11] : train loss 3.629099, val loss drop 5.5687 to 5.4869\n",
      "[12] : train loss 2.555838, val loss drop 5.4869 to 3.0142\n",
      "[13] : train loss 1.604894, val loss drop 3.0142 to 1.4149\n",
      "[15] : train loss 1.198176, val loss drop 1.4149 to 0.6509\n",
      "[16] : train loss 0.729123, val loss drop 0.6509 to 0.4008\n",
      "[24] : train loss 0.883866, val loss drop 0.4008 to 0.3507\n",
      "[30] : train loss 0.484002, val loss drop 0.3507 to 0.2719\n",
      "[36] : train loss 0.349994, val loss drop 0.2719 to 0.2527\n",
      "[43] : train loss 0.669562, val loss drop 0.2527 to 0.1787\n",
      "[54] : train loss 0.433817, val loss drop 0.1787 to 0.1770\n",
      "[59] : train loss 0.387945, val loss drop 0.1770 to 0.1599\n",
      "[60] : train loss 0.170165, val loss drop 0.1599 to 0.1277\n",
      "[61] : train loss 0.373095, val loss drop 0.1277 to 0.0740\n",
      "[95] : train loss 0.230060, val loss drop 0.0740 to 0.0675\n",
      "[99] : train loss 0.154549, val loss drop 0.0675 to 0.0437\n",
      "fold 10\n",
      "[1] : train loss 96.954753, val loss drop 10000000.0000 to 74.9896\n",
      "[3] : train loss 82.051844, val loss drop 74.9896 to 60.5593\n",
      "[4] : train loss 74.352592, val loss drop 60.5593 to 59.9324\n",
      "[5] : train loss 64.851609, val loss drop 59.9324 to 46.1266\n",
      "[6] : train loss 53.401826, val loss drop 46.1266 to 41.7170\n",
      "[7] : train loss 40.751433, val loss drop 41.7170 to 31.9092\n",
      "[8] : train loss 27.658861, val loss drop 31.9092 to 21.6278\n",
      "[9] : train loss 15.511044, val loss drop 21.6278 to 16.7862\n",
      "[10] : train loss 8.292313, val loss drop 16.7862 to 7.4508\n",
      "[13] : train loss 1.637890, val loss drop 7.4508 to 1.5636\n",
      "[16] : train loss 0.976103, val loss drop 1.5636 to 0.8141\n",
      "[17] : train loss 1.014266, val loss drop 0.8141 to 0.8103\n",
      "[24] : train loss 0.759144, val loss drop 0.8103 to 0.4881\n",
      "[26] : train loss 0.589910, val loss drop 0.4881 to 0.3832\n",
      "[29] : train loss 0.623845, val loss drop 0.3832 to 0.3596\n",
      "[31] : train loss 1.303305, val loss drop 0.3596 to 0.3579\n",
      "[45] : train loss 0.589397, val loss drop 0.3579 to 0.2485\n",
      "[47] : train loss 0.387213, val loss drop 0.2485 to 0.2091\n",
      "[48] : train loss 0.366856, val loss drop 0.2091 to 0.2030\n",
      "[51] : train loss 0.490364, val loss drop 0.2030 to 0.0858\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))\n",
    "print(loss_per_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, '{}.csv'.format(save_path.split('/')[-1])), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

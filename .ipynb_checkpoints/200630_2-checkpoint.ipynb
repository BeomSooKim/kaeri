{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- normalized input을 channel-wise concat\n",
    "- 학습 속도 및 성능을 봤을 때 3x1, 4x1, 5x1이 가장 적당한듯\n",
    "- normalize 성능 향상됨 -> 다른 filter size로 합쳐서 ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)\n",
    "        #return x\n",
    "    \n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "    \n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmentation(noise add)\n",
    "- channel concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis = 2):\n",
    "    mu = np.expand_dims(x.mean(axis = 2), axis = axis)\n",
    "    sd = np.expand_dims(x.std(axis = 2), axis = axis)\n",
    "\n",
    "    normalized = (x - mu) / sd\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "# concatenate normalized data\n",
    "train_norm = normalize(train_f)\n",
    "test_norm = normalize(test_f)\n",
    "\n",
    "train_f = np.concatenate((train_f, train_norm), axis = 1)\n",
    "test_f = np.concatenate((test_f, test_norm), axis = 1)\n",
    "\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "        fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "        # define model\n",
    "        model = cnn_model(conv, fc)\n",
    "        #model = get_model()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.683443, val loss drop 10000000.0000 to 1.7286\n",
      "[2] : train loss 1.078386, val loss drop 1.7286 to 0.3802\n",
      "[3] : train loss 0.181513, val loss drop 0.3802 to 0.0607\n",
      "[4] : train loss 0.037842, val loss drop 0.0607 to 0.0371\n",
      "[5] : train loss 0.018275, val loss drop 0.0371 to 0.0121\n",
      "[8] : train loss 0.004013, val loss drop 0.0121 to 0.0065\n",
      "[11] : train loss 0.002808, val loss drop 0.0065 to 0.0024\n",
      "[14] : train loss 0.002625, val loss drop 0.0024 to 0.0024\n",
      "[15] : train loss 0.003351, val loss drop 0.0024 to 0.0017\n",
      "[19] : train loss 0.002554, val loss drop 0.0017 to 0.0016\n",
      "[24] : train loss 0.001602, val loss drop 0.0016 to 0.0012\n",
      "[28] : train loss 0.001581, val loss drop 0.0012 to 0.0011\n",
      "[30] : train loss 0.001960, val loss drop 0.0011 to 0.0009\n",
      "[31] : train loss 0.001271, val loss drop 0.0009 to 0.0008\n",
      "[41] : train loss 0.001441, val loss drop 0.0008 to 0.0007\n",
      "[45] : train loss 0.000725, val loss drop 0.0007 to 0.0006\n",
      "[57] : train loss 0.001663, val loss drop 0.0006 to 0.0005\n",
      "[92] : train loss 0.000963, val loss drop 0.0005 to 0.0004\n",
      "fold 2\n",
      "[1] : train loss 2.785003, val loss drop 10000000.0000 to 1.5642\n",
      "[2] : train loss 1.207566, val loss drop 1.5642 to 0.3484\n",
      "[3] : train loss 0.142958, val loss drop 0.3484 to 0.1900\n",
      "[4] : train loss 0.032434, val loss drop 0.1900 to 0.0291\n",
      "[5] : train loss 0.013390, val loss drop 0.0291 to 0.0283\n",
      "[7] : train loss 0.004475, val loss drop 0.0283 to 0.0045\n",
      "[8] : train loss 0.003340, val loss drop 0.0045 to 0.0029\n",
      "[10] : train loss 0.002512, val loss drop 0.0029 to 0.0023\n",
      "[13] : train loss 0.002148, val loss drop 0.0023 to 0.0015\n",
      "[16] : train loss 0.000972, val loss drop 0.0015 to 0.0014\n",
      "[19] : train loss 0.001699, val loss drop 0.0014 to 0.0013\n",
      "[23] : train loss 0.001285, val loss drop 0.0013 to 0.0009\n",
      "[24] : train loss 0.001015, val loss drop 0.0009 to 0.0007\n",
      "[28] : train loss 0.001023, val loss drop 0.0007 to 0.0006\n",
      "[48] : train loss 0.000782, val loss drop 0.0006 to 0.0004\n",
      "[62] : train loss 0.000521, val loss drop 0.0004 to 0.0004\n",
      "[72] : train loss 0.001039, val loss drop 0.0004 to 0.0003\n",
      "[92] : train loss 0.000759, val loss drop 0.0003 to 0.0003\n",
      "fold 3\n",
      "[1] : train loss 2.862629, val loss drop 10000000.0000 to 1.7194\n",
      "[2] : train loss 1.356680, val loss drop 1.7194 to 0.5118\n",
      "[3] : train loss 0.348972, val loss drop 0.5118 to 0.1613\n",
      "[5] : train loss 0.023349, val loss drop 0.1613 to 0.0374\n",
      "[6] : train loss 0.011903, val loss drop 0.0374 to 0.0092\n",
      "[8] : train loss 0.004789, val loss drop 0.0092 to 0.0073\n",
      "[9] : train loss 0.003871, val loss drop 0.0073 to 0.0032\n",
      "[11] : train loss 0.002429, val loss drop 0.0032 to 0.0027\n",
      "[16] : train loss 0.001906, val loss drop 0.0027 to 0.0017\n",
      "[17] : train loss 0.001268, val loss drop 0.0017 to 0.0015\n",
      "[18] : train loss 0.001308, val loss drop 0.0015 to 0.0009\n",
      "[26] : train loss 0.000870, val loss drop 0.0009 to 0.0009\n",
      "[36] : train loss 0.000809, val loss drop 0.0009 to 0.0006\n",
      "[49] : train loss 0.000919, val loss drop 0.0006 to 0.0005\n",
      "[64] : train loss 0.001559, val loss drop 0.0005 to 0.0005\n",
      "fold 4\n",
      "[1] : train loss 2.720268, val loss drop 10000000.0000 to 1.4053\n",
      "[2] : train loss 0.956151, val loss drop 1.4053 to 0.3872\n",
      "[3] : train loss 0.152667, val loss drop 0.3872 to 0.1602\n",
      "[4] : train loss 0.031725, val loss drop 0.1602 to 0.1153\n",
      "[5] : train loss 0.013711, val loss drop 0.1153 to 0.0283\n",
      "[6] : train loss 0.006734, val loss drop 0.0283 to 0.0057\n",
      "[10] : train loss 0.003552, val loss drop 0.0057 to 0.0030\n",
      "[13] : train loss 0.001261, val loss drop 0.0030 to 0.0020\n",
      "[14] : train loss 0.001308, val loss drop 0.0020 to 0.0018\n",
      "[15] : train loss 0.000985, val loss drop 0.0018 to 0.0009\n",
      "[17] : train loss 0.000829, val loss drop 0.0009 to 0.0009\n",
      "[25] : train loss 0.000764, val loss drop 0.0009 to 0.0008\n",
      "[26] : train loss 0.001719, val loss drop 0.0008 to 0.0008\n",
      "[32] : train loss 0.000966, val loss drop 0.0008 to 0.0007\n",
      "[38] : train loss 0.001232, val loss drop 0.0007 to 0.0005\n",
      "[45] : train loss 0.000890, val loss drop 0.0005 to 0.0004\n",
      "[70] : train loss 0.000658, val loss drop 0.0004 to 0.0003\n",
      "[88] : train loss 0.000352, val loss drop 0.0003 to 0.0003\n",
      "fold 5\n",
      "[1] : train loss 2.715799, val loss drop 10000000.0000 to 1.1805\n",
      "[2] : train loss 1.010304, val loss drop 1.1805 to 0.3082\n",
      "[3] : train loss 0.125238, val loss drop 0.3082 to 0.0954\n",
      "[5] : train loss 0.012606, val loss drop 0.0954 to 0.0574\n",
      "[6] : train loss 0.005924, val loss drop 0.0574 to 0.0101\n",
      "[7] : train loss 0.003633, val loss drop 0.0101 to 0.0029\n",
      "[11] : train loss 0.002064, val loss drop 0.0029 to 0.0019\n",
      "[17] : train loss 0.001705, val loss drop 0.0019 to 0.0011\n",
      "[20] : train loss 0.001670, val loss drop 0.0011 to 0.0007\n",
      "[28] : train loss 0.001252, val loss drop 0.0007 to 0.0007\n",
      "[35] : train loss 0.000672, val loss drop 0.0007 to 0.0004\n",
      "[44] : train loss 0.000535, val loss drop 0.0004 to 0.0004\n",
      "[79] : train loss 0.000443, val loss drop 0.0004 to 0.0003\n",
      "[83] : train loss 0.000545, val loss drop 0.0003 to 0.0002\n",
      "[89] : train loss 0.000638, val loss drop 0.0002 to 0.0002\n",
      "fold 6\n",
      "[1] : train loss 2.816491, val loss drop 10000000.0000 to 1.8399\n",
      "[2] : train loss 1.246970, val loss drop 1.8399 to 0.4453\n",
      "[3] : train loss 0.201961, val loss drop 0.4453 to 0.1027\n",
      "[4] : train loss 0.037782, val loss drop 0.1027 to 0.0744\n",
      "[6] : train loss 0.007268, val loss drop 0.0744 to 0.0186\n",
      "[7] : train loss 0.004713, val loss drop 0.0186 to 0.0045\n",
      "[8] : train loss 0.003181, val loss drop 0.0045 to 0.0045\n",
      "[11] : train loss 0.001725, val loss drop 0.0045 to 0.0033\n",
      "[13] : train loss 0.001871, val loss drop 0.0033 to 0.0018\n",
      "[16] : train loss 0.002021, val loss drop 0.0018 to 0.0018\n",
      "[19] : train loss 0.002045, val loss drop 0.0018 to 0.0016\n",
      "[29] : train loss 0.001304, val loss drop 0.0016 to 0.0015\n",
      "[41] : train loss 0.001883, val loss drop 0.0015 to 0.0010\n",
      "[48] : train loss 0.001387, val loss drop 0.0010 to 0.0009\n",
      "[50] : train loss 0.000710, val loss drop 0.0009 to 0.0008\n",
      "[54] : train loss 0.001397, val loss drop 0.0008 to 0.0008\n",
      "[61] : train loss 0.000965, val loss drop 0.0008 to 0.0006\n",
      "[72] : train loss 0.000566, val loss drop 0.0006 to 0.0005\n",
      "[73] : train loss 0.000571, val loss drop 0.0005 to 0.0004\n",
      "[87] : train loss 0.000418, val loss drop 0.0004 to 0.0003\n",
      "[88] : train loss 0.000323, val loss drop 0.0003 to 0.0003\n",
      "fold 7\n",
      "[1] : train loss 2.807061, val loss drop 10000000.0000 to 1.5671\n",
      "[2] : train loss 1.203586, val loss drop 1.5671 to 0.3787\n",
      "[3] : train loss 0.176105, val loss drop 0.3787 to 0.0539\n",
      "[5] : train loss 0.013503, val loss drop 0.0539 to 0.0269\n",
      "[6] : train loss 0.006011, val loss drop 0.0269 to 0.0072\n",
      "[7] : train loss 0.003123, val loss drop 0.0072 to 0.0054\n",
      "[8] : train loss 0.002467, val loss drop 0.0054 to 0.0046\n",
      "[9] : train loss 0.002713, val loss drop 0.0046 to 0.0035\n",
      "[10] : train loss 0.001532, val loss drop 0.0035 to 0.0031\n",
      "[11] : train loss 0.001488, val loss drop 0.0031 to 0.0028\n",
      "[12] : train loss 0.001961, val loss drop 0.0028 to 0.0016\n",
      "[18] : train loss 0.001505, val loss drop 0.0016 to 0.0016\n",
      "[20] : train loss 0.002017, val loss drop 0.0016 to 0.0016\n",
      "[24] : train loss 0.001789, val loss drop 0.0016 to 0.0015\n",
      "[25] : train loss 0.001017, val loss drop 0.0015 to 0.0006\n",
      "[44] : train loss 0.001557, val loss drop 0.0006 to 0.0005\n",
      "[48] : train loss 0.000632, val loss drop 0.0005 to 0.0005\n",
      "[52] : train loss 0.000594, val loss drop 0.0005 to 0.0004\n",
      "[87] : train loss 0.000597, val loss drop 0.0004 to 0.0003\n",
      "fold 8\n",
      "[1] : train loss 2.859345, val loss drop 10000000.0000 to 1.8386\n",
      "[2] : train loss 1.415182, val loss drop 1.8386 to 0.4782\n",
      "[3] : train loss 0.273244, val loss drop 0.4782 to 0.1411\n",
      "[5] : train loss 0.023171, val loss drop 0.1411 to 0.0900\n",
      "[6] : train loss 0.013402, val loss drop 0.0900 to 0.0255\n",
      "[7] : train loss 0.007064, val loss drop 0.0255 to 0.0060\n",
      "[8] : train loss 0.004672, val loss drop 0.0060 to 0.0038\n",
      "[9] : train loss 0.003233, val loss drop 0.0038 to 0.0027\n",
      "[13] : train loss 0.002392, val loss drop 0.0027 to 0.0016\n",
      "[19] : train loss 0.001443, val loss drop 0.0016 to 0.0011\n",
      "[20] : train loss 0.001359, val loss drop 0.0011 to 0.0010\n",
      "[33] : train loss 0.000912, val loss drop 0.0010 to 0.0007\n",
      "[44] : train loss 0.001289, val loss drop 0.0007 to 0.0006\n",
      "[53] : train loss 0.000840, val loss drop 0.0006 to 0.0005\n",
      "[75] : train loss 0.000629, val loss drop 0.0005 to 0.0003\n",
      "[82] : train loss 0.000857, val loss drop 0.0003 to 0.0003\n",
      "fold 9\n",
      "[1] : train loss 2.676806, val loss drop 10000000.0000 to 1.6554\n",
      "[2] : train loss 1.320334, val loss drop 1.6554 to 0.7547\n",
      "[3] : train loss 0.431546, val loss drop 0.7547 to 0.2367\n",
      "[4] : train loss 0.081315, val loss drop 0.2367 to 0.0478\n",
      "[6] : train loss 0.010174, val loss drop 0.0478 to 0.0335\n",
      "[7] : train loss 0.005594, val loss drop 0.0335 to 0.0166\n",
      "[8] : train loss 0.003199, val loss drop 0.0166 to 0.0093\n",
      "[9] : train loss 0.002535, val loss drop 0.0093 to 0.0020\n",
      "[16] : train loss 0.001557, val loss drop 0.0020 to 0.0014\n",
      "[30] : train loss 0.001293, val loss drop 0.0014 to 0.0009\n",
      "[32] : train loss 0.001035, val loss drop 0.0009 to 0.0009\n",
      "[33] : train loss 0.001231, val loss drop 0.0009 to 0.0008\n",
      "[51] : train loss 0.001185, val loss drop 0.0008 to 0.0006\n",
      "[59] : train loss 0.001244, val loss drop 0.0006 to 0.0004\n",
      "[60] : train loss 0.000866, val loss drop 0.0004 to 0.0004\n",
      "fold 10\n",
      "[1] : train loss 2.681166, val loss drop 10000000.0000 to 1.9113\n",
      "[2] : train loss 1.187862, val loss drop 1.9113 to 0.5085\n",
      "[3] : train loss 0.302557, val loss drop 0.5085 to 0.1330\n",
      "[4] : train loss 0.050854, val loss drop 0.1330 to 0.1072\n",
      "[5] : train loss 0.018598, val loss drop 0.1072 to 0.0477\n",
      "[6] : train loss 0.009099, val loss drop 0.0477 to 0.0078\n",
      "[8] : train loss 0.003605, val loss drop 0.0078 to 0.0052\n",
      "[9] : train loss 0.003237, val loss drop 0.0052 to 0.0034\n",
      "[11] : train loss 0.002387, val loss drop 0.0034 to 0.0023\n",
      "[15] : train loss 0.001851, val loss drop 0.0023 to 0.0020\n",
      "[17] : train loss 0.002600, val loss drop 0.0020 to 0.0019\n",
      "[22] : train loss 0.000800, val loss drop 0.0019 to 0.0016\n",
      "[23] : train loss 0.001198, val loss drop 0.0016 to 0.0014\n",
      "[24] : train loss 0.001869, val loss drop 0.0014 to 0.0007\n",
      "[47] : train loss 0.001029, val loss drop 0.0007 to 0.0006\n",
      "[52] : train loss 0.001012, val loss drop 0.0006 to 0.0005\n",
      "[56] : train loss 0.000598, val loss drop 0.0005 to 0.0004\n",
      "[57] : train loss 0.000390, val loss drop 0.0004 to 0.0003\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 17.877696, val loss drop 10000000.0000 to 0.1766\n",
      "[3] : train loss 0.081594, val loss drop 0.1766 to 0.0615\n",
      "[4] : train loss 0.043814, val loss drop 0.0615 to 0.0553\n",
      "[5] : train loss 0.029965, val loss drop 0.0553 to 0.0257\n",
      "[6] : train loss 0.018489, val loss drop 0.0257 to 0.0163\n",
      "[7] : train loss 0.010788, val loss drop 0.0163 to 0.0150\n",
      "[8] : train loss 0.009075, val loss drop 0.0150 to 0.0119\n",
      "[9] : train loss 0.008379, val loss drop 0.0119 to 0.0098\n",
      "[12] : train loss 0.010497, val loss drop 0.0098 to 0.0077\n",
      "[13] : train loss 0.005326, val loss drop 0.0077 to 0.0076\n",
      "[14] : train loss 0.004838, val loss drop 0.0076 to 0.0066\n",
      "[18] : train loss 0.004714, val loss drop 0.0066 to 0.0061\n",
      "[19] : train loss 0.003994, val loss drop 0.0061 to 0.0051\n",
      "[25] : train loss 0.004479, val loss drop 0.0051 to 0.0045\n",
      "[27] : train loss 0.005124, val loss drop 0.0045 to 0.0045\n",
      "[33] : train loss 0.009831, val loss drop 0.0045 to 0.0029\n",
      "[41] : train loss 0.008390, val loss drop 0.0029 to 0.0027\n",
      "[49] : train loss 0.002592, val loss drop 0.0027 to 0.0024\n",
      "[65] : train loss 0.001881, val loss drop 0.0024 to 0.0021\n",
      "[79] : train loss 0.003888, val loss drop 0.0021 to 0.0019\n",
      "[81] : train loss 0.005988, val loss drop 0.0019 to 0.0016\n",
      "[82] : train loss 0.006222, val loss drop 0.0016 to 0.0014\n",
      "fold 2\n",
      "[1] : train loss 7.326682, val loss drop 10000000.0000 to 0.7404\n",
      "[2] : train loss 0.152661, val loss drop 0.7404 to 0.0997\n",
      "[3] : train loss 0.045022, val loss drop 0.0997 to 0.0309\n",
      "[4] : train loss 0.027758, val loss drop 0.0309 to 0.0178\n",
      "[5] : train loss 0.014113, val loss drop 0.0178 to 0.0165\n",
      "[6] : train loss 0.011009, val loss drop 0.0165 to 0.0112\n",
      "[9] : train loss 0.008871, val loss drop 0.0112 to 0.0087\n",
      "[10] : train loss 0.006459, val loss drop 0.0087 to 0.0069\n",
      "[11] : train loss 0.005118, val loss drop 0.0069 to 0.0042\n",
      "[24] : train loss 0.005142, val loss drop 0.0042 to 0.0036\n",
      "[25] : train loss 0.004287, val loss drop 0.0036 to 0.0035\n",
      "[26] : train loss 0.002071, val loss drop 0.0035 to 0.0033\n",
      "[28] : train loss 0.002625, val loss drop 0.0033 to 0.0029\n",
      "[35] : train loss 0.004024, val loss drop 0.0029 to 0.0016\n",
      "[46] : train loss 0.001921, val loss drop 0.0016 to 0.0015\n",
      "[51] : train loss 0.002123, val loss drop 0.0015 to 0.0013\n",
      "[65] : train loss 0.002441, val loss drop 0.0013 to 0.0013\n",
      "[66] : train loss 0.001463, val loss drop 0.0013 to 0.0012\n",
      "[67] : train loss 0.001252, val loss drop 0.0012 to 0.0011\n",
      "[81] : train loss 0.002555, val loss drop 0.0011 to 0.0010\n",
      "[96] : train loss 0.004421, val loss drop 0.0010 to 0.0010\n",
      "fold 3\n",
      "[1] : train loss 3.158647, val loss drop 10000000.0000 to 1.5340\n",
      "[2] : train loss 0.328052, val loss drop 1.5340 to 0.3095\n",
      "[3] : train loss 0.096633, val loss drop 0.3095 to 0.0710\n",
      "[4] : train loss 0.040753, val loss drop 0.0710 to 0.0269\n",
      "[5] : train loss 0.021759, val loss drop 0.0269 to 0.0158\n",
      "[6] : train loss 0.011620, val loss drop 0.0158 to 0.0082\n",
      "[7] : train loss 0.008190, val loss drop 0.0082 to 0.0077\n",
      "[8] : train loss 0.008274, val loss drop 0.0077 to 0.0066\n",
      "[11] : train loss 0.004209, val loss drop 0.0066 to 0.0045\n",
      "[13] : train loss 0.003658, val loss drop 0.0045 to 0.0039\n",
      "[16] : train loss 0.002765, val loss drop 0.0039 to 0.0032\n",
      "[27] : train loss 0.001753, val loss drop 0.0032 to 0.0027\n",
      "[28] : train loss 0.002163, val loss drop 0.0027 to 0.0019\n",
      "[44] : train loss 0.005973, val loss drop 0.0019 to 0.0018\n",
      "[47] : train loss 0.001601, val loss drop 0.0018 to 0.0015\n",
      "[52] : train loss 0.003310, val loss drop 0.0015 to 0.0012\n",
      "[71] : train loss 0.001843, val loss drop 0.0012 to 0.0012\n",
      "[76] : train loss 0.001093, val loss drop 0.0012 to 0.0010\n",
      "[89] : train loss 0.001242, val loss drop 0.0010 to 0.0009\n",
      "[95] : train loss 0.001946, val loss drop 0.0009 to 0.0009\n",
      "fold 4\n",
      "[1] : train loss 6.383701, val loss drop 10000000.0000 to 0.1718\n",
      "[2] : train loss 0.082370, val loss drop 0.1718 to 0.0339\n",
      "[3] : train loss 0.021817, val loss drop 0.0339 to 0.0182\n",
      "[4] : train loss 0.013445, val loss drop 0.0182 to 0.0080\n",
      "[6] : train loss 0.007491, val loss drop 0.0080 to 0.0069\n",
      "[7] : train loss 0.005295, val loss drop 0.0069 to 0.0041\n",
      "[9] : train loss 0.005354, val loss drop 0.0041 to 0.0033\n",
      "[12] : train loss 0.003087, val loss drop 0.0033 to 0.0030\n",
      "[13] : train loss 0.002620, val loss drop 0.0030 to 0.0027\n",
      "[14] : train loss 0.002642, val loss drop 0.0027 to 0.0021\n",
      "[17] : train loss 0.003995, val loss drop 0.0021 to 0.0019\n",
      "[19] : train loss 0.003202, val loss drop 0.0019 to 0.0016\n",
      "[25] : train loss 0.003768, val loss drop 0.0016 to 0.0016\n",
      "[26] : train loss 0.002858, val loss drop 0.0016 to 0.0012\n",
      "[34] : train loss 0.001938, val loss drop 0.0012 to 0.0010\n",
      "[47] : train loss 0.006248, val loss drop 0.0010 to 0.0010\n",
      "[67] : train loss 0.002411, val loss drop 0.0010 to 0.0008\n",
      "[69] : train loss 0.001031, val loss drop 0.0008 to 0.0005\n",
      "[76] : train loss 0.002961, val loss drop 0.0005 to 0.0005\n",
      "[94] : train loss 0.001039, val loss drop 0.0005 to 0.0005\n",
      "[97] : train loss 0.001368, val loss drop 0.0005 to 0.0005\n",
      "[98] : train loss 0.000737, val loss drop 0.0005 to 0.0004\n",
      "[99] : train loss 0.001272, val loss drop 0.0004 to 0.0004\n",
      "fold 5\n",
      "[1] : train loss 11.818444, val loss drop 10000000.0000 to 0.6001\n",
      "[2] : train loss 0.222490, val loss drop 0.6001 to 0.2633\n",
      "[3] : train loss 0.090392, val loss drop 0.2633 to 0.0793\n",
      "[4] : train loss 0.043261, val loss drop 0.0793 to 0.0387\n",
      "[5] : train loss 0.024396, val loss drop 0.0387 to 0.0175\n",
      "[6] : train loss 0.013990, val loss drop 0.0175 to 0.0174\n",
      "[7] : train loss 0.013846, val loss drop 0.0174 to 0.0141\n",
      "[9] : train loss 0.011915, val loss drop 0.0141 to 0.0119\n",
      "[11] : train loss 0.007381, val loss drop 0.0119 to 0.0098\n",
      "[12] : train loss 0.006898, val loss drop 0.0098 to 0.0059\n",
      "[15] : train loss 0.005014, val loss drop 0.0059 to 0.0053\n",
      "[16] : train loss 0.004364, val loss drop 0.0053 to 0.0047\n",
      "[21] : train loss 0.004538, val loss drop 0.0047 to 0.0040\n",
      "[27] : train loss 0.003068, val loss drop 0.0040 to 0.0034\n",
      "[33] : train loss 0.003211, val loss drop 0.0034 to 0.0029\n",
      "[46] : train loss 0.002193, val loss drop 0.0029 to 0.0025\n",
      "[47] : train loss 0.002568, val loss drop 0.0025 to 0.0018\n",
      "[56] : train loss 0.004428, val loss drop 0.0018 to 0.0018\n",
      "[92] : train loss 0.001210, val loss drop 0.0018 to 0.0010\n",
      "fold 6\n",
      "[1] : train loss 2.321624, val loss drop 10000000.0000 to 0.5430\n",
      "[2] : train loss 0.144917, val loss drop 0.5430 to 0.0342\n",
      "[3] : train loss 0.044263, val loss drop 0.0342 to 0.0264\n",
      "[4] : train loss 0.019631, val loss drop 0.0264 to 0.0129\n",
      "[5] : train loss 0.010625, val loss drop 0.0129 to 0.0106\n",
      "[6] : train loss 0.010680, val loss drop 0.0106 to 0.0070\n",
      "[7] : train loss 0.006717, val loss drop 0.0070 to 0.0048\n",
      "[8] : train loss 0.006150, val loss drop 0.0048 to 0.0035\n",
      "[11] : train loss 0.003812, val loss drop 0.0035 to 0.0032\n",
      "[12] : train loss 0.004353, val loss drop 0.0032 to 0.0029\n",
      "[13] : train loss 0.002857, val loss drop 0.0029 to 0.0029\n",
      "[14] : train loss 0.002887, val loss drop 0.0029 to 0.0026\n",
      "[16] : train loss 0.002624, val loss drop 0.0026 to 0.0021\n",
      "[19] : train loss 0.002454, val loss drop 0.0021 to 0.0020\n",
      "[20] : train loss 0.002032, val loss drop 0.0020 to 0.0017\n",
      "[25] : train loss 0.001482, val loss drop 0.0017 to 0.0016\n",
      "[32] : train loss 0.002145, val loss drop 0.0016 to 0.0014\n",
      "[38] : train loss 0.001909, val loss drop 0.0014 to 0.0013\n",
      "[42] : train loss 0.004138, val loss drop 0.0013 to 0.0011\n",
      "[53] : train loss 0.001173, val loss drop 0.0011 to 0.0009\n",
      "[70] : train loss 0.000628, val loss drop 0.0009 to 0.0007\n",
      "[71] : train loss 0.001878, val loss drop 0.0007 to 0.0007\n",
      "[72] : train loss 0.000640, val loss drop 0.0007 to 0.0007\n",
      "[78] : train loss 0.000897, val loss drop 0.0007 to 0.0007\n",
      "[82] : train loss 0.000519, val loss drop 0.0007 to 0.0006\n",
      "[87] : train loss 0.001557, val loss drop 0.0006 to 0.0005\n",
      "fold 7\n",
      "[1] : train loss 4.746773, val loss drop 10000000.0000 to 1.2149\n",
      "[2] : train loss 0.135489, val loss drop 1.2149 to 0.0418\n",
      "[4] : train loss 0.020649, val loss drop 0.0418 to 0.0175\n",
      "[5] : train loss 0.011118, val loss drop 0.0175 to 0.0071\n",
      "[6] : train loss 0.008181, val loss drop 0.0071 to 0.0068\n",
      "[8] : train loss 0.006881, val loss drop 0.0068 to 0.0057\n",
      "[9] : train loss 0.005524, val loss drop 0.0057 to 0.0047\n",
      "[10] : train loss 0.005895, val loss drop 0.0047 to 0.0035\n",
      "[13] : train loss 0.004623, val loss drop 0.0035 to 0.0030\n",
      "[18] : train loss 0.004678, val loss drop 0.0030 to 0.0029\n",
      "[19] : train loss 0.002485, val loss drop 0.0029 to 0.0026\n",
      "[21] : train loss 0.001937, val loss drop 0.0026 to 0.0024\n",
      "[24] : train loss 0.002173, val loss drop 0.0024 to 0.0022\n",
      "[25] : train loss 0.002941, val loss drop 0.0022 to 0.0020\n",
      "[27] : train loss 0.003221, val loss drop 0.0020 to 0.0017\n",
      "[47] : train loss 0.005301, val loss drop 0.0017 to 0.0015\n",
      "[48] : train loss 0.002478, val loss drop 0.0015 to 0.0011\n",
      "[52] : train loss 0.002624, val loss drop 0.0011 to 0.0011\n",
      "[62] : train loss 0.004405, val loss drop 0.0011 to 0.0010\n",
      "[79] : train loss 0.001211, val loss drop 0.0010 to 0.0006\n",
      "fold 8\n",
      "[1] : train loss 1.921218, val loss drop 10000000.0000 to 0.6170\n",
      "[2] : train loss 0.097364, val loss drop 0.6170 to 0.0467\n",
      "[3] : train loss 0.031721, val loss drop 0.0467 to 0.0173\n",
      "[4] : train loss 0.012715, val loss drop 0.0173 to 0.0088\n",
      "[5] : train loss 0.007290, val loss drop 0.0088 to 0.0065\n",
      "[6] : train loss 0.004972, val loss drop 0.0065 to 0.0050\n",
      "[7] : train loss 0.004195, val loss drop 0.0050 to 0.0038\n",
      "[9] : train loss 0.003300, val loss drop 0.0038 to 0.0037\n",
      "[10] : train loss 0.002916, val loss drop 0.0037 to 0.0036\n",
      "[12] : train loss 0.006415, val loss drop 0.0036 to 0.0028\n",
      "[21] : train loss 0.004827, val loss drop 0.0028 to 0.0021\n",
      "[22] : train loss 0.001580, val loss drop 0.0021 to 0.0016\n",
      "[26] : train loss 0.005757, val loss drop 0.0016 to 0.0013\n",
      "[36] : train loss 0.000917, val loss drop 0.0013 to 0.0010\n",
      "[40] : train loss 0.001039, val loss drop 0.0010 to 0.0010\n",
      "[54] : train loss 0.000743, val loss drop 0.0010 to 0.0009\n",
      "[60] : train loss 0.005616, val loss drop 0.0009 to 0.0008\n",
      "[63] : train loss 0.003974, val loss drop 0.0008 to 0.0008\n",
      "[64] : train loss 0.002515, val loss drop 0.0008 to 0.0008\n",
      "[78] : train loss 0.001238, val loss drop 0.0008 to 0.0005\n",
      "[96] : train loss 0.000826, val loss drop 0.0005 to 0.0004\n",
      "fold 9\n",
      "[1] : train loss 7.001677, val loss drop 10000000.0000 to 0.9421\n",
      "[2] : train loss 0.153446, val loss drop 0.9421 to 0.0472\n",
      "[3] : train loss 0.049131, val loss drop 0.0472 to 0.0268\n",
      "[4] : train loss 0.019236, val loss drop 0.0268 to 0.0192\n",
      "[5] : train loss 0.011714, val loss drop 0.0192 to 0.0104\n",
      "[6] : train loss 0.009754, val loss drop 0.0104 to 0.0081\n",
      "[11] : train loss 0.005139, val loss drop 0.0081 to 0.0060\n",
      "[12] : train loss 0.004523, val loss drop 0.0060 to 0.0038\n",
      "[15] : train loss 0.005440, val loss drop 0.0038 to 0.0036\n",
      "[18] : train loss 0.003065, val loss drop 0.0036 to 0.0033\n",
      "[20] : train loss 0.002906, val loss drop 0.0033 to 0.0024\n",
      "[29] : train loss 0.002567, val loss drop 0.0024 to 0.0023\n",
      "[32] : train loss 0.002564, val loss drop 0.0023 to 0.0019\n",
      "[40] : train loss 0.002679, val loss drop 0.0019 to 0.0017\n",
      "[43] : train loss 0.001920, val loss drop 0.0017 to 0.0016\n",
      "[46] : train loss 0.003112, val loss drop 0.0016 to 0.0012\n",
      "[65] : train loss 0.002082, val loss drop 0.0012 to 0.0011\n",
      "[82] : train loss 0.001244, val loss drop 0.0011 to 0.0011\n",
      "[83] : train loss 0.000852, val loss drop 0.0011 to 0.0010\n",
      "[85] : train loss 0.001460, val loss drop 0.0010 to 0.0007\n",
      "fold 10\n",
      "[1] : train loss 4.752056, val loss drop 10000000.0000 to 0.7027\n",
      "[2] : train loss 0.081198, val loss drop 0.7027 to 0.0321\n",
      "[3] : train loss 0.031317, val loss drop 0.0321 to 0.0271\n",
      "[4] : train loss 0.014021, val loss drop 0.0271 to 0.0185\n",
      "[5] : train loss 0.012109, val loss drop 0.0185 to 0.0065\n",
      "[6] : train loss 0.008021, val loss drop 0.0065 to 0.0058\n",
      "[7] : train loss 0.006205, val loss drop 0.0058 to 0.0052\n",
      "[8] : train loss 0.004804, val loss drop 0.0052 to 0.0047\n",
      "[10] : train loss 0.005163, val loss drop 0.0047 to 0.0038\n",
      "[12] : train loss 0.003623, val loss drop 0.0038 to 0.0031\n",
      "[13] : train loss 0.003417, val loss drop 0.0031 to 0.0028\n",
      "[18] : train loss 0.002949, val loss drop 0.0028 to 0.0020\n",
      "[22] : train loss 0.002294, val loss drop 0.0020 to 0.0014\n",
      "[38] : train loss 0.003689, val loss drop 0.0014 to 0.0010\n",
      "[46] : train loss 0.001403, val loss drop 0.0010 to 0.0009\n",
      "[59] : train loss 0.002923, val loss drop 0.0009 to 0.0008\n",
      "[64] : train loss 0.001292, val loss drop 0.0008 to 0.0007\n",
      "[71] : train loss 0.000976, val loss drop 0.0007 to 0.0006\n",
      "[92] : train loss 0.001804, val loss drop 0.0006 to 0.0006\n",
      "[99] : train loss 0.000757, val loss drop 0.0006 to 0.0004\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 43.552420, val loss drop 10000000.0000 to 5.3200\n",
      "[4] : train loss 0.550472, val loss drop 5.3200 to 3.0951\n",
      "[5] : train loss 0.494935, val loss drop 3.0951 to 3.0272\n",
      "[6] : train loss 0.702108, val loss drop 3.0272 to 1.4643\n",
      "[7] : train loss 0.732003, val loss drop 1.4643 to 0.3825\n",
      "[8] : train loss 0.496727, val loss drop 0.3825 to 0.3573\n",
      "[10] : train loss 0.579082, val loss drop 0.3573 to 0.2410\n",
      "[11] : train loss 0.272985, val loss drop 0.2410 to 0.2176\n",
      "[12] : train loss 0.659581, val loss drop 0.2176 to 0.1949\n",
      "[16] : train loss 0.150516, val loss drop 0.1949 to 0.1523\n",
      "[18] : train loss 0.445162, val loss drop 0.1523 to 0.1463\n",
      "[28] : train loss 0.284466, val loss drop 0.1463 to 0.0911\n",
      "[29] : train loss 0.209263, val loss drop 0.0911 to 0.0487\n",
      "[33] : train loss 0.096607, val loss drop 0.0487 to 0.0287\n",
      "[64] : train loss 0.154720, val loss drop 0.0287 to 0.0287\n",
      "[73] : train loss 0.148633, val loss drop 0.0287 to 0.0171\n",
      "[93] : train loss 0.074384, val loss drop 0.0171 to 0.0169\n",
      "fold 2\n",
      "[1] : train loss 50.866728, val loss drop 10000000.0000 to 10.5054\n",
      "[4] : train loss 0.556660, val loss drop 10.5054 to 3.4503\n",
      "[5] : train loss 0.418157, val loss drop 3.4503 to 0.2750\n",
      "[12] : train loss 0.197433, val loss drop 0.2750 to 0.1228\n",
      "[23] : train loss 0.166155, val loss drop 0.1228 to 0.0602\n",
      "[34] : train loss 0.104146, val loss drop 0.0602 to 0.0385\n",
      "[46] : train loss 0.302443, val loss drop 0.0385 to 0.0357\n",
      "[50] : train loss 0.094461, val loss drop 0.0357 to 0.0229\n",
      "[92] : train loss 0.084533, val loss drop 0.0229 to 0.0127\n",
      "[94] : train loss 0.036516, val loss drop 0.0127 to 0.0087\n",
      "fold 3\n",
      "[1] : train loss 26.185824, val loss drop 10000000.0000 to 180.7995\n",
      "[2] : train loss 2.148076, val loss drop 180.7995 to 12.2069\n",
      "[3] : train loss 1.075731, val loss drop 12.2069 to 4.3325\n",
      "[4] : train loss 0.637645, val loss drop 4.3325 to 0.9493\n",
      "[5] : train loss 0.479485, val loss drop 0.9493 to 0.5572\n",
      "[6] : train loss 0.314131, val loss drop 0.5572 to 0.4516\n",
      "[7] : train loss 0.475905, val loss drop 0.4516 to 0.2854\n",
      "[8] : train loss 0.206210, val loss drop 0.2854 to 0.1648\n",
      "[9] : train loss 0.288400, val loss drop 0.1648 to 0.1126\n",
      "[10] : train loss 0.143553, val loss drop 0.1126 to 0.0659\n",
      "[21] : train loss 0.186917, val loss drop 0.0659 to 0.0412\n",
      "[37] : train loss 0.133197, val loss drop 0.0412 to 0.0216\n",
      "[43] : train loss 0.105302, val loss drop 0.0216 to 0.0200\n",
      "[44] : train loss 0.159725, val loss drop 0.0200 to 0.0173\n",
      "[56] : train loss 0.036240, val loss drop 0.0173 to 0.0135\n",
      "[57] : train loss 0.116601, val loss drop 0.0135 to 0.0092\n",
      "[79] : train loss 0.099522, val loss drop 0.0092 to 0.0078\n",
      "fold 4\n",
      "[1] : train loss 39.257453, val loss drop 10000000.0000 to 22.5743\n",
      "[3] : train loss 1.043645, val loss drop 22.5743 to 19.3755\n",
      "[4] : train loss 0.555569, val loss drop 19.3755 to 2.8649\n",
      "[5] : train loss 0.398194, val loss drop 2.8649 to 0.2986\n",
      "[6] : train loss 0.695002, val loss drop 0.2986 to 0.2115\n",
      "[8] : train loss 0.191007, val loss drop 0.2115 to 0.1597\n",
      "[12] : train loss 0.129380, val loss drop 0.1597 to 0.0857\n",
      "[28] : train loss 0.191046, val loss drop 0.0857 to 0.0571\n",
      "[30] : train loss 0.219210, val loss drop 0.0571 to 0.0394\n",
      "[37] : train loss 0.154900, val loss drop 0.0394 to 0.0280\n",
      "[47] : train loss 0.093890, val loss drop 0.0280 to 0.0265\n",
      "[51] : train loss 0.074645, val loss drop 0.0265 to 0.0208\n",
      "[65] : train loss 0.068788, val loss drop 0.0208 to 0.0115\n",
      "[97] : train loss 0.191270, val loss drop 0.0115 to 0.0058\n",
      "fold 5\n",
      "[1] : train loss 40.324012, val loss drop 10000000.0000 to 6.7019\n",
      "[4] : train loss 0.771969, val loss drop 6.7019 to 3.8369\n",
      "[5] : train loss 0.480139, val loss drop 3.8369 to 0.7510\n",
      "[6] : train loss 0.421900, val loss drop 0.7510 to 0.6211\n",
      "[7] : train loss 0.363156, val loss drop 0.6211 to 0.3172\n",
      "[8] : train loss 0.309602, val loss drop 0.3172 to 0.2641\n",
      "[9] : train loss 0.368342, val loss drop 0.2641 to 0.0803\n",
      "[19] : train loss 0.125677, val loss drop 0.0803 to 0.0484\n",
      "[24] : train loss 0.233897, val loss drop 0.0484 to 0.0208\n",
      "[57] : train loss 0.093266, val loss drop 0.0208 to 0.0109\n",
      "[84] : train loss 0.062322, val loss drop 0.0109 to 0.0101\n",
      "fold 6\n",
      "[1] : train loss 67.853356, val loss drop 10000000.0000 to 22.6422\n",
      "[4] : train loss 1.003475, val loss drop 22.6422 to 7.9984\n",
      "[5] : train loss 0.695399, val loss drop 7.9984 to 1.8135\n",
      "[7] : train loss 0.477070, val loss drop 1.8135 to 0.3511\n",
      "[9] : train loss 0.280897, val loss drop 0.3511 to 0.2525\n",
      "[10] : train loss 0.212350, val loss drop 0.2525 to 0.2177\n",
      "[13] : train loss 0.364296, val loss drop 0.2177 to 0.1424\n",
      "[16] : train loss 0.161165, val loss drop 0.1424 to 0.1024\n",
      "[17] : train loss 0.197742, val loss drop 0.1024 to 0.0592\n",
      "[43] : train loss 0.106927, val loss drop 0.0592 to 0.0345\n",
      "[44] : train loss 0.133710, val loss drop 0.0345 to 0.0285\n",
      "[61] : train loss 0.119079, val loss drop 0.0285 to 0.0162\n",
      "fold 7\n",
      "[1] : train loss 27.882758, val loss drop 10000000.0000 to 97.9407\n",
      "[2] : train loss 2.472635, val loss drop 97.9407 to 22.6149\n",
      "[3] : train loss 1.266062, val loss drop 22.6149 to 16.2051\n",
      "[4] : train loss 0.610857, val loss drop 16.2051 to 2.3303\n",
      "[5] : train loss 0.521316, val loss drop 2.3303 to 0.3318\n",
      "[6] : train loss 0.276580, val loss drop 0.3318 to 0.2800\n",
      "[7] : train loss 0.472026, val loss drop 0.2800 to 0.2314\n",
      "[8] : train loss 0.229924, val loss drop 0.2314 to 0.1701\n",
      "[10] : train loss 0.268472, val loss drop 0.1701 to 0.1558\n",
      "[11] : train loss 0.237257, val loss drop 0.1558 to 0.0510\n",
      "[19] : train loss 0.142107, val loss drop 0.0510 to 0.0419\n",
      "[26] : train loss 0.104386, val loss drop 0.0419 to 0.0334\n",
      "[43] : train loss 0.339064, val loss drop 0.0334 to 0.0192\n",
      "[66] : train loss 0.065139, val loss drop 0.0192 to 0.0183\n",
      "[67] : train loss 0.127542, val loss drop 0.0183 to 0.0075\n",
      "fold 8\n",
      "[1] : train loss 32.683599, val loss drop 10000000.0000 to 93.2398\n",
      "[2] : train loss 2.719986, val loss drop 93.2398 to 12.0391\n",
      "[3] : train loss 1.146041, val loss drop 12.0391 to 4.4645\n",
      "[4] : train loss 0.658750, val loss drop 4.4645 to 1.0899\n",
      "[5] : train loss 0.444908, val loss drop 1.0899 to 0.1831\n",
      "[12] : train loss 0.233858, val loss drop 0.1831 to 0.1704\n",
      "[14] : train loss 0.360564, val loss drop 0.1704 to 0.1684\n",
      "[17] : train loss 0.189116, val loss drop 0.1684 to 0.1394\n",
      "[18] : train loss 0.252174, val loss drop 0.1394 to 0.0738\n",
      "[20] : train loss 0.234371, val loss drop 0.0738 to 0.0676\n",
      "[21] : train loss 0.122343, val loss drop 0.0676 to 0.0556\n",
      "[31] : train loss 0.300695, val loss drop 0.0556 to 0.0549\n",
      "[36] : train loss 0.086794, val loss drop 0.0549 to 0.0262\n",
      "[50] : train loss 0.125883, val loss drop 0.0262 to 0.0173\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "# define model\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [2, 375, n_features], (5, 1))\n",
    "fc = classifier([128, 64, 32, 16], input_size = 512*1*n_features, output_size = len(name))\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, '{}.csv'.format(save_path.split('/')[-1])), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

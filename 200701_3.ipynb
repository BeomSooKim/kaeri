{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cnn architecture 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "def fix_seed(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, scheduler, ep, loss_func):\n",
    "    \n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step(ep+ i/len(train_data)) for cosine annealing\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cnn(nn.Module):\n",
    "    def __init__(self, n_feature, out_len):\n",
    "        super(custom_cnn, self).__init__()\n",
    "        self.conv_kernel = (5, 1)\n",
    "        self.pool_kernel = (2, 1)\n",
    "        \n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=16, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.ELU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*1*n_feature, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(16, out_len)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.fe(x)\n",
    "        return self.fc(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 150\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis = 2):\n",
    "    mu = np.expand_dims(x.mean(axis = 2), axis = axis)\n",
    "    sd = np.expand_dims(x.std(axis = 2), axis = axis)\n",
    "\n",
    "    normalized = (x - mu) / sd\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "# concatenate normalized data\n",
    "train_norm = normalize(train_f)\n",
    "test_norm = normalize(test_f)\n",
    "\n",
    "train_f = np.concatenate((train_f, train_norm), axis = 1)\n",
    "test_f = np.concatenate((test_f, test_norm), axis = 1)\n",
    "\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        model = custom_cnn(n_features, len(name))\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        optimizer = torch.optim.Adam(model.parameters(curr_loss), lr = base_lr)\n",
    "        #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5, patience = 10, threshold = 1e-6, verbose = True)\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, scheduler, ep, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "            scheduler.step(val_loss)\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 3.003824, val loss drop 10000000.0000 to 2.5648\n",
      "[2] : train loss 1.652170, val loss drop 2.5648 to 0.7717\n",
      "[3] : train loss 0.307460, val loss drop 0.7717 to 0.0962\n",
      "[5] : train loss 0.017626, val loss drop 0.0962 to 0.0713\n",
      "[6] : train loss 0.008962, val loss drop 0.0713 to 0.0132\n",
      "[7] : train loss 0.004532, val loss drop 0.0132 to 0.0049\n",
      "[13] : train loss 0.003255, val loss drop 0.0049 to 0.0041\n",
      "[14] : train loss 0.002334, val loss drop 0.0041 to 0.0014\n",
      "[19] : train loss 0.001425, val loss drop 0.0014 to 0.0012\n",
      "[20] : train loss 0.001319, val loss drop 0.0012 to 0.0009\n",
      "[22] : train loss 0.000773, val loss drop 0.0009 to 0.0007\n",
      "[24] : train loss 0.000860, val loss drop 0.0007 to 0.0006\n",
      "[31] : train loss 0.000674, val loss drop 0.0006 to 0.0006\n",
      "[32] : train loss 0.000630, val loss drop 0.0006 to 0.0006\n",
      "[35] : train loss 0.000849, val loss drop 0.0006 to 0.0005\n",
      "Epoch    46: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[48] : train loss 0.000513, val loss drop 0.0005 to 0.0004\n",
      "[52] : train loss 0.000375, val loss drop 0.0004 to 0.0004\n",
      "[63] : train loss 0.000401, val loss drop 0.0004 to 0.0004\n",
      "[64] : train loss 0.000391, val loss drop 0.0004 to 0.0003\n",
      "[65] : train loss 0.000378, val loss drop 0.0003 to 0.0002\n",
      "Epoch    76: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[79] : train loss 0.000162, val loss drop 0.0002 to 0.0002\n",
      "[82] : train loss 0.000305, val loss drop 0.0002 to 0.0002\n",
      "Epoch    93: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch   104: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[112] : train loss 0.000189, val loss drop 0.0002 to 0.0001\n",
      "[123] : train loss 0.000108, val loss drop 0.0001 to 0.0001\n",
      "Epoch   134: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[135] : train loss 0.000130, val loss drop 0.0001 to 0.0001\n",
      "[143] : train loss 0.000101, val loss drop 0.0001 to 0.0001\n",
      "fold 2\n",
      "[1] : train loss 2.790748, val loss drop 10000000.0000 to 1.7241\n",
      "[2] : train loss 1.297008, val loss drop 1.7241 to 0.4814\n",
      "[3] : train loss 0.264503, val loss drop 0.4814 to 0.0894\n",
      "[5] : train loss 0.017057, val loss drop 0.0894 to 0.0482\n",
      "[6] : train loss 0.009716, val loss drop 0.0482 to 0.0103\n",
      "[7] : train loss 0.005098, val loss drop 0.0103 to 0.0061\n",
      "[8] : train loss 0.002902, val loss drop 0.0061 to 0.0038\n",
      "[9] : train loss 0.002443, val loss drop 0.0038 to 0.0031\n",
      "[12] : train loss 0.001376, val loss drop 0.0031 to 0.0016\n",
      "[14] : train loss 0.001088, val loss drop 0.0016 to 0.0010\n",
      "[24] : train loss 0.000993, val loss drop 0.0010 to 0.0006\n",
      "[34] : train loss 0.000782, val loss drop 0.0006 to 0.0004\n",
      "Epoch    45: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[48] : train loss 0.000539, val loss drop 0.0004 to 0.0004\n",
      "[49] : train loss 0.000284, val loss drop 0.0004 to 0.0004\n",
      "[50] : train loss 0.000282, val loss drop 0.0004 to 0.0003\n",
      "Epoch    61: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[63] : train loss 0.000222, val loss drop 0.0003 to 0.0002\n",
      "[65] : train loss 0.000159, val loss drop 0.0002 to 0.0002\n",
      "Epoch    76: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[80] : train loss 0.000129, val loss drop 0.0002 to 0.0002\n",
      "[85] : train loss 0.000148, val loss drop 0.0002 to 0.0001\n",
      "Epoch    96: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[103] : train loss 0.000177, val loss drop 0.0001 to 0.0001\n",
      "[110] : train loss 0.000202, val loss drop 0.0001 to 0.0001\n",
      "Epoch   121: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   132: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[136] : train loss 0.000203, val loss drop 0.0001 to 0.0001\n",
      "[145] : train loss 0.000086, val loss drop 0.0001 to 0.0001\n",
      "fold 3\n",
      "[1] : train loss 2.795531, val loss drop 10000000.0000 to 1.8271\n",
      "[2] : train loss 1.276090, val loss drop 1.8271 to 0.5530\n",
      "[3] : train loss 0.404370, val loss drop 0.5530 to 0.1292\n",
      "[4] : train loss 0.060890, val loss drop 0.1292 to 0.0956\n",
      "[5] : train loss 0.019691, val loss drop 0.0956 to 0.0405\n",
      "[6] : train loss 0.009528, val loss drop 0.0405 to 0.0140\n",
      "[9] : train loss 0.003342, val loss drop 0.0140 to 0.0069\n",
      "[10] : train loss 0.004208, val loss drop 0.0069 to 0.0030\n",
      "[12] : train loss 0.004543, val loss drop 0.0030 to 0.0026\n",
      "[14] : train loss 0.002345, val loss drop 0.0026 to 0.0018\n",
      "[21] : train loss 0.000921, val loss drop 0.0018 to 0.0012\n",
      "[26] : train loss 0.002016, val loss drop 0.0012 to 0.0010\n",
      "[28] : train loss 0.001279, val loss drop 0.0010 to 0.0007\n",
      "Epoch    39: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[41] : train loss 0.000503, val loss drop 0.0007 to 0.0006\n",
      "[45] : train loss 0.000414, val loss drop 0.0006 to 0.0005\n",
      "[46] : train loss 0.000718, val loss drop 0.0005 to 0.0004\n",
      "[53] : train loss 0.000741, val loss drop 0.0004 to 0.0003\n",
      "Epoch    64: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[66] : train loss 0.000419, val loss drop 0.0003 to 0.0003\n",
      "[68] : train loss 0.000551, val loss drop 0.0003 to 0.0002\n",
      "[69] : train loss 0.000286, val loss drop 0.0002 to 0.0002\n",
      "[76] : train loss 0.000278, val loss drop 0.0002 to 0.0002\n",
      "Epoch    87: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[88] : train loss 0.000375, val loss drop 0.0002 to 0.0002\n",
      "[89] : train loss 0.000354, val loss drop 0.0002 to 0.0002\n",
      "[90] : train loss 0.000311, val loss drop 0.0002 to 0.0002\n",
      "[92] : train loss 0.000495, val loss drop 0.0002 to 0.0002\n",
      "Epoch   103: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[104] : train loss 0.000199, val loss drop 0.0002 to 0.0001\n",
      "Epoch   115: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[123] : train loss 0.000101, val loss drop 0.0001 to 0.0001\n",
      "Epoch   134: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[145] : train loss 0.000195, val loss drop 0.0001 to 0.0001\n",
      "fold 4\n",
      "[1] : train loss 2.698167, val loss drop 10000000.0000 to 1.6508\n",
      "[2] : train loss 0.986998, val loss drop 1.6508 to 0.3130\n",
      "[3] : train loss 0.141083, val loss drop 0.3130 to 0.1033\n",
      "[4] : train loss 0.035705, val loss drop 0.1033 to 0.0615\n",
      "[5] : train loss 0.015791, val loss drop 0.0615 to 0.0583\n",
      "[6] : train loss 0.007927, val loss drop 0.0583 to 0.0102\n",
      "[8] : train loss 0.003533, val loss drop 0.0102 to 0.0095\n",
      "[9] : train loss 0.002725, val loss drop 0.0095 to 0.0023\n",
      "[14] : train loss 0.001143, val loss drop 0.0023 to 0.0020\n",
      "[19] : train loss 0.002891, val loss drop 0.0020 to 0.0011\n",
      "Epoch    30: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[31] : train loss 0.000430, val loss drop 0.0011 to 0.0005\n",
      "[33] : train loss 0.000508, val loss drop 0.0005 to 0.0005\n",
      "[34] : train loss 0.000378, val loss drop 0.0005 to 0.0003\n",
      "[41] : train loss 0.000269, val loss drop 0.0003 to 0.0003\n",
      "Epoch    52: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    63: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[65] : train loss 0.000169, val loss drop 0.0003 to 0.0002\n",
      "[69] : train loss 0.000157, val loss drop 0.0002 to 0.0002\n",
      "[75] : train loss 0.000142, val loss drop 0.0002 to 0.0002\n",
      "Epoch    86: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[90] : train loss 0.000187, val loss drop 0.0002 to 0.0002\n",
      "[100] : train loss 0.000136, val loss drop 0.0002 to 0.0002\n",
      "Epoch   111: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   122: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[129] : train loss 0.000140, val loss drop 0.0002 to 0.0001\n",
      "Epoch   140: reducing learning rate of group 0 to 7.8125e-06.\n",
      "fold 5\n",
      "[1] : train loss 2.875659, val loss drop 10000000.0000 to 1.7684\n",
      "[2] : train loss 1.147636, val loss drop 1.7684 to 0.2841\n",
      "[3] : train loss 0.144552, val loss drop 0.2841 to 0.1426\n",
      "[5] : train loss 0.014820, val loss drop 0.1426 to 0.0419\n",
      "[6] : train loss 0.008332, val loss drop 0.0419 to 0.0092\n",
      "[7] : train loss 0.004958, val loss drop 0.0092 to 0.0055\n",
      "[8] : train loss 0.002807, val loss drop 0.0055 to 0.0027\n",
      "[9] : train loss 0.002511, val loss drop 0.0027 to 0.0016\n",
      "[11] : train loss 0.001383, val loss drop 0.0016 to 0.0016\n",
      "[17] : train loss 0.001427, val loss drop 0.0016 to 0.0014\n",
      "[18] : train loss 0.000762, val loss drop 0.0014 to 0.0007\n",
      "[28] : train loss 0.000676, val loss drop 0.0007 to 0.0006\n",
      "[31] : train loss 0.000946, val loss drop 0.0006 to 0.0006\n",
      "[32] : train loss 0.000854, val loss drop 0.0006 to 0.0005\n",
      "Epoch    43: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[49] : train loss 0.000402, val loss drop 0.0005 to 0.0004\n",
      "[53] : train loss 0.000457, val loss drop 0.0004 to 0.0003\n",
      "[59] : train loss 0.000322, val loss drop 0.0003 to 0.0002\n",
      "[64] : train loss 0.000387, val loss drop 0.0002 to 0.0002\n",
      "[69] : train loss 0.000312, val loss drop 0.0002 to 0.0002\n",
      "[74] : train loss 0.000362, val loss drop 0.0002 to 0.0002\n",
      "Epoch    85: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[95] : train loss 0.000224, val loss drop 0.0002 to 0.0002\n",
      "[102] : train loss 0.000204, val loss drop 0.0002 to 0.0002\n",
      "Epoch   113: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[114] : train loss 0.000328, val loss drop 0.0002 to 0.0002\n",
      "[122] : train loss 0.000231, val loss drop 0.0002 to 0.0001\n",
      "[125] : train loss 0.000163, val loss drop 0.0001 to 0.0001\n",
      "[126] : train loss 0.000087, val loss drop 0.0001 to 0.0001\n",
      "Epoch   137: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[144] : train loss 0.000135, val loss drop 0.0001 to 0.0001\n",
      "fold 6\n",
      "[1] : train loss 2.784622, val loss drop 10000000.0000 to 2.3456\n",
      "[2] : train loss 1.673862, val loss drop 2.3456 to 1.3974\n",
      "[3] : train loss 0.996530, val loss drop 1.3974 to 1.0561\n",
      "[4] : train loss 0.703546, val loss drop 1.0561 to 0.4664\n",
      "[5] : train loss 0.310364, val loss drop 0.4664 to 0.1111\n",
      "[6] : train loss 0.047272, val loss drop 0.1111 to 0.0280\n",
      "[8] : train loss 0.008620, val loss drop 0.0280 to 0.0077\n",
      "[9] : train loss 0.004998, val loss drop 0.0077 to 0.0055\n",
      "[10] : train loss 0.003968, val loss drop 0.0055 to 0.0037\n",
      "[15] : train loss 0.002346, val loss drop 0.0037 to 0.0021\n",
      "[17] : train loss 0.001528, val loss drop 0.0021 to 0.0018\n",
      "[19] : train loss 0.001457, val loss drop 0.0018 to 0.0015\n",
      "[25] : train loss 0.001114, val loss drop 0.0015 to 0.0012\n",
      "[28] : train loss 0.002132, val loss drop 0.0012 to 0.0009\n",
      "Epoch    39: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[42] : train loss 0.000995, val loss drop 0.0009 to 0.0008\n",
      "[43] : train loss 0.000964, val loss drop 0.0008 to 0.0007\n",
      "[44] : train loss 0.000783, val loss drop 0.0007 to 0.0005\n",
      "[45] : train loss 0.001087, val loss drop 0.0005 to 0.0004\n",
      "[48] : train loss 0.000652, val loss drop 0.0004 to 0.0003\n",
      "Epoch    59: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[60] : train loss 0.000607, val loss drop 0.0003 to 0.0003\n",
      "[67] : train loss 0.000588, val loss drop 0.0003 to 0.0003\n",
      "[73] : train loss 0.000413, val loss drop 0.0003 to 0.0003\n",
      "[81] : train loss 0.000468, val loss drop 0.0003 to 0.0003\n",
      "[83] : train loss 0.000420, val loss drop 0.0003 to 0.0002\n",
      "Epoch    94: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[97] : train loss 0.000482, val loss drop 0.0002 to 0.0002\n",
      "[99] : train loss 0.000269, val loss drop 0.0002 to 0.0002\n",
      "[109] : train loss 0.000342, val loss drop 0.0002 to 0.0002\n",
      "Epoch   120: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[126] : train loss 0.000290, val loss drop 0.0002 to 0.0002\n",
      "[129] : train loss 0.000209, val loss drop 0.0002 to 0.0001\n",
      "Epoch   140: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[143] : train loss 0.000287, val loss drop 0.0001 to 0.0001\n",
      "fold 7\n",
      "[1] : train loss 2.925622, val loss drop 10000000.0000 to 1.9686\n",
      "[2] : train loss 1.470711, val loss drop 1.9686 to 0.4988\n",
      "[3] : train loss 0.243048, val loss drop 0.4988 to 0.1014\n",
      "[5] : train loss 0.018331, val loss drop 0.1014 to 0.0219\n",
      "[6] : train loss 0.008110, val loss drop 0.0219 to 0.0131\n",
      "[7] : train loss 0.005135, val loss drop 0.0131 to 0.0057\n",
      "[8] : train loss 0.002952, val loss drop 0.0057 to 0.0040\n",
      "[9] : train loss 0.002526, val loss drop 0.0040 to 0.0032\n",
      "[10] : train loss 0.001828, val loss drop 0.0032 to 0.0025\n",
      "[12] : train loss 0.001323, val loss drop 0.0025 to 0.0019\n",
      "[13] : train loss 0.001302, val loss drop 0.0019 to 0.0014\n",
      "[17] : train loss 0.001663, val loss drop 0.0014 to 0.0014\n",
      "[18] : train loss 0.001025, val loss drop 0.0014 to 0.0008\n",
      "[28] : train loss 0.000747, val loss drop 0.0008 to 0.0008\n",
      "[31] : train loss 0.001304, val loss drop 0.0008 to 0.0006\n",
      "Epoch    42: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[46] : train loss 0.000724, val loss drop 0.0006 to 0.0005\n",
      "[47] : train loss 0.000360, val loss drop 0.0005 to 0.0004\n",
      "[51] : train loss 0.000258, val loss drop 0.0004 to 0.0003\n",
      "[59] : train loss 0.000428, val loss drop 0.0003 to 0.0003\n",
      "Epoch    70: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[73] : train loss 0.000304, val loss drop 0.0003 to 0.0002\n",
      "[77] : train loss 0.000398, val loss drop 0.0002 to 0.0002\n",
      "Epoch    88: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[89] : train loss 0.000173, val loss drop 0.0002 to 0.0002\n",
      "[99] : train loss 0.000238, val loss drop 0.0002 to 0.0002\n",
      "[104] : train loss 0.000162, val loss drop 0.0002 to 0.0001\n",
      "Epoch   115: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[123] : train loss 0.000142, val loss drop 0.0001 to 0.0001\n",
      "Epoch   134: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[138] : train loss 0.000127, val loss drop 0.0001 to 0.0001\n",
      "Epoch   149: reducing learning rate of group 0 to 1.5625e-05.\n",
      "fold 8\n",
      "[1] : train loss 2.876223, val loss drop 10000000.0000 to 2.4316\n",
      "[2] : train loss 1.998382, val loss drop 2.4316 to 1.5321\n",
      "[3] : train loss 0.961478, val loss drop 1.5321 to 0.3572\n",
      "[4] : train loss 0.181416, val loss drop 0.3572 to 0.0858\n",
      "[5] : train loss 0.037603, val loss drop 0.0858 to 0.0470\n",
      "[6] : train loss 0.017869, val loss drop 0.0470 to 0.0298\n",
      "[7] : train loss 0.008826, val loss drop 0.0298 to 0.0103\n",
      "[8] : train loss 0.005423, val loss drop 0.0103 to 0.0088\n",
      "[9] : train loss 0.004209, val loss drop 0.0088 to 0.0046\n",
      "[11] : train loss 0.002719, val loss drop 0.0046 to 0.0027\n",
      "[12] : train loss 0.001925, val loss drop 0.0027 to 0.0024\n",
      "[20] : train loss 0.002851, val loss drop 0.0024 to 0.0015\n",
      "[27] : train loss 0.000899, val loss drop 0.0015 to 0.0012\n",
      "[28] : train loss 0.000890, val loss drop 0.0012 to 0.0008\n",
      "[30] : train loss 0.000834, val loss drop 0.0008 to 0.0006\n",
      "Epoch    41: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[51] : train loss 0.000592, val loss drop 0.0006 to 0.0004\n",
      "[52] : train loss 0.000434, val loss drop 0.0004 to 0.0004\n",
      "[60] : train loss 0.000594, val loss drop 0.0004 to 0.0003\n",
      "Epoch    71: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[74] : train loss 0.000158, val loss drop 0.0003 to 0.0003\n",
      "Epoch    85: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[86] : train loss 0.000205, val loss drop 0.0003 to 0.0003\n",
      "[97] : train loss 0.000390, val loss drop 0.0003 to 0.0002\n",
      "[99] : train loss 0.000212, val loss drop 0.0002 to 0.0002\n",
      "Epoch   110: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch   121: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[131] : train loss 0.000124, val loss drop 0.0002 to 0.0002\n",
      "[141] : train loss 0.000113, val loss drop 0.0002 to 0.0002\n",
      "fold 9\n",
      "[1] : train loss 2.699310, val loss drop 10000000.0000 to 1.4584\n",
      "[2] : train loss 1.020672, val loss drop 1.4584 to 0.2474\n",
      "[3] : train loss 0.108426, val loss drop 0.2474 to 0.0329\n",
      "[6] : train loss 0.006704, val loss drop 0.0329 to 0.0052\n",
      "[7] : train loss 0.003135, val loss drop 0.0052 to 0.0040\n",
      "[8] : train loss 0.002129, val loss drop 0.0040 to 0.0038\n",
      "[11] : train loss 0.001546, val loss drop 0.0038 to 0.0015\n",
      "[13] : train loss 0.001504, val loss drop 0.0015 to 0.0011\n",
      "[15] : train loss 0.001239, val loss drop 0.0011 to 0.0010\n",
      "[16] : train loss 0.001702, val loss drop 0.0010 to 0.0008\n",
      "[17] : train loss 0.000804, val loss drop 0.0008 to 0.0007\n",
      "[19] : train loss 0.001002, val loss drop 0.0007 to 0.0006\n",
      "[28] : train loss 0.000680, val loss drop 0.0006 to 0.0003\n",
      "Epoch    39: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[44] : train loss 0.000481, val loss drop 0.0003 to 0.0003\n",
      "Epoch    55: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[58] : train loss 0.000474, val loss drop 0.0003 to 0.0002\n",
      "[60] : train loss 0.000361, val loss drop 0.0002 to 0.0002\n",
      "[64] : train loss 0.000280, val loss drop 0.0002 to 0.0002\n",
      "Epoch    75: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[84] : train loss 0.000115, val loss drop 0.0002 to 0.0002\n",
      "[94] : train loss 0.000169, val loss drop 0.0002 to 0.0002\n",
      "Epoch   105: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[106] : train loss 0.000304, val loss drop 0.0002 to 0.0002\n",
      "Epoch   117: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[121] : train loss 0.000126, val loss drop 0.0002 to 0.0001\n",
      "[122] : train loss 0.000171, val loss drop 0.0001 to 0.0001\n",
      "[128] : train loss 0.000161, val loss drop 0.0001 to 0.0001\n",
      "[133] : train loss 0.000089, val loss drop 0.0001 to 0.0001\n",
      "[137] : train loss 0.000115, val loss drop 0.0001 to 0.0001\n",
      "Epoch   148: reducing learning rate of group 0 to 1.5625e-05.\n",
      "fold 10\n",
      "[1] : train loss 2.869585, val loss drop 10000000.0000 to 2.1376\n",
      "[2] : train loss 1.596283, val loss drop 2.1376 to 1.0731\n",
      "[3] : train loss 0.514748, val loss drop 1.0731 to 0.1471\n",
      "[4] : train loss 0.083340, val loss drop 0.1471 to 0.0964\n",
      "[5] : train loss 0.025568, val loss drop 0.0964 to 0.0645\n",
      "[6] : train loss 0.010983, val loss drop 0.0645 to 0.0141\n",
      "[8] : train loss 0.003944, val loss drop 0.0141 to 0.0093\n",
      "[9] : train loss 0.003688, val loss drop 0.0093 to 0.0061\n",
      "[11] : train loss 0.003683, val loss drop 0.0061 to 0.0032\n",
      "[12] : train loss 0.002618, val loss drop 0.0032 to 0.0023\n",
      "[17] : train loss 0.001622, val loss drop 0.0023 to 0.0015\n",
      "[19] : train loss 0.001504, val loss drop 0.0015 to 0.0009\n",
      "[27] : train loss 0.001255, val loss drop 0.0009 to 0.0008\n",
      "[33] : train loss 0.002301, val loss drop 0.0008 to 0.0008\n",
      "[37] : train loss 0.000866, val loss drop 0.0008 to 0.0005\n",
      "Epoch    48: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[50] : train loss 0.001018, val loss drop 0.0005 to 0.0005\n",
      "[51] : train loss 0.000916, val loss drop 0.0005 to 0.0004\n",
      "[53] : train loss 0.000850, val loss drop 0.0004 to 0.0004\n",
      "[54] : train loss 0.000657, val loss drop 0.0004 to 0.0003\n",
      "[56] : train loss 0.000372, val loss drop 0.0003 to 0.0002\n",
      "[58] : train loss 0.000300, val loss drop 0.0002 to 0.0002\n",
      "Epoch    69: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[76] : train loss 0.000270, val loss drop 0.0002 to 0.0002\n",
      "[80] : train loss 0.000304, val loss drop 0.0002 to 0.0002\n",
      "Epoch    91: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch   102: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[109] : train loss 0.000172, val loss drop 0.0002 to 0.0001\n",
      "[117] : train loss 0.000185, val loss drop 0.0001 to 0.0001\n",
      "[119] : train loss 0.000262, val loss drop 0.0001 to 0.0001\n",
      "Epoch   130: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   141: reducing learning rate of group 0 to 1.5625e-05.\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 9.338630, val loss drop 10000000.0000 to 4.0952\n",
      "[2] : train loss 0.728049, val loss drop 4.0952 to 0.3853\n",
      "[3] : train loss 0.201027, val loss drop 0.3853 to 0.1311\n",
      "[4] : train loss 0.087667, val loss drop 0.1311 to 0.0939\n",
      "[5] : train loss 0.044580, val loss drop 0.0939 to 0.0445\n",
      "[6] : train loss 0.025490, val loss drop 0.0445 to 0.0217\n",
      "[7] : train loss 0.017593, val loss drop 0.0217 to 0.0205\n",
      "[9] : train loss 0.010892, val loss drop 0.0205 to 0.0124\n",
      "[13] : train loss 0.008141, val loss drop 0.0124 to 0.0123\n",
      "[14] : train loss 0.006759, val loss drop 0.0123 to 0.0104\n",
      "[15] : train loss 0.006827, val loss drop 0.0104 to 0.0081\n",
      "[19] : train loss 0.004605, val loss drop 0.0081 to 0.0069\n",
      "[21] : train loss 0.004736, val loss drop 0.0069 to 0.0058\n",
      "[28] : train loss 0.006018, val loss drop 0.0058 to 0.0056\n",
      "[29] : train loss 0.003513, val loss drop 0.0056 to 0.0049\n",
      "[31] : train loss 0.004010, val loss drop 0.0049 to 0.0043\n",
      "[38] : train loss 0.004253, val loss drop 0.0043 to 0.0035\n",
      "[39] : train loss 0.002632, val loss drop 0.0035 to 0.0034\n",
      "Epoch    50: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[53] : train loss 0.002606, val loss drop 0.0034 to 0.0026\n",
      "Epoch    64: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[65] : train loss 0.001285, val loss drop 0.0026 to 0.0022\n",
      "[66] : train loss 0.001439, val loss drop 0.0022 to 0.0021\n",
      "Epoch    77: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[78] : train loss 0.001031, val loss drop 0.0021 to 0.0020\n",
      "[81] : train loss 0.001166, val loss drop 0.0020 to 0.0018\n",
      "Epoch    92: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch   103: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   114: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch   125: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[127] : train loss 0.000779, val loss drop 0.0018 to 0.0017\n",
      "Epoch   138: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch   149: reducing learning rate of group 0 to 1.9531e-06.\n",
      "fold 2\n",
      "[1] : train loss 14.709924, val loss drop 10000000.0000 to 1.2790\n",
      "[2] : train loss 0.767984, val loss drop 1.2790 to 0.5467\n",
      "[3] : train loss 0.221538, val loss drop 0.5467 to 0.1423\n",
      "[4] : train loss 0.079497, val loss drop 0.1423 to 0.0514\n",
      "[5] : train loss 0.044767, val loss drop 0.0514 to 0.0356\n",
      "[6] : train loss 0.026958, val loss drop 0.0356 to 0.0284\n",
      "[7] : train loss 0.018884, val loss drop 0.0284 to 0.0219\n",
      "[8] : train loss 0.017022, val loss drop 0.0219 to 0.0146\n",
      "[9] : train loss 0.013892, val loss drop 0.0146 to 0.0126\n",
      "[10] : train loss 0.012031, val loss drop 0.0126 to 0.0117\n",
      "[13] : train loss 0.010156, val loss drop 0.0117 to 0.0114\n",
      "[14] : train loss 0.009919, val loss drop 0.0114 to 0.0096\n",
      "[15] : train loss 0.008963, val loss drop 0.0096 to 0.0093\n",
      "[16] : train loss 0.008500, val loss drop 0.0093 to 0.0074\n",
      "[21] : train loss 0.007517, val loss drop 0.0074 to 0.0061\n",
      "[25] : train loss 0.005493, val loss drop 0.0061 to 0.0050\n",
      "Epoch    36: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[37] : train loss 0.003225, val loss drop 0.0050 to 0.0041\n",
      "[38] : train loss 0.003120, val loss drop 0.0041 to 0.0038\n",
      "[41] : train loss 0.004109, val loss drop 0.0038 to 0.0033\n",
      "Epoch    52: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[59] : train loss 0.002878, val loss drop 0.0033 to 0.0031\n",
      "Epoch    70: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[76] : train loss 0.002162, val loss drop 0.0031 to 0.0031\n",
      "[83] : train loss 0.002369, val loss drop 0.0031 to 0.0030\n",
      "[85] : train loss 0.001869, val loss drop 0.0030 to 0.0026\n",
      "Epoch    96: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[101] : train loss 0.001698, val loss drop 0.0026 to 0.0025\n",
      "[107] : train loss 0.001886, val loss drop 0.0025 to 0.0023\n",
      "Epoch   118: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   129: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch   140: reducing learning rate of group 0 to 7.8125e-06.\n",
      "fold 3\n",
      "[1] : train loss 12.137720, val loss drop 10000000.0000 to 4.4259\n",
      "[2] : train loss 0.506402, val loss drop 4.4259 to 0.6555\n",
      "[3] : train loss 0.136547, val loss drop 0.6555 to 0.1300\n",
      "[4] : train loss 0.051966, val loss drop 0.1300 to 0.0426\n",
      "[5] : train loss 0.031658, val loss drop 0.0426 to 0.0139\n",
      "[7] : train loss 0.011248, val loss drop 0.0139 to 0.0093\n",
      "[8] : train loss 0.009129, val loss drop 0.0093 to 0.0076\n",
      "[9] : train loss 0.007658, val loss drop 0.0076 to 0.0067\n",
      "[10] : train loss 0.007109, val loss drop 0.0067 to 0.0052\n",
      "[12] : train loss 0.005455, val loss drop 0.0052 to 0.0038\n",
      "[18] : train loss 0.004296, val loss drop 0.0038 to 0.0032\n",
      "[23] : train loss 0.004885, val loss drop 0.0032 to 0.0026\n",
      "[24] : train loss 0.003063, val loss drop 0.0026 to 0.0023\n",
      "[29] : train loss 0.002924, val loss drop 0.0023 to 0.0019\n",
      "[34] : train loss 0.002070, val loss drop 0.0019 to 0.0019\n",
      "[45] : train loss 0.004240, val loss drop 0.0019 to 0.0019\n",
      "[48] : train loss 0.003638, val loss drop 0.0019 to 0.0015\n",
      "Epoch    59: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[61] : train loss 0.001002, val loss drop 0.0015 to 0.0015\n",
      "[66] : train loss 0.001121, val loss drop 0.0015 to 0.0014\n",
      "[74] : train loss 0.001401, val loss drop 0.0014 to 0.0014\n",
      "Epoch    85: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[88] : train loss 0.001787, val loss drop 0.0014 to 0.0011\n",
      "[91] : train loss 0.000873, val loss drop 0.0011 to 0.0011\n",
      "Epoch   102: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[106] : train loss 0.001132, val loss drop 0.0011 to 0.0011\n",
      "[117] : train loss 0.000480, val loss drop 0.0011 to 0.0009\n",
      "Epoch   128: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[131] : train loss 0.000734, val loss drop 0.0009 to 0.0008\n",
      "Epoch   142: reducing learning rate of group 0 to 3.1250e-05.\n",
      "fold 4\n",
      "[1] : train loss 17.038261, val loss drop 10000000.0000 to 5.8764\n",
      "[2] : train loss 0.419399, val loss drop 5.8764 to 0.5803\n",
      "[3] : train loss 0.128943, val loss drop 0.5803 to 0.0836\n",
      "[4] : train loss 0.065455, val loss drop 0.0836 to 0.0382\n",
      "[5] : train loss 0.030081, val loss drop 0.0382 to 0.0224\n",
      "[6] : train loss 0.018929, val loss drop 0.0224 to 0.0201\n",
      "[9] : train loss 0.018509, val loss drop 0.0201 to 0.0116\n",
      "[10] : train loss 0.012150, val loss drop 0.0116 to 0.0071\n",
      "[16] : train loss 0.010207, val loss drop 0.0071 to 0.0060\n",
      "[19] : train loss 0.011700, val loss drop 0.0060 to 0.0055\n",
      "[24] : train loss 0.014613, val loss drop 0.0055 to 0.0043\n",
      "[28] : train loss 0.010342, val loss drop 0.0043 to 0.0037\n",
      "[37] : train loss 0.013193, val loss drop 0.0037 to 0.0033\n",
      "[41] : train loss 0.012423, val loss drop 0.0033 to 0.0031\n",
      "[47] : train loss 0.004410, val loss drop 0.0031 to 0.0026\n",
      "[53] : train loss 0.003494, val loss drop 0.0026 to 0.0018\n",
      "[63] : train loss 0.003140, val loss drop 0.0018 to 0.0016\n",
      "Epoch    74: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[75] : train loss 0.001554, val loss drop 0.0016 to 0.0015\n",
      "[78] : train loss 0.001610, val loss drop 0.0015 to 0.0013\n",
      "Epoch    89: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[96] : train loss 0.000858, val loss drop 0.0013 to 0.0012\n",
      "[99] : train loss 0.001232, val loss drop 0.0012 to 0.0011\n",
      "Epoch   110: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[121] : train loss 0.001123, val loss drop 0.0011 to 0.0011\n",
      "[122] : train loss 0.000974, val loss drop 0.0011 to 0.0010\n",
      "Epoch   133: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch   144: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[150] : train loss 0.000780, val loss drop 0.0010 to 0.0009\n",
      "fold 5\n",
      "[1] : train loss 9.076488, val loss drop 10000000.0000 to 5.2724\n",
      "[2] : train loss 0.363321, val loss drop 5.2724 to 0.1135\n",
      "[3] : train loss 0.135905, val loss drop 0.1135 to 0.0969\n",
      "[4] : train loss 0.075535, val loss drop 0.0969 to 0.0886\n",
      "[5] : train loss 0.042333, val loss drop 0.0886 to 0.0490\n",
      "[6] : train loss 0.025172, val loss drop 0.0490 to 0.0197\n",
      "[8] : train loss 0.070038, val loss drop 0.0197 to 0.0140\n",
      "[9] : train loss 0.022772, val loss drop 0.0140 to 0.0120\n",
      "[12] : train loss 0.026703, val loss drop 0.0120 to 0.0087\n",
      "[15] : train loss 0.016633, val loss drop 0.0087 to 0.0087\n",
      "[20] : train loss 0.009354, val loss drop 0.0087 to 0.0083\n",
      "[22] : train loss 0.013255, val loss drop 0.0083 to 0.0059\n",
      "[26] : train loss 0.007336, val loss drop 0.0059 to 0.0051\n",
      "[27] : train loss 0.005032, val loss drop 0.0051 to 0.0040\n",
      "Epoch    38: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[42] : train loss 0.003687, val loss drop 0.0040 to 0.0030\n",
      "Epoch    53: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[58] : train loss 0.002868, val loss drop 0.0030 to 0.0028\n",
      "[60] : train loss 0.003800, val loss drop 0.0028 to 0.0024\n",
      "[63] : train loss 0.001390, val loss drop 0.0024 to 0.0022\n",
      "[69] : train loss 0.001786, val loss drop 0.0022 to 0.0021\n",
      "Epoch    80: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[88] : train loss 0.001476, val loss drop 0.0021 to 0.0020\n",
      "Epoch    99: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[105] : train loss 0.001044, val loss drop 0.0020 to 0.0019\n",
      "Epoch   116: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   127: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[131] : train loss 0.000809, val loss drop 0.0019 to 0.0019\n",
      "[137] : train loss 0.000923, val loss drop 0.0019 to 0.0019\n",
      "[139] : train loss 0.000906, val loss drop 0.0019 to 0.0017\n",
      "Epoch   150: reducing learning rate of group 0 to 7.8125e-06.\n",
      "fold 6\n",
      "[1] : train loss 14.342341, val loss drop 10000000.0000 to 3.0835\n",
      "[2] : train loss 0.699263, val loss drop 3.0835 to 0.2795\n",
      "[3] : train loss 0.211281, val loss drop 0.2795 to 0.1532\n",
      "[4] : train loss 0.090869, val loss drop 0.1532 to 0.1007\n",
      "[5] : train loss 0.051824, val loss drop 0.1007 to 0.0500\n",
      "[6] : train loss 0.028375, val loss drop 0.0500 to 0.0222\n",
      "[7] : train loss 0.019734, val loss drop 0.0222 to 0.0174\n",
      "[11] : train loss 0.012665, val loss drop 0.0174 to 0.0133\n",
      "[12] : train loss 0.011010, val loss drop 0.0133 to 0.0107\n",
      "[13] : train loss 0.007950, val loss drop 0.0107 to 0.0085\n",
      "[17] : train loss 0.011524, val loss drop 0.0085 to 0.0078\n",
      "[24] : train loss 0.005443, val loss drop 0.0078 to 0.0046\n",
      "[32] : train loss 0.006503, val loss drop 0.0046 to 0.0040\n",
      "[37] : train loss 0.004667, val loss drop 0.0040 to 0.0038\n",
      "[47] : train loss 0.004362, val loss drop 0.0038 to 0.0025\n",
      "[57] : train loss 0.001928, val loss drop 0.0025 to 0.0018\n",
      "Epoch    68: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    79: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    90: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch   101: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[110] : train loss 0.000813, val loss drop 0.0018 to 0.0018\n",
      "[111] : train loss 0.001020, val loss drop 0.0018 to 0.0017\n",
      "Epoch   122: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[123] : train loss 0.000730, val loss drop 0.0017 to 0.0017\n",
      "Epoch   134: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[141] : train loss 0.000714, val loss drop 0.0017 to 0.0017\n",
      "fold 7\n",
      "[1] : train loss 9.644319, val loss drop 10000000.0000 to 1.0840\n",
      "[2] : train loss 0.371154, val loss drop 1.0840 to 0.1596\n",
      "[3] : train loss 0.111647, val loss drop 0.1596 to 0.1461\n",
      "[4] : train loss 0.038222, val loss drop 0.1461 to 0.0367\n",
      "[5] : train loss 0.018418, val loss drop 0.0367 to 0.0157\n",
      "[6] : train loss 0.012342, val loss drop 0.0157 to 0.0117\n",
      "[7] : train loss 0.008996, val loss drop 0.0117 to 0.0076\n",
      "[9] : train loss 0.007069, val loss drop 0.0076 to 0.0056\n",
      "[12] : train loss 0.005711, val loss drop 0.0056 to 0.0046\n",
      "[14] : train loss 0.004161, val loss drop 0.0046 to 0.0043\n",
      "[19] : train loss 0.004769, val loss drop 0.0043 to 0.0035\n",
      "[24] : train loss 0.003703, val loss drop 0.0035 to 0.0029\n",
      "[27] : train loss 0.002754, val loss drop 0.0029 to 0.0021\n",
      "[36] : train loss 0.002197, val loss drop 0.0021 to 0.0020\n",
      "[47] : train loss 0.009865, val loss drop 0.0020 to 0.0018\n",
      "[54] : train loss 0.008155, val loss drop 0.0018 to 0.0017\n",
      "[55] : train loss 0.004384, val loss drop 0.0017 to 0.0014\n",
      "[61] : train loss 0.003096, val loss drop 0.0014 to 0.0012\n",
      "Epoch    72: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    83: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[88] : train loss 0.001390, val loss drop 0.0012 to 0.0011\n",
      "[89] : train loss 0.000832, val loss drop 0.0011 to 0.0010\n",
      "[96] : train loss 0.001267, val loss drop 0.0010 to 0.0008\n",
      "Epoch   107: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[108] : train loss 0.000704, val loss drop 0.0008 to 0.0008\n",
      "Epoch   119: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch   130: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   141: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[149] : train loss 0.000558, val loss drop 0.0008 to 0.0008\n",
      "fold 8\n",
      "[1] : train loss 37.553790, val loss drop 10000000.0000 to 2.1340\n",
      "[2] : train loss 0.549802, val loss drop 2.1340 to 0.4525\n",
      "[3] : train loss 0.185914, val loss drop 0.4525 to 0.1043\n",
      "[4] : train loss 0.068405, val loss drop 0.1043 to 0.0436\n",
      "[5] : train loss 0.036964, val loss drop 0.0436 to 0.0390\n",
      "[6] : train loss 0.027791, val loss drop 0.0390 to 0.0300\n",
      "[7] : train loss 0.022083, val loss drop 0.0300 to 0.0238\n",
      "[8] : train loss 0.019863, val loss drop 0.0238 to 0.0187\n",
      "[12] : train loss 0.010799, val loss drop 0.0187 to 0.0174\n",
      "[14] : train loss 0.011982, val loss drop 0.0174 to 0.0125\n",
      "[16] : train loss 0.008753, val loss drop 0.0125 to 0.0109\n",
      "[21] : train loss 0.005527, val loss drop 0.0109 to 0.0090\n",
      "[25] : train loss 0.005564, val loss drop 0.0090 to 0.0076\n",
      "[29] : train loss 0.005441, val loss drop 0.0076 to 0.0069\n",
      "[37] : train loss 0.006880, val loss drop 0.0069 to 0.0062\n",
      "[39] : train loss 0.004889, val loss drop 0.0062 to 0.0057\n",
      "[42] : train loss 0.003200, val loss drop 0.0057 to 0.0055\n",
      "[50] : train loss 0.003504, val loss drop 0.0055 to 0.0044\n",
      "[52] : train loss 0.003220, val loss drop 0.0044 to 0.0044\n",
      "[56] : train loss 0.006425, val loss drop 0.0044 to 0.0040\n",
      "Epoch    67: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[68] : train loss 0.001855, val loss drop 0.0040 to 0.0038\n",
      "[71] : train loss 0.002482, val loss drop 0.0038 to 0.0036\n",
      "[75] : train loss 0.002395, val loss drop 0.0036 to 0.0032\n",
      "[81] : train loss 0.002109, val loss drop 0.0032 to 0.0029\n",
      "[89] : train loss 0.001350, val loss drop 0.0029 to 0.0028\n",
      "Epoch   100: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[104] : train loss 0.002009, val loss drop 0.0028 to 0.0026\n",
      "Epoch   115: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[125] : train loss 0.001317, val loss drop 0.0026 to 0.0025\n",
      "[128] : train loss 0.001662, val loss drop 0.0025 to 0.0025\n",
      "[133] : train loss 0.000829, val loss drop 0.0025 to 0.0024\n",
      "[135] : train loss 0.001099, val loss drop 0.0024 to 0.0022\n",
      "Epoch   146: reducing learning rate of group 0 to 6.2500e-05.\n",
      "fold 9\n",
      "[1] : train loss 19.843627, val loss drop 10000000.0000 to 5.6746\n",
      "[2] : train loss 0.475862, val loss drop 5.6746 to 0.2537\n",
      "[3] : train loss 0.154182, val loss drop 0.2537 to 0.0718\n",
      "[4] : train loss 0.070618, val loss drop 0.0718 to 0.0599\n",
      "[5] : train loss 0.039194, val loss drop 0.0599 to 0.0303\n",
      "[6] : train loss 0.023642, val loss drop 0.0303 to 0.0222\n",
      "[7] : train loss 0.020127, val loss drop 0.0222 to 0.0176\n",
      "[8] : train loss 0.015801, val loss drop 0.0176 to 0.0166\n",
      "[9] : train loss 0.014934, val loss drop 0.0166 to 0.0162\n",
      "[10] : train loss 0.012928, val loss drop 0.0162 to 0.0131\n",
      "[11] : train loss 0.013280, val loss drop 0.0131 to 0.0108\n",
      "[19] : train loss 0.006026, val loss drop 0.0108 to 0.0072\n",
      "[21] : train loss 0.005586, val loss drop 0.0072 to 0.0059\n",
      "[27] : train loss 0.004748, val loss drop 0.0059 to 0.0057\n",
      "Epoch    38: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[39] : train loss 0.002598, val loss drop 0.0057 to 0.0055\n",
      "[40] : train loss 0.002646, val loss drop 0.0055 to 0.0045\n",
      "[42] : train loss 0.003286, val loss drop 0.0045 to 0.0038\n",
      "[49] : train loss 0.002482, val loss drop 0.0038 to 0.0038\n",
      "Epoch    60: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[65] : train loss 0.002129, val loss drop 0.0038 to 0.0034\n",
      "[74] : train loss 0.002084, val loss drop 0.0034 to 0.0033\n",
      "Epoch    85: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[92] : train loss 0.001925, val loss drop 0.0033 to 0.0032\n",
      "[97] : train loss 0.001495, val loss drop 0.0032 to 0.0027\n",
      "Epoch   108: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch   119: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   130: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch   141: reducing learning rate of group 0 to 7.8125e-06.\n",
      "fold 10\n",
      "[1] : train loss 17.527186, val loss drop 10000000.0000 to 0.8207\n",
      "[2] : train loss 0.427735, val loss drop 0.8207 to 0.3842\n",
      "[3] : train loss 0.151361, val loss drop 0.3842 to 0.2397\n",
      "[4] : train loss 0.067198, val loss drop 0.2397 to 0.0436\n",
      "[5] : train loss 0.037961, val loss drop 0.0436 to 0.0371\n",
      "[6] : train loss 0.021274, val loss drop 0.0371 to 0.0231\n",
      "[8] : train loss 0.016468, val loss drop 0.0231 to 0.0126\n",
      "[9] : train loss 0.014738, val loss drop 0.0126 to 0.0110\n",
      "[13] : train loss 0.012126, val loss drop 0.0110 to 0.0107\n",
      "[17] : train loss 0.006802, val loss drop 0.0107 to 0.0088\n",
      "[20] : train loss 0.005393, val loss drop 0.0088 to 0.0081\n",
      "[22] : train loss 0.004608, val loss drop 0.0081 to 0.0072\n",
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[36] : train loss 0.002840, val loss drop 0.0072 to 0.0067\n",
      "[37] : train loss 0.002619, val loss drop 0.0067 to 0.0056\n",
      "[38] : train loss 0.002984, val loss drop 0.0056 to 0.0055\n",
      "[40] : train loss 0.004612, val loss drop 0.0055 to 0.0049\n",
      "[51] : train loss 0.003042, val loss drop 0.0049 to 0.0047\n",
      "Epoch    62: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[65] : train loss 0.002991, val loss drop 0.0047 to 0.0043\n",
      "[69] : train loss 0.001828, val loss drop 0.0043 to 0.0041\n",
      "Epoch    80: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[87] : train loss 0.001579, val loss drop 0.0041 to 0.0039\n",
      "[94] : train loss 0.001226, val loss drop 0.0039 to 0.0036\n",
      "[105] : train loss 0.001206, val loss drop 0.0036 to 0.0035\n",
      "Epoch   116: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[124] : train loss 0.001217, val loss drop 0.0035 to 0.0034\n",
      "Epoch   135: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[143] : train loss 0.001784, val loss drop 0.0034 to 0.0033\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 49.127627, val loss drop 10000000.0000 to 21.4002\n",
      "[2] : train loss 4.234449, val loss drop 21.4002 to 16.2873\n",
      "[4] : train loss 0.921993, val loss drop 16.2873 to 1.3542\n",
      "[5] : train loss 0.410334, val loss drop 1.3542 to 0.3893\n",
      "[6] : train loss 0.289676, val loss drop 0.3893 to 0.3603\n",
      "[9] : train loss 0.823641, val loss drop 0.3603 to 0.1885\n",
      "[18] : train loss 0.240560, val loss drop 0.1885 to 0.0793\n",
      "[19] : train loss 0.121356, val loss drop 0.0793 to 0.0779\n",
      "[24] : train loss 0.214111, val loss drop 0.0779 to 0.0659\n",
      "[27] : train loss 0.288714, val loss drop 0.0659 to 0.0553\n",
      "Epoch    38: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[42] : train loss 0.112968, val loss drop 0.0553 to 0.0256\n",
      "[51] : train loss 0.149049, val loss drop 0.0256 to 0.0215\n",
      "[53] : train loss 0.087610, val loss drop 0.0215 to 0.0200\n",
      "[56] : train loss 0.037824, val loss drop 0.0200 to 0.0147\n",
      "Epoch    67: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[78] : train loss 0.081208, val loss drop 0.0147 to 0.0090\n",
      "Epoch    89: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch   100: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[108] : train loss 0.059874, val loss drop 0.0090 to 0.0089\n",
      "Epoch   119: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[127] : train loss 0.048922, val loss drop 0.0089 to 0.0078\n",
      "[128] : train loss 0.063284, val loss drop 0.0078 to 0.0077\n",
      "Epoch   139: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch   150: reducing learning rate of group 0 to 7.8125e-06.\n",
      "fold 2\n",
      "[1] : train loss 31.074008, val loss drop 10000000.0000 to 43.9409\n",
      "[2] : train loss 2.972407, val loss drop 43.9409 to 28.7111\n",
      "[3] : train loss 0.910436, val loss drop 28.7111 to 12.8629\n",
      "[4] : train loss 0.432785, val loss drop 12.8629 to 1.4737\n",
      "[5] : train loss 0.387614, val loss drop 1.4737 to 0.2343\n",
      "[7] : train loss 0.263227, val loss drop 0.2343 to 0.1245\n",
      "[14] : train loss 0.153790, val loss drop 0.1245 to 0.0720\n",
      "[18] : train loss 0.269125, val loss drop 0.0720 to 0.0715\n",
      "[21] : train loss 0.202855, val loss drop 0.0715 to 0.0612\n",
      "[28] : train loss 0.258476, val loss drop 0.0612 to 0.0401\n",
      "[34] : train loss 0.188550, val loss drop 0.0401 to 0.0265\n",
      "Epoch    45: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[52] : train loss 0.086972, val loss drop 0.0265 to 0.0251\n",
      "Epoch    63: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[67] : train loss 0.078925, val loss drop 0.0251 to 0.0181\n",
      "[68] : train loss 0.049896, val loss drop 0.0181 to 0.0160\n",
      "[69] : train loss 0.040128, val loss drop 0.0160 to 0.0152\n",
      "[72] : train loss 0.076360, val loss drop 0.0152 to 0.0077\n",
      "Epoch    83: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[90] : train loss 0.086018, val loss drop 0.0077 to 0.0075\n",
      "[93] : train loss 0.058231, val loss drop 0.0075 to 0.0066\n",
      "[95] : train loss 0.030860, val loss drop 0.0066 to 0.0064\n",
      "Epoch   106: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch   117: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[123] : train loss 0.065118, val loss drop 0.0064 to 0.0061\n",
      "[127] : train loss 0.018371, val loss drop 0.0061 to 0.0054\n",
      "Epoch   138: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[139] : train loss 0.050114, val loss drop 0.0054 to 0.0054\n",
      "[150] : train loss 0.059028, val loss drop 0.0054 to 0.0049\n",
      "fold 3\n",
      "[1] : train loss 34.657982, val loss drop 10000000.0000 to 69.4994\n",
      "[2] : train loss 2.748221, val loss drop 69.4994 to 21.3694\n",
      "[3] : train loss 1.027697, val loss drop 21.3694 to 19.6776\n",
      "[4] : train loss 0.530329, val loss drop 19.6776 to 4.4284\n",
      "[5] : train loss 0.598156, val loss drop 4.4284 to 1.3238\n",
      "[6] : train loss 0.259219, val loss drop 1.3238 to 0.3652\n",
      "[7] : train loss 0.176116, val loss drop 0.3652 to 0.1765\n",
      "[10] : train loss 0.216787, val loss drop 0.1765 to 0.1518\n",
      "[14] : train loss 0.383450, val loss drop 0.1518 to 0.0466\n",
      "[18] : train loss 0.130741, val loss drop 0.0466 to 0.0288\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[40] : train loss 0.060674, val loss drop 0.0288 to 0.0287\n",
      "[49] : train loss 0.184868, val loss drop 0.0287 to 0.0084\n",
      "Epoch    60: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[72] : train loss 0.031258, val loss drop 0.0084 to 0.0083\n",
      "[74] : train loss 0.043004, val loss drop 0.0083 to 0.0070\n",
      "[80] : train loss 0.042644, val loss drop 0.0070 to 0.0061\n",
      "Epoch    91: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[92] : train loss 0.046994, val loss drop 0.0061 to 0.0058\n",
      "[95] : train loss 0.055868, val loss drop 0.0058 to 0.0052\n",
      "Epoch   106: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   117: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[118] : train loss 0.039821, val loss drop 0.0052 to 0.0046\n",
      "[124] : train loss 0.050131, val loss drop 0.0046 to 0.0046\n",
      "[131] : train loss 0.042394, val loss drop 0.0046 to 0.0045\n",
      "[133] : train loss 0.050463, val loss drop 0.0045 to 0.0040\n",
      "Epoch   144: reducing learning rate of group 0 to 7.8125e-06.\n",
      "fold 4\n",
      "[1] : train loss 46.274098, val loss drop 10000000.0000 to 8.2684\n",
      "[4] : train loss 1.021253, val loss drop 8.2684 to 6.7493\n",
      "[5] : train loss 0.520000, val loss drop 6.7493 to 0.9526\n",
      "[6] : train loss 0.377478, val loss drop 0.9526 to 0.3377\n",
      "[7] : train loss 0.388536, val loss drop 0.3377 to 0.3301\n",
      "[11] : train loss 0.337379, val loss drop 0.3301 to 0.1460\n",
      "[16] : train loss 0.224003, val loss drop 0.1460 to 0.1024\n",
      "[24] : train loss 0.206096, val loss drop 0.1024 to 0.0961\n",
      "[26] : train loss 0.242741, val loss drop 0.0961 to 0.0644\n",
      "[30] : train loss 0.138923, val loss drop 0.0644 to 0.0591\n",
      "[35] : train loss 0.116307, val loss drop 0.0591 to 0.0555\n",
      "[41] : train loss 0.117573, val loss drop 0.0555 to 0.0407\n",
      "Epoch    52: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[57] : train loss 0.083499, val loss drop 0.0407 to 0.0172\n",
      "[58] : train loss 0.074078, val loss drop 0.0172 to 0.0096\n",
      "[69] : train loss 0.071999, val loss drop 0.0096 to 0.0086\n",
      "Epoch    80: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[91] : train loss 0.049247, val loss drop 0.0086 to 0.0075\n",
      "[97] : train loss 0.055739, val loss drop 0.0075 to 0.0068\n",
      "[106] : train loss 0.036133, val loss drop 0.0068 to 0.0046\n",
      "Epoch   117: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch   128: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[133] : train loss 0.057991, val loss drop 0.0046 to 0.0040\n",
      "Epoch   144: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[148] : train loss 0.027364, val loss drop 0.0040 to 0.0039\n",
      "fold 5\n",
      "[1] : train loss 50.248665, val loss drop 10000000.0000 to 10.0856\n",
      "[2] : train loss 6.287017, val loss drop 10.0856 to 8.6833\n",
      "[3] : train loss 1.912208, val loss drop 8.6833 to 8.2604\n",
      "[4] : train loss 0.951312, val loss drop 8.2604 to 0.6747\n",
      "[7] : train loss 0.608967, val loss drop 0.6747 to 0.2109\n",
      "[9] : train loss 0.277451, val loss drop 0.2109 to 0.1440\n",
      "[13] : train loss 0.409773, val loss drop 0.1440 to 0.0908\n",
      "[21] : train loss 0.107512, val loss drop 0.0908 to 0.0690\n",
      "[25] : train loss 0.358923, val loss drop 0.0690 to 0.0390\n",
      "[28] : train loss 0.166623, val loss drop 0.0390 to 0.0208\n",
      "Epoch    39: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[45] : train loss 0.175102, val loss drop 0.0208 to 0.0152\n",
      "Epoch    56: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[60] : train loss 0.077080, val loss drop 0.0152 to 0.0117\n",
      "[63] : train loss 0.036971, val loss drop 0.0117 to 0.0080\n",
      "Epoch    74: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    85: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[93] : train loss 0.025034, val loss drop 0.0080 to 0.0076\n",
      "Epoch   104: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[112] : train loss 0.031246, val loss drop 0.0076 to 0.0076\n",
      "[116] : train loss 0.073150, val loss drop 0.0076 to 0.0075\n",
      "[117] : train loss 0.080607, val loss drop 0.0075 to 0.0073\n",
      "[120] : train loss 0.071049, val loss drop 0.0073 to 0.0068\n",
      "[124] : train loss 0.023788, val loss drop 0.0068 to 0.0065\n",
      "[134] : train loss 0.015994, val loss drop 0.0065 to 0.0064\n",
      "Epoch   145: reducing learning rate of group 0 to 1.5625e-05.\n",
      "fold 6\n",
      "[1] : train loss 35.993766, val loss drop 10000000.0000 to 58.9024\n",
      "[2] : train loss 2.677561, val loss drop 58.9024 to 53.8467\n",
      "[3] : train loss 0.862024, val loss drop 53.8467 to 16.3388\n",
      "[4] : train loss 0.493239, val loss drop 16.3388 to 4.0024\n",
      "[5] : train loss 0.339882, val loss drop 4.0024 to 0.9212\n",
      "[6] : train loss 0.317735, val loss drop 0.9212 to 0.1796\n",
      "[9] : train loss 0.305635, val loss drop 0.1796 to 0.1333\n",
      "[12] : train loss 0.190743, val loss drop 0.1333 to 0.1038\n",
      "[20] : train loss 0.191458, val loss drop 0.1038 to 0.0621\n",
      "[26] : train loss 0.134801, val loss drop 0.0621 to 0.0528\n",
      "[34] : train loss 0.168130, val loss drop 0.0528 to 0.0294\n",
      "Epoch    45: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[46] : train loss 0.264722, val loss drop 0.0294 to 0.0287\n",
      "[48] : train loss 0.059885, val loss drop 0.0287 to 0.0112\n",
      "Epoch    59: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[63] : train loss 0.090732, val loss drop 0.0112 to 0.0101\n",
      "[71] : train loss 0.036549, val loss drop 0.0101 to 0.0081\n",
      "[81] : train loss 0.041106, val loss drop 0.0081 to 0.0065\n",
      "[83] : train loss 0.056688, val loss drop 0.0065 to 0.0058\n",
      "Epoch    94: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[101] : train loss 0.068015, val loss drop 0.0058 to 0.0047\n",
      "Epoch   112: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[119] : train loss 0.050852, val loss drop 0.0047 to 0.0046\n",
      "Epoch   130: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[131] : train loss 0.030423, val loss drop 0.0046 to 0.0045\n",
      "[136] : train loss 0.038722, val loss drop 0.0045 to 0.0038\n",
      "Epoch   147: reducing learning rate of group 0 to 1.5625e-05.\n",
      "fold 7\n",
      "[1] : train loss 52.332977, val loss drop 10000000.0000 to 16.7676\n",
      "[2] : train loss 7.174505, val loss drop 16.7676 to 3.0088\n",
      "[4] : train loss 0.927978, val loss drop 3.0088 to 2.9450\n",
      "[5] : train loss 0.555536, val loss drop 2.9450 to 1.1458\n",
      "[6] : train loss 0.390887, val loss drop 1.1458 to 0.2874\n",
      "[7] : train loss 0.340774, val loss drop 0.2874 to 0.1859\n",
      "[12] : train loss 0.289129, val loss drop 0.1859 to 0.1208\n",
      "[16] : train loss 0.186605, val loss drop 0.1208 to 0.0958\n",
      "[22] : train loss 0.132225, val loss drop 0.0958 to 0.0699\n",
      "[29] : train loss 0.181418, val loss drop 0.0699 to 0.0488\n",
      "Epoch    40: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[42] : train loss 0.070520, val loss drop 0.0488 to 0.0394\n",
      "[44] : train loss 0.086603, val loss drop 0.0394 to 0.0228\n",
      "Epoch    55: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[58] : train loss 0.100605, val loss drop 0.0228 to 0.0158\n",
      "[60] : train loss 0.034534, val loss drop 0.0158 to 0.0117\n",
      "Epoch    71: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[74] : train loss 0.043577, val loss drop 0.0117 to 0.0090\n",
      "[76] : train loss 0.032781, val loss drop 0.0090 to 0.0070\n",
      "Epoch    87: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch    98: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch   109: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch   120: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[125] : train loss 0.037875, val loss drop 0.0070 to 0.0066\n",
      "Epoch   136: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch   147: reducing learning rate of group 0 to 1.9531e-06.\n",
      "fold 8\n",
      "[1] : train loss 41.016978, val loss drop 10000000.0000 to 14.8706\n",
      "[4] : train loss 0.607033, val loss drop 14.8706 to 3.5525\n",
      "[5] : train loss 0.396478, val loss drop 3.5525 to 0.8229\n",
      "[6] : train loss 0.384311, val loss drop 0.8229 to 0.5816\n",
      "[7] : train loss 0.349000, val loss drop 0.5816 to 0.1770\n",
      "[15] : train loss 0.176589, val loss drop 0.1770 to 0.1351\n",
      "[16] : train loss 0.196800, val loss drop 0.1351 to 0.1197\n",
      "[17] : train loss 0.326332, val loss drop 0.1197 to 0.1182\n",
      "[19] : train loss 0.099776, val loss drop 0.1182 to 0.0899\n",
      "[24] : train loss 0.043960, val loss drop 0.0899 to 0.0750\n",
      "[26] : train loss 0.082194, val loss drop 0.0750 to 0.0565\n",
      "[27] : train loss 0.286243, val loss drop 0.0565 to 0.0540\n",
      "[28] : train loss 0.250434, val loss drop 0.0540 to 0.0524\n",
      "[30] : train loss 0.070217, val loss drop 0.0524 to 0.0298\n",
      "Epoch    41: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[47] : train loss 0.131691, val loss drop 0.0298 to 0.0281\n",
      "[48] : train loss 0.064830, val loss drop 0.0281 to 0.0165\n",
      "Epoch    59: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[66] : train loss 0.080879, val loss drop 0.0165 to 0.0164\n",
      "[68] : train loss 0.071689, val loss drop 0.0164 to 0.0103\n",
      "[76] : train loss 0.135649, val loss drop 0.0103 to 0.0089\n",
      "[80] : train loss 0.092489, val loss drop 0.0089 to 0.0074\n",
      "Epoch    91: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch   102: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[106] : train loss 0.047170, val loss drop 0.0074 to 0.0071\n",
      "[114] : train loss 0.049689, val loss drop 0.0071 to 0.0070\n",
      "[115] : train loss 0.046467, val loss drop 0.0070 to 0.0070\n",
      "[116] : train loss 0.039329, val loss drop 0.0070 to 0.0069\n",
      "[118] : train loss 0.038966, val loss drop 0.0069 to 0.0062\n",
      "[124] : train loss 0.050792, val loss drop 0.0062 to 0.0058\n",
      "Epoch   135: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[146] : train loss 0.029781, val loss drop 0.0058 to 0.0058\n",
      "fold 9\n",
      "[1] : train loss 44.124807, val loss drop 10000000.0000 to 21.8752\n",
      "[3] : train loss 1.908211, val loss drop 21.8752 to 5.9789\n",
      "[4] : train loss 0.899479, val loss drop 5.9789 to 1.8225\n",
      "[5] : train loss 0.605590, val loss drop 1.8225 to 1.7759\n",
      "[6] : train loss 0.417877, val loss drop 1.7759 to 0.8116\n",
      "[7] : train loss 0.325073, val loss drop 0.8116 to 0.1837\n",
      "[9] : train loss 0.182194, val loss drop 0.1837 to 0.1016\n",
      "[11] : train loss 0.225735, val loss drop 0.1016 to 0.0689\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[23] : train loss 0.102277, val loss drop 0.0689 to 0.0360\n",
      "[34] : train loss 0.112333, val loss drop 0.0360 to 0.0359\n",
      "[41] : train loss 0.169840, val loss drop 0.0359 to 0.0239\n",
      "[43] : train loss 0.114778, val loss drop 0.0239 to 0.0222\n",
      "[50] : train loss 0.098183, val loss drop 0.0222 to 0.0160\n",
      "Epoch    61: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[71] : train loss 0.054628, val loss drop 0.0160 to 0.0100\n",
      "Epoch    82: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[87] : train loss 0.060669, val loss drop 0.0100 to 0.0087\n",
      "Epoch    98: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch   109: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[110] : train loss 0.052243, val loss drop 0.0087 to 0.0074\n",
      "[113] : train loss 0.051639, val loss drop 0.0074 to 0.0063\n",
      "Epoch   124: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch   135: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[138] : train loss 0.044848, val loss drop 0.0063 to 0.0058\n",
      "Epoch   149: reducing learning rate of group 0 to 3.9063e-06.\n",
      "fold 10\n",
      "[1] : train loss 28.508054, val loss drop 10000000.0000 to 31.1391\n",
      "[2] : train loss 1.703418, val loss drop 31.1391 to 5.2877\n",
      "[3] : train loss 0.686672, val loss drop 5.2877 to 1.7853\n",
      "[4] : train loss 0.415947, val loss drop 1.7853 to 1.3777\n",
      "[5] : train loss 0.382849, val loss drop 1.3777 to 0.1320\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[17] : train loss 0.249295, val loss drop 0.1320 to 0.0508\n",
      "[20] : train loss 0.132518, val loss drop 0.0508 to 0.0160\n",
      "Epoch    31: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[34] : train loss 0.057166, val loss drop 0.0160 to 0.0145\n",
      "[38] : train loss 0.109874, val loss drop 0.0145 to 0.0140\n",
      "Epoch    49: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[56] : train loss 0.253442, val loss drop 0.0140 to 0.0121\n",
      "[58] : train loss 0.041724, val loss drop 0.0121 to 0.0109\n",
      "Epoch    69: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[70] : train loss 0.129417, val loss drop 0.0109 to 0.0102\n",
      "[73] : train loss 0.054610, val loss drop 0.0102 to 0.0101\n",
      "[74] : train loss 0.112676, val loss drop 0.0101 to 0.0086\n",
      "[77] : train loss 0.056129, val loss drop 0.0086 to 0.0076\n",
      "Epoch    88: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch    99: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[109] : train loss 0.040679, val loss drop 0.0076 to 0.0072\n",
      "[112] : train loss 0.136503, val loss drop 0.0072 to 0.0070\n",
      "[118] : train loss 0.074514, val loss drop 0.0070 to 0.0068\n",
      "Epoch   129: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch   140: reducing learning rate of group 0 to 3.9063e-06.\n",
      "[143] : train loss 0.052714, val loss drop 0.0068 to 0.0068\n",
      "[144] : train loss 0.047421, val loss drop 0.0068 to 0.0068\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 2, axis = 1)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}\n",
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xy': 0.00013199864487914719,\n",
       " 'v': 0.001799068060519602,\n",
       " 'm': 0.005548691534818471}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_model # leaky relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "model = custom_cnn(n_features, len(name))\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "\n",
    "# define model\n",
    "model = custom_cnn(n_features, len(name))\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 2, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "\n",
    "# define model\n",
    "model = custom_cnn(n_features, len(name))\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-265.198059</td>\n",
       "      <td>-47.421284</td>\n",
       "      <td>112.415390</td>\n",
       "      <td>0.458206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>312.479004</td>\n",
       "      <td>-288.096252</td>\n",
       "      <td>89.512192</td>\n",
       "      <td>0.463330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-239.102463</td>\n",
       "      <td>134.976456</td>\n",
       "      <td>29.504105</td>\n",
       "      <td>0.379241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>171.477158</td>\n",
       "      <td>276.375397</td>\n",
       "      <td>26.742666</td>\n",
       "      <td>0.405207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-161.685226</td>\n",
       "      <td>197.064529</td>\n",
       "      <td>130.685638</td>\n",
       "      <td>0.434561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -265.198059  -47.421284  112.415390  0.458206\n",
       "1  2801  312.479004 -288.096252   89.512192  0.463330\n",
       "2  2802 -239.102463  134.976456   29.504105  0.379241\n",
       "3  2803  171.477158  276.375397   26.742666  0.405207\n",
       "4  2804 -161.685226  197.064529  130.685638  0.434561"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, '{}.csv'.format(save_path.split('/')[-1])), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E1_loss(y_pred, y_true):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: distance error normalized with 2e+04\n",
    "    '''\n",
    "    \n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))\n",
    "\n",
    "def total_loss(y_pred, y_true):\n",
    "    xy_t, xy_p = y_true[:,:2], y_pred[:,:2]\n",
    "    mv_t, mv_p = y_true[:,2:], y_pred[:,2:]\n",
    "    \n",
    "    e1 = torch.mean(torch.mean((xy_t - xy_p) ** 2, axis = 1)) / 2e+04\n",
    "    e2 = torch.mean(torch.mean((mv_t - mv_p) ** 2 / (mv_t + 1e-06), axis = 1))\n",
    "    \n",
    "    return e1 + e2\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_data, loss_func):\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convolution : feature extractor 모듈\n",
    "- classifier : linear 모듈\n",
    "- convolution에서 output shape 계산하는 함수 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self, filter_size, out_count = 4):\n",
    "        super(cnn, self).__init__()\n",
    "        self.conv1 = conv_bn(1, 16, filter_size)\n",
    "        self.conv2 = conv_bn(16, 32, filter_size)\n",
    "        self.conv3 = conv_bn(32, 64, filter_size)\n",
    "        self.conv4 = conv_bn(64, 128, filter_size)\n",
    "        self.conv5 = conv_bn(128, 256, filter_size)\n",
    "        self.conv6 = conv_bn(256, 512, filter_size) \n",
    "        \n",
    "        self.linear1 = nn.Linear(512*3*5, 64)\n",
    "        #self.linear2 = nn.Linear(64, 32)\n",
    "        #self.linear3 = nn.Linear(32, 16)\n",
    "        self.linear2= nn.Linear(64, out_count)\n",
    "\n",
    "        self.act = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        \n",
    "        x = x.flatten(start_dim = 1)\n",
    "        \n",
    "        x = self.act(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        #x = self.act(self.linear2(x))\n",
    "       # x = self.act(self.linear3(x))\n",
    "        #x = self.linear4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 200\n",
    "base_lr = 0.001\n",
    "name = 'XYMV'\n",
    "root_path = './tmp'\n",
    "initialize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 4)\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'D:/datasets/KAERI_dataset'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "train_target = train_t[list(name)].values#.astype(np.float32)\n",
    "print(train_target.shape)\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn((3,1), len(name))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "criterion = E1_loss\n",
    "model = model.cuda()\n",
    "if initialize:\n",
    "    model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 373, 5]              64\n",
      "               ELU-2           [-1, 16, 373, 5]               0\n",
      "       BatchNorm2d-3           [-1, 16, 373, 5]              32\n",
      "         MaxPool2d-4           [-1, 16, 186, 5]               0\n",
      "           conv_bn-5           [-1, 16, 186, 5]               0\n",
      "            Conv2d-6           [-1, 32, 184, 5]           1,568\n",
      "               ELU-7           [-1, 32, 184, 5]               0\n",
      "       BatchNorm2d-8           [-1, 32, 184, 5]              64\n",
      "         MaxPool2d-9            [-1, 32, 92, 5]               0\n",
      "          conv_bn-10            [-1, 32, 92, 5]               0\n",
      "           Conv2d-11            [-1, 64, 90, 5]           6,208\n",
      "              ELU-12            [-1, 64, 90, 5]               0\n",
      "      BatchNorm2d-13            [-1, 64, 90, 5]             128\n",
      "        MaxPool2d-14            [-1, 64, 45, 5]               0\n",
      "          conv_bn-15            [-1, 64, 45, 5]               0\n",
      "           Conv2d-16           [-1, 128, 43, 5]          24,704\n",
      "              ELU-17           [-1, 128, 43, 5]               0\n",
      "      BatchNorm2d-18           [-1, 128, 43, 5]             256\n",
      "        MaxPool2d-19           [-1, 128, 21, 5]               0\n",
      "          conv_bn-20           [-1, 128, 21, 5]               0\n",
      "           Conv2d-21           [-1, 256, 19, 5]          98,560\n",
      "              ELU-22           [-1, 256, 19, 5]               0\n",
      "      BatchNorm2d-23           [-1, 256, 19, 5]             512\n",
      "        MaxPool2d-24            [-1, 256, 9, 5]               0\n",
      "          conv_bn-25            [-1, 256, 9, 5]               0\n",
      "           Conv2d-26            [-1, 512, 7, 5]         393,728\n",
      "              ELU-27            [-1, 512, 7, 5]               0\n",
      "      BatchNorm2d-28            [-1, 512, 7, 5]           1,024\n",
      "        MaxPool2d-29            [-1, 512, 3, 5]               0\n",
      "          conv_bn-30            [-1, 512, 3, 5]               0\n",
      "           Linear-31                   [-1, 64]         491,584\n",
      "              ELU-32                   [-1, 64]               0\n",
      "           Linear-33                    [-1, 4]             260\n",
      "================================================================\n",
      "Total params: 1,018,692\n",
      "Trainable params: 1,018,692\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.78\n",
      "Params size (MB): 3.89\n",
      "Estimated Total Size (MB): 8.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 375, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx, valx, trainy, valy = train_test_split(train_f, train_target, test_size = 0.2, shuffle = True, random_state = 38)\n",
    "\n",
    "train_dataset = dfDataset(trainx.astype(np.float32), trainy)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 256, shuffle = True)\n",
    "val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 256, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] : train loss 1.653217, val loss drop 10000000.0000 to 1.6486\n",
      "[2] : train loss 1.077137, val loss drop 1.6486 to 1.0420\n",
      "[3] : train loss 0.497564, val loss drop 1.0420 to 0.4164\n",
      "[4] : train loss 0.166245, val loss drop 0.4164 to 0.1292\n",
      "[5] : train loss 0.056950, val loss drop 0.1292 to 0.0557\n",
      "[6] : train loss 0.027545, val loss drop 0.0557 to 0.0369\n",
      "[7] : train loss 0.014687, val loss drop 0.0369 to 0.0198\n",
      "[8] : train loss 0.0100, val loss 0.0254, not drop\n",
      "[9] : train loss 0.006900, val loss drop 0.0198 to 0.0071\n",
      "[10] : train loss 0.005200, val loss drop 0.0071 to 0.0066\n",
      "[11] : train loss 0.004370, val loss drop 0.0066 to 0.0057\n",
      "[12] : train loss 0.0030, val loss 0.0117, not drop\n",
      "[13] : train loss 0.002937, val loss drop 0.0057 to 0.0043\n",
      "[14] : train loss 0.0031, val loss 0.0129, not drop\n",
      "[15] : train loss 0.002740, val loss drop 0.0043 to 0.0029\n",
      "[16] : train loss 0.0026, val loss 0.0045, not drop\n",
      "[17] : train loss 0.0018, val loss 0.0037, not drop\n",
      "[18] : train loss 0.0018, val loss 0.0081, not drop\n",
      "[19] : train loss 0.0013, val loss 0.0054, not drop\n",
      "[20] : train loss 0.001401, val loss drop 0.0029 to 0.0023\n",
      "[21] : train loss 0.0014, val loss 0.0053, not drop\n",
      "[22] : train loss 0.0015, val loss 0.0063, not drop\n",
      "[23] : train loss 0.0014, val loss 0.0058, not drop\n",
      "[24] : train loss 0.0014, val loss 0.0035, not drop\n",
      "[25] : train loss 0.0014, val loss 0.0084, not drop\n",
      "[26] : train loss 0.001243, val loss drop 0.0023 to 0.0016\n",
      "[27] : train loss 0.0012, val loss 0.0038, not drop\n",
      "[28] : train loss 0.0010, val loss 0.0022, not drop\n",
      "[29] : train loss 0.0013, val loss 0.0030, not drop\n",
      "[30] : train loss 0.0012, val loss 0.0037, not drop\n",
      "[31] : train loss 0.0015, val loss 0.0022, not drop\n",
      "[32] : train loss 0.0010, val loss 0.0072, not drop\n",
      "[33] : train loss 0.001121, val loss drop 0.0016 to 0.0015\n",
      "[34] : train loss 0.0012, val loss 0.0016, not drop\n",
      "[35] : train loss 0.0012, val loss 0.0040, not drop\n",
      "[36] : train loss 0.0010, val loss 0.0018, not drop\n",
      "[37] : train loss 0.0007, val loss 0.0047, not drop\n",
      "[38] : train loss 0.0007, val loss 0.0019, not drop\n",
      "[39] : train loss 0.0011, val loss 0.0034, not drop\n",
      "[40] : train loss 0.0010, val loss 0.0016, not drop\n",
      "[41] : train loss 0.0009, val loss 0.0018, not drop\n",
      "[42] : train loss 0.0006, val loss 0.0051, not drop\n",
      "[43] : train loss 0.001083, val loss drop 0.0015 to 0.0009\n",
      "[44] : train loss 0.001296, val loss drop 0.0009 to 0.0008\n",
      "[45] : train loss 0.0007, val loss 0.0063, not drop\n",
      "[46] : train loss 0.0006, val loss 0.0014, not drop\n",
      "[47] : train loss 0.0008, val loss 0.0012, not drop\n",
      "[48] : train loss 0.000566, val loss drop 0.0008 to 0.0005\n",
      "[49] : train loss 0.000584, val loss drop 0.0005 to 0.0005\n",
      "[50] : train loss 0.0006, val loss 0.0018, not drop\n",
      "[51] : train loss 0.0006, val loss 0.0043, not drop\n",
      "[52] : train loss 0.0008, val loss 0.0027, not drop\n",
      "[53] : train loss 0.0009, val loss 0.0019, not drop\n",
      "[54] : train loss 0.0009, val loss 0.0035, not drop\n",
      "[55] : train loss 0.0009, val loss 0.0022, not drop\n",
      "[56] : train loss 0.0009, val loss 0.0015, not drop\n",
      "[57] : train loss 0.0009, val loss 0.0011, not drop\n",
      "[58] : train loss 0.0010, val loss 0.0010, not drop\n",
      "[59] : train loss 0.0009, val loss 0.0012, not drop\n",
      "[60] : train loss 0.0009, val loss 0.0017, not drop\n",
      "[61] : train loss 0.0007, val loss 0.0034, not drop\n",
      "[62] : train loss 0.0011, val loss 0.0050, not drop\n",
      "[63] : train loss 0.0009, val loss 0.0055, not drop\n",
      "[64] : train loss 0.0007, val loss 0.0037, not drop\n",
      "[65] : train loss 0.0011, val loss 0.0016, not drop\n",
      "[66] : train loss 0.0011, val loss 0.0011, not drop\n",
      "[67] : train loss 0.0007, val loss 0.0016, not drop\n",
      "[68] : train loss 0.0011, val loss 0.0024, not drop\n",
      "[69] : train loss 0.0012, val loss 0.0083, not drop\n",
      "[70] : train loss 0.0009, val loss 0.0009, not drop\n",
      "[71] : train loss 0.0009, val loss 0.0012, not drop\n",
      "[72] : train loss 0.0007, val loss 0.0007, not drop\n",
      "[73] : train loss 0.0005, val loss 0.0034, not drop\n",
      "[74] : train loss 0.0007, val loss 0.0019, not drop\n",
      "[75] : train loss 0.0008, val loss 0.0006, not drop\n",
      "[76] : train loss 0.0005, val loss 0.0005, not drop\n",
      "[77] : train loss 0.0005, val loss 0.0008, not drop\n",
      "[78] : train loss 0.0006, val loss 0.0021, not drop\n",
      "[79] : train loss 0.0006, val loss 0.0166, not drop\n",
      "[80] : train loss 0.0006, val loss 0.0016, not drop\n",
      "[81] : train loss 0.0006, val loss 0.0014, not drop\n",
      "[82] : train loss 0.0009, val loss 0.0011, not drop\n",
      "[83] : train loss 0.0008, val loss 0.0018, not drop\n",
      "[84] : train loss 0.0007, val loss 0.0008, not drop\n",
      "[85] : train loss 0.0006, val loss 0.0005, not drop\n",
      "[86] : train loss 0.0005, val loss 0.0005, not drop\n",
      "[87] : train loss 0.0006, val loss 0.0005, not drop\n",
      "[88] : train loss 0.0004, val loss 0.0005, not drop\n",
      "[89] : train loss 0.0006, val loss 0.0008, not drop\n",
      "[90] : train loss 0.0004, val loss 0.0008, not drop\n",
      "[91] : train loss 0.0005, val loss 0.0009, not drop\n",
      "[92] : train loss 0.0005, val loss 0.0043, not drop\n",
      "[93] : train loss 0.0007, val loss 0.0034, not drop\n",
      "[94] : train loss 0.0009, val loss 0.0008, not drop\n",
      "[95] : train loss 0.0008, val loss 0.0020, not drop\n",
      "[96] : train loss 0.0009, val loss 0.0029, not drop\n",
      "[97] : train loss 0.0010, val loss 0.0013, not drop\n",
      "[98] : train loss 0.0007, val loss 0.0016, not drop\n",
      "[99] : train loss 0.0007, val loss 0.0033, not drop\n",
      "[100] : train loss 0.0008, val loss 0.0028, not drop\n",
      "[101] : train loss 0.0006, val loss 0.0012, not drop\n",
      "[102] : train loss 0.0004, val loss 0.0010, not drop\n",
      "[103] : train loss 0.0003, val loss 0.0015, not drop\n",
      "[104] : train loss 0.0011, val loss 0.0020, not drop\n",
      "[105] : train loss 0.0008, val loss 0.0025, not drop\n",
      "[106] : train loss 0.0008, val loss 0.0007, not drop\n",
      "[107] : train loss 0.0006, val loss 0.0014, not drop\n",
      "[108] : train loss 0.0005, val loss 0.0010, not drop\n",
      "[109] : train loss 0.0004, val loss 0.0024, not drop\n",
      "[110] : train loss 0.0003, val loss 0.0006, not drop\n",
      "[111] : train loss 0.0004, val loss 0.0011, not drop\n",
      "[112] : train loss 0.0005, val loss 0.0022, not drop\n",
      "[113] : train loss 0.0007, val loss 0.0023, not drop\n",
      "[114] : train loss 0.0007, val loss 0.0034, not drop\n",
      "[115] : train loss 0.0007, val loss 0.0016, not drop\n",
      "[116] : train loss 0.0006, val loss 0.0013, not drop\n",
      "[117] : train loss 0.0005, val loss 0.0009, not drop\n",
      "[118] : train loss 0.0005, val loss 0.0053, not drop\n",
      "[119] : train loss 0.0004, val loss 0.0005, not drop\n",
      "[120] : train loss 0.0004, val loss 0.0009, not drop\n",
      "[121] : train loss 0.0003, val loss 0.0009, not drop\n",
      "[122] : train loss 0.0004, val loss 0.0009, not drop\n",
      "[123] : train loss 0.0004, val loss 0.0050, not drop\n",
      "[124] : train loss 0.0003, val loss 0.0008, not drop\n",
      "[125] : train loss 0.000563, val loss drop 0.0005 to 0.0003\n",
      "[126] : train loss 0.0004, val loss 0.0012, not drop\n",
      "[127] : train loss 0.0004, val loss 0.0005, not drop\n",
      "[128] : train loss 0.0004, val loss 0.0003, not drop\n",
      "[129] : train loss 0.0006, val loss 0.0017, not drop\n",
      "[130] : train loss 0.0008, val loss 0.0020, not drop\n",
      "[131] : train loss 0.0014, val loss 0.0035, not drop\n",
      "[132] : train loss 0.0011, val loss 0.0008, not drop\n",
      "[133] : train loss 0.0009, val loss 0.0007, not drop\n",
      "[134] : train loss 0.0010, val loss 0.0017, not drop\n",
      "[135] : train loss 0.0010, val loss 0.0021, not drop\n",
      "[136] : train loss 0.0011, val loss 0.0015, not drop\n",
      "[137] : train loss 0.0011, val loss 0.0036, not drop\n",
      "[138] : train loss 0.0007, val loss 0.0014, not drop\n",
      "[139] : train loss 0.0007, val loss 0.0009, not drop\n",
      "[140] : train loss 0.0006, val loss 0.0036, not drop\n",
      "[141] : train loss 0.0005, val loss 0.0015, not drop\n",
      "[142] : train loss 0.0009, val loss 0.0028, not drop\n",
      "[143] : train loss 0.0006, val loss 0.0015, not drop\n",
      "[144] : train loss 0.0004, val loss 0.0017, not drop\n",
      "[145] : train loss 0.0004, val loss 0.0003, not drop\n",
      "[146] : train loss 0.0005, val loss 0.0013, not drop\n",
      "[147] : train loss 0.0006, val loss 0.0004, not drop\n",
      "[148] : train loss 0.0005, val loss 0.0013, not drop\n",
      "[149] : train loss 0.0005, val loss 0.0025, not drop\n",
      "[150] : train loss 0.0012, val loss 0.0010, not drop\n",
      "[151] : train loss 0.0007, val loss 0.0028, not drop\n",
      "[152] : train loss 0.000539, val loss drop 0.0003 to 0.0003\n",
      "[153] : train loss 0.0008, val loss 0.0019, not drop\n",
      "[154] : train loss 0.0009, val loss 0.0044, not drop\n",
      "[155] : train loss 0.0010, val loss 0.0029, not drop\n",
      "[156] : train loss 0.0005, val loss 0.0039, not drop\n",
      "[157] : train loss 0.0005, val loss 0.0004, not drop\n",
      "[158] : train loss 0.0005, val loss 0.0010, not drop\n",
      "[159] : train loss 0.0004, val loss 0.0008, not drop\n",
      "[160] : train loss 0.0003, val loss 0.0003, not drop\n",
      "[161] : train loss 0.0002, val loss 0.0004, not drop\n",
      "[162] : train loss 0.0003, val loss 0.0016, not drop\n",
      "[163] : train loss 0.0003, val loss 0.0003, not drop\n",
      "[164] : train loss 0.0004, val loss 0.0031, not drop\n",
      "[165] : train loss 0.0006, val loss 0.0003, not drop\n",
      "[166] : train loss 0.0004, val loss 0.0010, not drop\n",
      "[167] : train loss 0.0003, val loss 0.0024, not drop\n",
      "[168] : train loss 0.0004, val loss 0.0003, not drop\n",
      "[169] : train loss 0.0003, val loss 0.0005, not drop\n",
      "[170] : train loss 0.0004, val loss 0.0004, not drop\n",
      "[171] : train loss 0.0005, val loss 0.0007, not drop\n",
      "[172] : train loss 0.0005, val loss 0.0011, not drop\n",
      "[173] : train loss 0.0009, val loss 0.0016, not drop\n",
      "[174] : train loss 0.0006, val loss 0.0016, not drop\n",
      "[175] : train loss 0.0008, val loss 0.0019, not drop\n",
      "[176] : train loss 0.000375, val loss drop 0.0003 to 0.0003\n",
      "[177] : train loss 0.0005, val loss 0.0035, not drop\n",
      "[178] : train loss 0.0007, val loss 0.0032, not drop\n",
      "[179] : train loss 0.0004, val loss 0.0015, not drop\n",
      "[180] : train loss 0.0005, val loss 0.0007, not drop\n",
      "[181] : train loss 0.0005, val loss 0.0012, not drop\n",
      "[182] : train loss 0.0005, val loss 0.0020, not drop\n",
      "[183] : train loss 0.0006, val loss 0.0037, not drop\n",
      "[184] : train loss 0.0006, val loss 0.0004, not drop\n",
      "[185] : train loss 0.0006, val loss 0.0004, not drop\n",
      "[186] : train loss 0.0008, val loss 0.0009, not drop\n",
      "[187] : train loss 0.0008, val loss 0.0023, not drop\n",
      "[188] : train loss 0.0008, val loss 0.0003, not drop\n",
      "[189] : train loss 0.0005, val loss 0.0009, not drop\n",
      "[190] : train loss 0.0006, val loss 0.0003, not drop\n",
      "[191] : train loss 0.0005, val loss 0.0005, not drop\n",
      "[192] : train loss 0.0003, val loss 0.0004, not drop\n",
      "[193] : train loss 0.0004, val loss 0.0006, not drop\n",
      "[194] : train loss 0.0005, val loss 0.0005, not drop\n",
      "[195] : train loss 0.0005, val loss 0.0006, not drop\n",
      "[196] : train loss 0.0004, val loss 0.0047, not drop\n",
      "[197] : train loss 0.0004, val loss 0.0032, not drop\n",
      "[198] : train loss 0.0005, val loss 0.0004, not drop\n",
      "[199] : train loss 0.0005, val loss 0.0023, not drop\n",
      "[200] : train loss 0.0005, val loss 0.0034, not drop\n"
     ]
    }
   ],
   "source": [
    "val_losses = []\n",
    "curr_loss = 1e+7\n",
    "os.makedirs(root_path) if not os.path.exists(root_path) else None\n",
    "for ep in range(1, EPOCH + 1):\n",
    "    model.train()\n",
    "    loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "    model.eval()\n",
    "    val_loss =eval_model(model, val_loader, criterion)\n",
    "    if curr_loss > val_loss:\n",
    "        print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "        curr_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(root_path, 'model_{}.pt'.format(name)))\n",
    "    else:\n",
    "        print('[{}] : train loss {:.4f}, val loss {:.4f}, not drop'.format(ep, np.mean(loss), val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn((3,1), len(name))\n",
    "model.load_state_dict(torch.load('./tmp/model_XYMV.pt'))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predict = model(test_f.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "submission[['X','Y','M','V']] = predict.detach().cpu().numpy()\n",
    "submission.to_csv(os.path.join(root_dir, 'cnn_20200625_total_loss.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

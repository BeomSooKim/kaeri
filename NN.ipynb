{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.target[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E1_loss(y_pred, y_true):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: distance error normalized with 2e+04\n",
    "    '''\n",
    "    \n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    '''\n",
    "    y_true: dataframe with true values of X,Y,M,V\n",
    "    y_pred: dataframe with pred values of X,Y,M,V\n",
    "    \n",
    "    return: sum of mass and velocity's mean squared percentage error\n",
    "    '''\n",
    "    \n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))\n",
    "\n",
    "def total_loss(y_pred, y_true):\n",
    "    xy_t, xy_p = y_true[:,:2], y_pred[:,:2]\n",
    "    mv_t, mv_p = y_true[:,2:], y_pred[:,2:]\n",
    "    \n",
    "    e1 = torch.mean(torch.mean((xy_t - xy_p) ** 2, axis = 1)) / 2e+04\n",
    "    e2 = torch.mean(torch.mean((mv_t - mv_p) ** 2 / (mv_t + 1e-06), axis = 1))\n",
    "    \n",
    "    return e1 + e2\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_data, loss_func):\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_bn(nn.Module):\n",
    "    def __init__(self, i_f, o_f, fs):\n",
    "        super(conv_bn, self).__init__()\n",
    "        self.conv = nn.Conv2d(i_f, o_f, fs)\n",
    "        self.act = nn.ELU()\n",
    "        self.bn = nn.BatchNorm2d(o_f)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 1), stride= (2, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.act(self.conv(x)))\n",
    "        return self.pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, h_list, input_shape, fs):\n",
    "        '''\n",
    "        input_shape : not include batch_size\n",
    "        '''\n",
    "        \n",
    "        super(conv_block, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.fs = fs\n",
    "        convs = []\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                convs.append(conv_bn(self.input_shape[0], h_list[i], fs))\n",
    "            else:\n",
    "                convs.append(conv_bn(h_list[i-1], h_list[i], fs))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self, h_list, input_size, output_size):\n",
    "        super(classifier, self).__init__()\n",
    "        layers = []\n",
    "        self.activation = nn.ELU()\n",
    "        for i in range(len(h_list)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, h_list[0]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(h_list[i-1], h_list[i]))\n",
    "            layers.append(nn.ELU())\n",
    "            \n",
    "        layers.append(nn.Linear(h_list[i], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn_model(nn.Module):\n",
    "    def __init__(self, cnn_block, fc_block):\n",
    "        super(cnn_model, self).__init__()\n",
    "        self.cnn = cnn_block\n",
    "        self.fc = fc_block\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- xy, m, v 쪼개서 학습 가능하게\n",
    "- cross validation\n",
    "- parallel cnn 구현\n",
    "- lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "name = 'XYMV'\n",
    "root_path = './model'\n",
    "initialize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 4)\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "train_target = train_t[list(name)].values#.astype(np.float32)\n",
    "print(train_target.shape)\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = classifier([64], input_size = 512*3*5, output_size = 4)\n",
    "conv = conv_block([16, 32, 64, 128, 256, 512], [1, 375, 4], (3, 1))\n",
    "\n",
    "model = cnn_model(conv, fc)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "criterion = E1_loss\n",
    "model = model.cuda()\n",
    "if initialize:\n",
    "    model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 373, 5]              64\n",
      "               ELU-2           [-1, 16, 373, 5]               0\n",
      "       BatchNorm2d-3           [-1, 16, 373, 5]              32\n",
      "         MaxPool2d-4           [-1, 16, 186, 5]               0\n",
      "           conv_bn-5           [-1, 16, 186, 5]               0\n",
      "            Conv2d-6           [-1, 32, 184, 5]           1,568\n",
      "               ELU-7           [-1, 32, 184, 5]               0\n",
      "       BatchNorm2d-8           [-1, 32, 184, 5]              64\n",
      "         MaxPool2d-9            [-1, 32, 92, 5]               0\n",
      "          conv_bn-10            [-1, 32, 92, 5]               0\n",
      "           Conv2d-11            [-1, 64, 90, 5]           6,208\n",
      "              ELU-12            [-1, 64, 90, 5]               0\n",
      "      BatchNorm2d-13            [-1, 64, 90, 5]             128\n",
      "        MaxPool2d-14            [-1, 64, 45, 5]               0\n",
      "          conv_bn-15            [-1, 64, 45, 5]               0\n",
      "           Conv2d-16           [-1, 128, 43, 5]          24,704\n",
      "              ELU-17           [-1, 128, 43, 5]               0\n",
      "      BatchNorm2d-18           [-1, 128, 43, 5]             256\n",
      "        MaxPool2d-19           [-1, 128, 21, 5]               0\n",
      "          conv_bn-20           [-1, 128, 21, 5]               0\n",
      "           Conv2d-21           [-1, 256, 19, 5]          98,560\n",
      "              ELU-22           [-1, 256, 19, 5]               0\n",
      "      BatchNorm2d-23           [-1, 256, 19, 5]             512\n",
      "        MaxPool2d-24            [-1, 256, 9, 5]               0\n",
      "          conv_bn-25            [-1, 256, 9, 5]               0\n",
      "           Conv2d-26            [-1, 512, 7, 5]         393,728\n",
      "              ELU-27            [-1, 512, 7, 5]               0\n",
      "      BatchNorm2d-28            [-1, 512, 7, 5]           1,024\n",
      "        MaxPool2d-29            [-1, 512, 3, 5]               0\n",
      "          conv_bn-30            [-1, 512, 3, 5]               0\n",
      "       conv_block-31            [-1, 512, 3, 5]               0\n",
      "           Linear-32                   [-1, 64]         491,584\n",
      "              ELU-33                   [-1, 64]               0\n",
      "           Linear-34                    [-1, 4]             260\n",
      "       classifier-35                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 1,018,692\n",
      "Trainable params: 1,018,692\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.84\n",
      "Params size (MB): 3.89\n",
      "Estimated Total Size (MB): 8.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 375, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx, valx, trainy, valy = train_test_split(train_f, train_target, test_size = 0.2, shuffle = True, random_state = 38)\n",
    "\n",
    "train_dataset = dfDataset(trainx.astype(np.float32), trainy)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 256, shuffle = True)\n",
    "val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 256, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] : train loss 1.616494, val loss drop 10000000.0000 to 1.6072\n",
      "[2] : train loss 0.996160, val loss drop 1.6072 to 1.0413\n",
      "[3] : train loss 0.433721, val loss drop 1.0413 to 0.3318\n",
      "[4] : train loss 0.141878, val loss drop 0.3318 to 0.2052\n",
      "[5] : train loss 0.056921, val loss drop 0.2052 to 0.0462\n",
      "[6] : train loss 0.027530, val loss drop 0.0462 to 0.0334\n",
      "[7] : train loss 0.016786, val loss drop 0.0334 to 0.0141\n",
      "[8] : train loss 0.0109, val loss 0.0426, not drop\n",
      "[9] : train loss 0.0077, val loss 0.0295, not drop\n",
      "[10] : train loss 0.0057, val loss 0.0161, not drop\n",
      "[11] : train loss 0.004488, val loss drop 0.0141 to 0.0073\n",
      "[12] : train loss 0.0044, val loss 0.0101, not drop\n",
      "[13] : train loss 0.0037, val loss 0.0171, not drop\n",
      "[14] : train loss 0.002893, val loss drop 0.0073 to 0.0058\n",
      "[15] : train loss 0.0029, val loss 0.0058, not drop\n",
      "[16] : train loss 0.002622, val loss drop 0.0058 to 0.0036\n",
      "[17] : train loss 0.002490, val loss drop 0.0036 to 0.0034\n",
      "[18] : train loss 0.0021, val loss 0.0072, not drop\n",
      "[19] : train loss 0.001985, val loss drop 0.0034 to 0.0023\n",
      "[20] : train loss 0.0019, val loss 0.0044, not drop\n",
      "[21] : train loss 0.0025, val loss 0.0033, not drop\n",
      "[22] : train loss 0.0018, val loss 0.0043, not drop\n",
      "[23] : train loss 0.0020, val loss 0.0081, not drop\n",
      "[24] : train loss 0.0014, val loss 0.0116, not drop\n",
      "[25] : train loss 0.0012, val loss 0.0080, not drop\n",
      "[26] : train loss 0.000996, val loss drop 0.0023 to 0.0022\n",
      "[27] : train loss 0.001049, val loss drop 0.0022 to 0.0013\n",
      "[28] : train loss 0.0023, val loss 0.0034, not drop\n",
      "[29] : train loss 0.0023, val loss 0.0019, not drop\n",
      "[30] : train loss 0.0017, val loss 0.0145, not drop\n",
      "[31] : train loss 0.0013, val loss 0.0019, not drop\n",
      "[32] : train loss 0.0011, val loss 0.0036, not drop\n",
      "[33] : train loss 0.0011, val loss 0.0043, not drop\n",
      "[34] : train loss 0.0011, val loss 0.0025, not drop\n",
      "[35] : train loss 0.0014, val loss 0.0048, not drop\n",
      "[36] : train loss 0.0014, val loss 0.0046, not drop\n",
      "[37] : train loss 0.0016, val loss 0.0037, not drop\n",
      "[38] : train loss 0.0017, val loss 0.0031, not drop\n",
      "[39] : train loss 0.0010, val loss 0.0016, not drop\n",
      "[40] : train loss 0.0009, val loss 0.0015, not drop\n",
      "[41] : train loss 0.0012, val loss 0.0015, not drop\n",
      "[42] : train loss 0.0008, val loss 0.0045, not drop\n",
      "[43] : train loss 0.0005, val loss 0.0042, not drop\n",
      "[44] : train loss 0.0007, val loss 0.0014, not drop\n",
      "[45] : train loss 0.0005, val loss 0.0017, not drop\n",
      "[46] : train loss 0.0007, val loss 0.0076, not drop\n",
      "[47] : train loss 0.000868, val loss drop 0.0013 to 0.0012\n",
      "[48] : train loss 0.0011, val loss 0.0028, not drop\n",
      "[49] : train loss 0.0007, val loss 0.0020, not drop\n",
      "[50] : train loss 0.000695, val loss drop 0.0012 to 0.0010\n",
      "[51] : train loss 0.000756, val loss drop 0.0010 to 0.0010\n",
      "[52] : train loss 0.0006, val loss 0.0039, not drop\n",
      "[53] : train loss 0.0016, val loss 0.0015, not drop\n",
      "[54] : train loss 0.0011, val loss 0.0093, not drop\n",
      "[55] : train loss 0.000812, val loss drop 0.0010 to 0.0006\n",
      "[56] : train loss 0.0006, val loss 0.0014, not drop\n",
      "[57] : train loss 0.0007, val loss 0.0031, not drop\n",
      "[58] : train loss 0.0006, val loss 0.0048, not drop\n",
      "[59] : train loss 0.0006, val loss 0.0033, not drop\n",
      "[60] : train loss 0.0004, val loss 0.0033, not drop\n",
      "[61] : train loss 0.0007, val loss 0.0033, not drop\n",
      "[62] : train loss 0.0005, val loss 0.0032, not drop\n",
      "[63] : train loss 0.0007, val loss 0.0008, not drop\n",
      "[64] : train loss 0.0008, val loss 0.0063, not drop\n",
      "[65] : train loss 0.0006, val loss 0.0010, not drop\n",
      "[66] : train loss 0.0005, val loss 0.0009, not drop\n",
      "[67] : train loss 0.0005, val loss 0.0008, not drop\n",
      "[68] : train loss 0.000631, val loss drop 0.0006 to 0.0005\n",
      "[69] : train loss 0.0009, val loss 0.0011, not drop\n",
      "[70] : train loss 0.0012, val loss 0.0019, not drop\n",
      "[71] : train loss 0.0008, val loss 0.0039, not drop\n",
      "[72] : train loss 0.0006, val loss 0.0049, not drop\n",
      "[73] : train loss 0.0012, val loss 0.0017, not drop\n",
      "[74] : train loss 0.0011, val loss 0.0029, not drop\n",
      "[75] : train loss 0.0012, val loss 0.0037, not drop\n",
      "[76] : train loss 0.0008, val loss 0.0051, not drop\n",
      "[77] : train loss 0.0011, val loss 0.0006, not drop\n",
      "[78] : train loss 0.0008, val loss 0.0045, not drop\n",
      "[79] : train loss 0.0005, val loss 0.0007, not drop\n",
      "[80] : train loss 0.0008, val loss 0.0048, not drop\n",
      "[81] : train loss 0.0011, val loss 0.0022, not drop\n",
      "[82] : train loss 0.0011, val loss 0.0070, not drop\n",
      "[83] : train loss 0.0012, val loss 0.0012, not drop\n",
      "[84] : train loss 0.0008, val loss 0.0006, not drop\n",
      "[85] : train loss 0.0006, val loss 0.0008, not drop\n",
      "[86] : train loss 0.0005, val loss 0.0006, not drop\n",
      "[87] : train loss 0.0005, val loss 0.0009, not drop\n",
      "[88] : train loss 0.0007, val loss 0.0006, not drop\n",
      "[89] : train loss 0.000431, val loss drop 0.0005 to 0.0004\n",
      "[90] : train loss 0.0005, val loss 0.0005, not drop\n",
      "[91] : train loss 0.0005, val loss 0.0006, not drop\n",
      "[92] : train loss 0.0005, val loss 0.0005, not drop\n",
      "[93] : train loss 0.0003, val loss 0.0023, not drop\n",
      "[94] : train loss 0.0003, val loss 0.0011, not drop\n",
      "[95] : train loss 0.0006, val loss 0.0010, not drop\n",
      "[96] : train loss 0.0003, val loss 0.0013, not drop\n",
      "[97] : train loss 0.0005, val loss 0.0011, not drop\n",
      "[98] : train loss 0.0006, val loss 0.0008, not drop\n",
      "[99] : train loss 0.000412, val loss drop 0.0004 to 0.0004\n",
      "[100] : train loss 0.0003, val loss 0.0010, not drop\n"
     ]
    }
   ],
   "source": [
    "val_losses = []\n",
    "curr_loss = 1e+7\n",
    "os.makedirs(root_path) if not os.path.exists(root_path) else None\n",
    "for ep in range(1, EPOCH + 1):\n",
    "    model.train()\n",
    "    loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "    model.eval()\n",
    "    val_loss =eval_model(model, val_loader, criterion)\n",
    "    if curr_loss > val_loss:\n",
    "        print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "        curr_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(root_path, 'model_{}.pt'.format(name)))\n",
    "    else:\n",
    "        print('[{}] : train loss {:.4f}, val loss {:.4f}, not drop'.format(ep, np.mean(loss), val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn((3,1), len(name))\n",
    "model.load_state_dict(torch.load('./tmp/model_XYMV.pt'))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predict = model(test_f.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "submission[['X','Y','M','V']] = predict.detach().cpu().numpy()\n",
    "submission.to_csv(os.path.join(root_dir, 'cnn_20200625_total_loss.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cnn architecture 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from metric import E1_loss, E2_loss, total_loss\n",
    "from models import classifier, cnn_model, conv_block, cnn_parallel\n",
    "from utils import train_model, eval_model, dfDataset, weights_init\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 34\n",
    "def fix_seed(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(SEED)\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class, function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(object):\n",
    "    def __init__(self, mu, sd, shape):\n",
    "        self.mu = mu\n",
    "        self.sd = sd\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = np.random.normal(self.mu, self.sd, self.shape)\n",
    "        #noise = torch.FloatTensor(noise)\n",
    "        return x + noise.astype(np.float32)\n",
    "\n",
    "class dfDataset(Dataset):\n",
    "    def __init__(self, x, y, transform = None):\n",
    "        self.data = x\n",
    "        self.target = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batchX, batchY = self.data[index], self.target[index]\n",
    "        if self.transform:\n",
    "            batchX = self.transform(batchX)\n",
    "        return batchX, batchY\n",
    "    \n",
    "def weights_init(m, initializer = nn.init.kaiming_uniform_):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        initializer(m.weight)\n",
    "        \n",
    "def train_model(model, train_data, weight, optimizer, loss_func):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    \n",
    "    return loss_sum / len(train_data)\n",
    "\n",
    "def eval_model(model, val_data, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        for i, (x, y) in enumerate(val_data):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss += loss_func(pred, y).item()\n",
    "    return loss / len(val_data)\n",
    "\n",
    "def E1_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2, axis = 1)) / 2e+04\n",
    "\n",
    "def E2_loss(y_pred, y_true):\n",
    "    _t, _p = y_true, y_pred\n",
    "    \n",
    "    return torch.mean(torch.mean((_t - _p) ** 2 / (_t + 1e-06), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_cnn(nn.Module):\n",
    "    def __init__(self, n_feature, out_len):\n",
    "        super(custom_cnn, self).__init__()\n",
    "        self.conv_kernel = (4, 1)\n",
    "        self.pool_kernel = (2, 1)\n",
    "        \n",
    "        self.fe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size = self.conv_kernel, stride = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = self.pool_kernel)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*2*n_feature, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(32, out_len)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.fe(x)\n",
    "        return self.fc(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "base_lr = 0.001\n",
    "now = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "save_path = './model/{}'.format(now)\n",
    "initialize = True\n",
    "print_summary = True\n",
    "batch_size = 256\n",
    "nfold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, axis = 2):\n",
    "    mu = np.expand_dims(x.mean(axis = 2), axis = axis)\n",
    "    sd = np.expand_dims(x.std(axis = 2), axis = axis)\n",
    "\n",
    "    normalized = (x - mu) / sd\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def furier(x, fs = 250000, fmax = 375,  N = 375):\n",
    "    dt = 1/250000\n",
    "    df = fmax / N\n",
    "    f = np.arange(0, N) * df\n",
    "    xf = np.fft.fft(x) * dt\n",
    "    #return xf[0:int(N/2 + 1)]\n",
    "    return np.abs(xf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    root_dir = 'D:/datasets/KAERI_dataset/'\n",
    "else:\n",
    "    root_dir = '/home/bskim/project/kaeri/KAERI_dataset/'\n",
    "\n",
    "train_f = pd.read_csv(os.path.join(root_dir, 'train_features.csv'))\n",
    "train_t = pd.read_csv(os.path.join(root_dir, 'train_target.csv'))\n",
    "test_f = pd.read_csv(os.path.join(root_dir, 'test_features.csv'))\n",
    "\n",
    "train_f = train_f[['Time','S1','S2','S3','S4']].values\n",
    "train_f = train_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "test_f = test_f[['Time','S1','S2','S3','S4']].values\n",
    "test_f = test_f.reshape((-1, 1, 375, 5))#.astype(np.float32)\n",
    "\n",
    "train_furier = np.apply_over_axes(furier, train_f, axes = (0, 1, 3))\n",
    "test_furier = np.apply_over_axes(furier, test_f, axes = (0, 1, 3))\n",
    "\n",
    "# concatenate normalized data\n",
    "train_norm = normalize(train_f)\n",
    "test_norm = normalize(test_f)\n",
    "\n",
    "train_f = np.concatenate((train_f, train_norm, train_furier), axis = 1)\n",
    "test_f = np.concatenate((test_f, test_norm, test_furier), axis = 1)\n",
    "\n",
    "test_f = torch.FloatTensor(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_train(name, feature, target):\n",
    "    print('{} train...'.format(name))\n",
    "    n_features = feature.shape[-1]\n",
    "    os.makedirs(save_path) if not os.path.exists(save_path) else None\n",
    "    # make dataset\n",
    "    train_target = target[list(name)].values\n",
    "\n",
    "    fold = KFold(nfold, shuffle = True, random_state= 25)\n",
    "    loss_per_cv = []\n",
    "    noise_add = Noise(0, 0.001, feature.shape[1:])\n",
    "    for i, (train_idx, val_idx) in enumerate(fold.split(feature, y = train_target)):\n",
    "        print('fold {}'.format(i+1))\n",
    "        trainx = feature[train_idx]\n",
    "        valx = feature[val_idx]\n",
    "        trainy = train_target[train_idx]\n",
    "        valy = train_target[val_idx]\n",
    "\n",
    "        train_dataset = dfDataset(trainx.astype(np.float32), trainy, transform = noise_add)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        val_dataset = dfDataset(valx.astype(np.float32), valy)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        model = custom_cnn(n_features, len(name))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = base_lr)\n",
    "\n",
    "        if name == 'XY':\n",
    "            criterion = E1_loss\n",
    "        else:\n",
    "            criterion = E2_loss\n",
    "\n",
    "        model = model.cuda()\n",
    "        if initialize:\n",
    "            model.apply(weights_init)\n",
    "\n",
    "        curr_loss = 1e+7\n",
    "        #train\n",
    "        for ep in range(1, EPOCH + 1):\n",
    "            loss = train_model(model, train_loader, criterion, optimizer, criterion)\n",
    "            val_loss =eval_model(model, val_loader, criterion)\n",
    "            if curr_loss > val_loss:\n",
    "                print('[{}] : train loss {:4f}, val loss drop {:.4f} to {:.4f}'.format(ep, np.mean(loss), curr_loss, val_loss))\n",
    "                curr_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i+1)))\n",
    "        loss_per_cv.append(curr_loss)\n",
    "    return loss_per_cv           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY train...\n",
      "fold 1\n",
      "[1] : train loss 2.717373, val loss drop 10000000.0000 to 1.6474\n",
      "[2] : train loss 1.082835, val loss drop 1.6474 to 0.5067\n",
      "[3] : train loss 0.302926, val loss drop 0.5067 to 0.2521\n",
      "[4] : train loss 0.108339, val loss drop 0.2521 to 0.1023\n",
      "[5] : train loss 0.062582, val loss drop 0.1023 to 0.0667\n",
      "[6] : train loss 0.045505, val loss drop 0.0667 to 0.0596\n",
      "[7] : train loss 0.031944, val loss drop 0.0596 to 0.0328\n",
      "[8] : train loss 0.019824, val loss drop 0.0328 to 0.0304\n",
      "[10] : train loss 0.009638, val loss drop 0.0304 to 0.0132\n",
      "[11] : train loss 0.007943, val loss drop 0.0132 to 0.0120\n",
      "[12] : train loss 0.006501, val loss drop 0.0120 to 0.0106\n",
      "[14] : train loss 0.004631, val loss drop 0.0106 to 0.0054\n",
      "[20] : train loss 0.004488, val loss drop 0.0054 to 0.0047\n",
      "[21] : train loss 0.003231, val loss drop 0.0047 to 0.0042\n",
      "[23] : train loss 0.002112, val loss drop 0.0042 to 0.0039\n",
      "[24] : train loss 0.001917, val loss drop 0.0039 to 0.0031\n",
      "[30] : train loss 0.001781, val loss drop 0.0031 to 0.0022\n",
      "[36] : train loss 0.001220, val loss drop 0.0022 to 0.0021\n",
      "[38] : train loss 0.001458, val loss drop 0.0021 to 0.0018\n",
      "[48] : train loss 0.000884, val loss drop 0.0018 to 0.0015\n",
      "[50] : train loss 0.000826, val loss drop 0.0015 to 0.0012\n",
      "[51] : train loss 0.000712, val loss drop 0.0012 to 0.0012\n",
      "[71] : train loss 0.000730, val loss drop 0.0012 to 0.0009\n",
      "[72] : train loss 0.000573, val loss drop 0.0009 to 0.0008\n",
      "fold 2\n",
      "[1] : train loss 2.796801, val loss drop 10000000.0000 to 1.4542\n",
      "[2] : train loss 1.108842, val loss drop 1.4542 to 0.3164\n",
      "[3] : train loss 0.217710, val loss drop 0.3164 to 0.0850\n",
      "[5] : train loss 0.042673, val loss drop 0.0850 to 0.0297\n",
      "[7] : train loss 0.012766, val loss drop 0.0297 to 0.0226\n",
      "[8] : train loss 0.009355, val loss drop 0.0226 to 0.0129\n",
      "[9] : train loss 0.008281, val loss drop 0.0129 to 0.0121\n",
      "[10] : train loss 0.006145, val loss drop 0.0121 to 0.0072\n",
      "[11] : train loss 0.005107, val loss drop 0.0072 to 0.0058\n",
      "[13] : train loss 0.005214, val loss drop 0.0058 to 0.0033\n",
      "[16] : train loss 0.003167, val loss drop 0.0033 to 0.0032\n",
      "[17] : train loss 0.002834, val loss drop 0.0032 to 0.0026\n",
      "[19] : train loss 0.002579, val loss drop 0.0026 to 0.0025\n",
      "[20] : train loss 0.002105, val loss drop 0.0025 to 0.0024\n",
      "[21] : train loss 0.002229, val loss drop 0.0024 to 0.0024\n",
      "[25] : train loss 0.001646, val loss drop 0.0024 to 0.0016\n",
      "[27] : train loss 0.001078, val loss drop 0.0016 to 0.0014\n",
      "[32] : train loss 0.001209, val loss drop 0.0014 to 0.0013\n",
      "[37] : train loss 0.001351, val loss drop 0.0013 to 0.0013\n",
      "[40] : train loss 0.000779, val loss drop 0.0013 to 0.0012\n",
      "[43] : train loss 0.000890, val loss drop 0.0012 to 0.0011\n",
      "[50] : train loss 0.000856, val loss drop 0.0011 to 0.0008\n",
      "[57] : train loss 0.000644, val loss drop 0.0008 to 0.0006\n",
      "[79] : train loss 0.000437, val loss drop 0.0006 to 0.0005\n",
      "[88] : train loss 0.000380, val loss drop 0.0005 to 0.0004\n",
      "fold 3\n",
      "[1] : train loss 2.632825, val loss drop 10000000.0000 to 1.0625\n",
      "[2] : train loss 0.814180, val loss drop 1.0625 to 0.3177\n",
      "[3] : train loss 0.194067, val loss drop 0.3177 to 0.1155\n",
      "[4] : train loss 0.066332, val loss drop 0.1155 to 0.0789\n",
      "[5] : train loss 0.036725, val loss drop 0.0789 to 0.0223\n",
      "[6] : train loss 0.021548, val loss drop 0.0223 to 0.0219\n",
      "[8] : train loss 0.011180, val loss drop 0.0219 to 0.0123\n",
      "[9] : train loss 0.008206, val loss drop 0.0123 to 0.0109\n",
      "[11] : train loss 0.006970, val loss drop 0.0109 to 0.0092\n",
      "[13] : train loss 0.005303, val loss drop 0.0092 to 0.0037\n",
      "[19] : train loss 0.002677, val loss drop 0.0037 to 0.0035\n",
      "[20] : train loss 0.002235, val loss drop 0.0035 to 0.0023\n",
      "[21] : train loss 0.001659, val loss drop 0.0023 to 0.0023\n",
      "[24] : train loss 0.001676, val loss drop 0.0023 to 0.0023\n",
      "[33] : train loss 0.001688, val loss drop 0.0023 to 0.0022\n",
      "[42] : train loss 0.000984, val loss drop 0.0022 to 0.0018\n",
      "[46] : train loss 0.001109, val loss drop 0.0018 to 0.0016\n",
      "[47] : train loss 0.001027, val loss drop 0.0016 to 0.0012\n",
      "[58] : train loss 0.000576, val loss drop 0.0012 to 0.0010\n",
      "[76] : train loss 0.001223, val loss drop 0.0010 to 0.0009\n",
      "[81] : train loss 0.000632, val loss drop 0.0009 to 0.0007\n",
      "fold 4\n",
      "[1] : train loss 2.767146, val loss drop 10000000.0000 to 1.4627\n",
      "[2] : train loss 1.119925, val loss drop 1.4627 to 0.3274\n",
      "[3] : train loss 0.257676, val loss drop 0.3274 to 0.3231\n",
      "[4] : train loss 0.085063, val loss drop 0.3231 to 0.0639\n",
      "[5] : train loss 0.039675, val loss drop 0.0639 to 0.0436\n",
      "[6] : train loss 0.024654, val loss drop 0.0436 to 0.0289\n",
      "[7] : train loss 0.017162, val loss drop 0.0289 to 0.0152\n",
      "[10] : train loss 0.007135, val loss drop 0.0152 to 0.0084\n",
      "[12] : train loss 0.005780, val loss drop 0.0084 to 0.0059\n",
      "[16] : train loss 0.003667, val loss drop 0.0059 to 0.0041\n",
      "[17] : train loss 0.002890, val loss drop 0.0041 to 0.0035\n",
      "[19] : train loss 0.002252, val loss drop 0.0035 to 0.0031\n",
      "[20] : train loss 0.002351, val loss drop 0.0031 to 0.0022\n",
      "[32] : train loss 0.001728, val loss drop 0.0022 to 0.0020\n",
      "[38] : train loss 0.000932, val loss drop 0.0020 to 0.0017\n",
      "[40] : train loss 0.000704, val loss drop 0.0017 to 0.0014\n",
      "[42] : train loss 0.001001, val loss drop 0.0014 to 0.0013\n",
      "[49] : train loss 0.000620, val loss drop 0.0013 to 0.0012\n",
      "[51] : train loss 0.000702, val loss drop 0.0012 to 0.0009\n",
      "[61] : train loss 0.000724, val loss drop 0.0009 to 0.0008\n",
      "[93] : train loss 0.000593, val loss drop 0.0008 to 0.0006\n",
      "fold 5\n",
      "[1] : train loss 2.723295, val loss drop 10000000.0000 to 1.2978\n",
      "[2] : train loss 1.008383, val loss drop 1.2978 to 0.3102\n",
      "[3] : train loss 0.200948, val loss drop 0.3102 to 0.1293\n",
      "[4] : train loss 0.080472, val loss drop 0.1293 to 0.1028\n",
      "[5] : train loss 0.042901, val loss drop 0.1028 to 0.0353\n",
      "[6] : train loss 0.024719, val loss drop 0.0353 to 0.0220\n",
      "[8] : train loss 0.010685, val loss drop 0.0220 to 0.0123\n",
      "[9] : train loss 0.008351, val loss drop 0.0123 to 0.0117\n",
      "[11] : train loss 0.007462, val loss drop 0.0117 to 0.0083\n",
      "[12] : train loss 0.004834, val loss drop 0.0083 to 0.0073\n",
      "[13] : train loss 0.003812, val loss drop 0.0073 to 0.0049\n",
      "[16] : train loss 0.002550, val loss drop 0.0049 to 0.0039\n",
      "[17] : train loss 0.002225, val loss drop 0.0039 to 0.0036\n",
      "[20] : train loss 0.002233, val loss drop 0.0036 to 0.0032\n",
      "[21] : train loss 0.002238, val loss drop 0.0032 to 0.0027\n",
      "[25] : train loss 0.001802, val loss drop 0.0027 to 0.0024\n",
      "[27] : train loss 0.001628, val loss drop 0.0024 to 0.0021\n",
      "[30] : train loss 0.001202, val loss drop 0.0021 to 0.0013\n",
      "[41] : train loss 0.000977, val loss drop 0.0013 to 0.0012\n",
      "[42] : train loss 0.000892, val loss drop 0.0012 to 0.0012\n",
      "[46] : train loss 0.000860, val loss drop 0.0012 to 0.0010\n",
      "[49] : train loss 0.001024, val loss drop 0.0010 to 0.0010\n",
      "[56] : train loss 0.000706, val loss drop 0.0010 to 0.0009\n",
      "[59] : train loss 0.000460, val loss drop 0.0009 to 0.0007\n",
      "[86] : train loss 0.000487, val loss drop 0.0007 to 0.0006\n",
      "[98] : train loss 0.000404, val loss drop 0.0006 to 0.0006\n",
      "[99] : train loss 0.000422, val loss drop 0.0006 to 0.0005\n",
      "fold 6\n",
      "[1] : train loss 2.780952, val loss drop 10000000.0000 to 1.3721\n",
      "[2] : train loss 1.058016, val loss drop 1.3721 to 0.3572\n",
      "[3] : train loss 0.236712, val loss drop 0.3572 to 0.1315\n",
      "[4] : train loss 0.090407, val loss drop 0.1315 to 0.0863\n",
      "[5] : train loss 0.041608, val loss drop 0.0863 to 0.0460\n",
      "[6] : train loss 0.027268, val loss drop 0.0460 to 0.0322\n",
      "[7] : train loss 0.017648, val loss drop 0.0322 to 0.0293\n",
      "[8] : train loss 0.015412, val loss drop 0.0293 to 0.0172\n",
      "[10] : train loss 0.008296, val loss drop 0.0172 to 0.0170\n",
      "[11] : train loss 0.006393, val loss drop 0.0170 to 0.0095\n",
      "[13] : train loss 0.004153, val loss drop 0.0095 to 0.0072\n",
      "[16] : train loss 0.003893, val loss drop 0.0072 to 0.0047\n",
      "[19] : train loss 0.002392, val loss drop 0.0047 to 0.0047\n",
      "[20] : train loss 0.002007, val loss drop 0.0047 to 0.0034\n",
      "[23] : train loss 0.002635, val loss drop 0.0034 to 0.0028\n",
      "[28] : train loss 0.001551, val loss drop 0.0028 to 0.0020\n",
      "[30] : train loss 0.001139, val loss drop 0.0020 to 0.0020\n",
      "[34] : train loss 0.001139, val loss drop 0.0020 to 0.0017\n",
      "[37] : train loss 0.000922, val loss drop 0.0017 to 0.0016\n",
      "[38] : train loss 0.000885, val loss drop 0.0016 to 0.0015\n",
      "[54] : train loss 0.001369, val loss drop 0.0015 to 0.0015\n",
      "[56] : train loss 0.000687, val loss drop 0.0015 to 0.0011\n",
      "[57] : train loss 0.000508, val loss drop 0.0011 to 0.0009\n",
      "[62] : train loss 0.000404, val loss drop 0.0009 to 0.0008\n",
      "[86] : train loss 0.000664, val loss drop 0.0008 to 0.0007\n",
      "[94] : train loss 0.000400, val loss drop 0.0007 to 0.0006\n",
      "fold 7\n",
      "[1] : train loss 2.645571, val loss drop 10000000.0000 to 1.0568\n",
      "[2] : train loss 0.796902, val loss drop 1.0568 to 0.1920\n",
      "[3] : train loss 0.175791, val loss drop 0.1920 to 0.0818\n",
      "[4] : train loss 0.065533, val loss drop 0.0818 to 0.0315\n",
      "[5] : train loss 0.033258, val loss drop 0.0315 to 0.0274\n",
      "[6] : train loss 0.019309, val loss drop 0.0274 to 0.0164\n",
      "[8] : train loss 0.009077, val loss drop 0.0164 to 0.0124\n",
      "[9] : train loss 0.007112, val loss drop 0.0124 to 0.0052\n",
      "[11] : train loss 0.006343, val loss drop 0.0052 to 0.0039\n",
      "[14] : train loss 0.003276, val loss drop 0.0039 to 0.0036\n",
      "[15] : train loss 0.002909, val loss drop 0.0036 to 0.0027\n",
      "[16] : train loss 0.002501, val loss drop 0.0027 to 0.0025\n",
      "[17] : train loss 0.002800, val loss drop 0.0025 to 0.0025\n",
      "[21] : train loss 0.001973, val loss drop 0.0025 to 0.0025\n",
      "[22] : train loss 0.001640, val loss drop 0.0025 to 0.0016\n",
      "[23] : train loss 0.001302, val loss drop 0.0016 to 0.0015\n",
      "[32] : train loss 0.001146, val loss drop 0.0015 to 0.0013\n",
      "[34] : train loss 0.000897, val loss drop 0.0013 to 0.0011\n",
      "[38] : train loss 0.000642, val loss drop 0.0011 to 0.0007\n",
      "[49] : train loss 0.000754, val loss drop 0.0007 to 0.0007\n",
      "[55] : train loss 0.000448, val loss drop 0.0007 to 0.0006\n",
      "[56] : train loss 0.000384, val loss drop 0.0006 to 0.0006\n",
      "[63] : train loss 0.000660, val loss drop 0.0006 to 0.0005\n",
      "[74] : train loss 0.000319, val loss drop 0.0005 to 0.0005\n",
      "[90] : train loss 0.000440, val loss drop 0.0005 to 0.0005\n",
      "fold 8\n",
      "[1] : train loss 2.676951, val loss drop 10000000.0000 to 1.3644\n",
      "[2] : train loss 0.811631, val loss drop 1.3644 to 0.3183\n",
      "[3] : train loss 0.190633, val loss drop 0.3183 to 0.0991\n",
      "[4] : train loss 0.077668, val loss drop 0.0991 to 0.0552\n",
      "[5] : train loss 0.037319, val loss drop 0.0552 to 0.0238\n",
      "[6] : train loss 0.025690, val loss drop 0.0238 to 0.0212\n",
      "[7] : train loss 0.017399, val loss drop 0.0212 to 0.0159\n",
      "[8] : train loss 0.012175, val loss drop 0.0159 to 0.0076\n",
      "[12] : train loss 0.004417, val loss drop 0.0076 to 0.0064\n",
      "[13] : train loss 0.004476, val loss drop 0.0064 to 0.0050\n",
      "[14] : train loss 0.004508, val loss drop 0.0050 to 0.0039\n",
      "[16] : train loss 0.002952, val loss drop 0.0039 to 0.0031\n",
      "[17] : train loss 0.002907, val loss drop 0.0031 to 0.0028\n",
      "[23] : train loss 0.001989, val loss drop 0.0028 to 0.0024\n",
      "[24] : train loss 0.001262, val loss drop 0.0024 to 0.0024\n",
      "[25] : train loss 0.001306, val loss drop 0.0024 to 0.0021\n",
      "[27] : train loss 0.001594, val loss drop 0.0021 to 0.0015\n",
      "[31] : train loss 0.001032, val loss drop 0.0015 to 0.0013\n",
      "[32] : train loss 0.000915, val loss drop 0.0013 to 0.0012\n",
      "[42] : train loss 0.001238, val loss drop 0.0012 to 0.0011\n",
      "[59] : train loss 0.000604, val loss drop 0.0011 to 0.0007\n",
      "[74] : train loss 0.000717, val loss drop 0.0007 to 0.0006\n",
      "[94] : train loss 0.000372, val loss drop 0.0006 to 0.0006\n",
      "[100] : train loss 0.000398, val loss drop 0.0006 to 0.0005\n",
      "fold 9\n",
      "[1] : train loss 2.665742, val loss drop 10000000.0000 to 1.0186\n",
      "[2] : train loss 0.790424, val loss drop 1.0186 to 0.3142\n",
      "[3] : train loss 0.185354, val loss drop 0.3142 to 0.1361\n",
      "[4] : train loss 0.068725, val loss drop 0.1361 to 0.0366\n",
      "[5] : train loss 0.035381, val loss drop 0.0366 to 0.0347\n",
      "[6] : train loss 0.023831, val loss drop 0.0347 to 0.0169\n",
      "[7] : train loss 0.015437, val loss drop 0.0169 to 0.0126\n",
      "[8] : train loss 0.010527, val loss drop 0.0126 to 0.0106\n",
      "[11] : train loss 0.009141, val loss drop 0.0106 to 0.0049\n",
      "[12] : train loss 0.005732, val loss drop 0.0049 to 0.0048\n",
      "[13] : train loss 0.004750, val loss drop 0.0048 to 0.0034\n",
      "[14] : train loss 0.003894, val loss drop 0.0034 to 0.0033\n",
      "[19] : train loss 0.002137, val loss drop 0.0033 to 0.0026\n",
      "[20] : train loss 0.001871, val loss drop 0.0026 to 0.0023\n",
      "[24] : train loss 0.001654, val loss drop 0.0023 to 0.0021\n",
      "[26] : train loss 0.001947, val loss drop 0.0021 to 0.0018\n",
      "[29] : train loss 0.001231, val loss drop 0.0018 to 0.0017\n",
      "[32] : train loss 0.001028, val loss drop 0.0017 to 0.0013\n",
      "[33] : train loss 0.000893, val loss drop 0.0013 to 0.0012\n",
      "[35] : train loss 0.001033, val loss drop 0.0012 to 0.0010\n",
      "[48] : train loss 0.000684, val loss drop 0.0010 to 0.0009\n",
      "[52] : train loss 0.000608, val loss drop 0.0009 to 0.0008\n",
      "[55] : train loss 0.000602, val loss drop 0.0008 to 0.0008\n",
      "[56] : train loss 0.000477, val loss drop 0.0008 to 0.0006\n",
      "[75] : train loss 0.000502, val loss drop 0.0006 to 0.0006\n",
      "[87] : train loss 0.000561, val loss drop 0.0006 to 0.0005\n",
      "fold 10\n",
      "[1] : train loss 2.509600, val loss drop 10000000.0000 to 0.8037\n",
      "[2] : train loss 0.638783, val loss drop 0.8037 to 0.4603\n",
      "[3] : train loss 0.168371, val loss drop 0.4603 to 0.0979\n",
      "[4] : train loss 0.075464, val loss drop 0.0979 to 0.0729\n",
      "[5] : train loss 0.035375, val loss drop 0.0729 to 0.0303\n",
      "[6] : train loss 0.021587, val loss drop 0.0303 to 0.0192\n",
      "[8] : train loss 0.009068, val loss drop 0.0192 to 0.0131\n",
      "[9] : train loss 0.007258, val loss drop 0.0131 to 0.0107\n",
      "[10] : train loss 0.006357, val loss drop 0.0107 to 0.0078\n",
      "[11] : train loss 0.005335, val loss drop 0.0078 to 0.0057\n",
      "[13] : train loss 0.003146, val loss drop 0.0057 to 0.0048\n",
      "[14] : train loss 0.003142, val loss drop 0.0048 to 0.0044\n",
      "[15] : train loss 0.002430, val loss drop 0.0044 to 0.0035\n",
      "[16] : train loss 0.002574, val loss drop 0.0035 to 0.0024\n",
      "[21] : train loss 0.001977, val loss drop 0.0024 to 0.0020\n",
      "[27] : train loss 0.001373, val loss drop 0.0020 to 0.0020\n",
      "[32] : train loss 0.001061, val loss drop 0.0020 to 0.0016\n",
      "[33] : train loss 0.000915, val loss drop 0.0016 to 0.0015\n",
      "[38] : train loss 0.000817, val loss drop 0.0015 to 0.0015\n",
      "[41] : train loss 0.000675, val loss drop 0.0015 to 0.0010\n",
      "[61] : train loss 0.000466, val loss drop 0.0010 to 0.0010\n",
      "[62] : train loss 0.000807, val loss drop 0.0010 to 0.0009\n",
      "[67] : train loss 0.000677, val loss drop 0.0009 to 0.0008\n",
      "[71] : train loss 0.000463, val loss drop 0.0008 to 0.0008\n",
      "[74] : train loss 0.000449, val loss drop 0.0008 to 0.0007\n",
      "[100] : train loss 0.000293, val loss drop 0.0007 to 0.0006\n",
      "V train...\n",
      "fold 1\n",
      "[1] : train loss 9.443218, val loss drop 10000000.0000 to 2.1493\n",
      "[2] : train loss 0.408946, val loss drop 2.1493 to 0.2556\n",
      "[3] : train loss 0.124012, val loss drop 0.2556 to 0.2108\n",
      "[4] : train loss 0.053687, val loss drop 0.2108 to 0.0344\n",
      "[5] : train loss 0.021206, val loss drop 0.0344 to 0.0141\n",
      "[7] : train loss 0.013037, val loss drop 0.0141 to 0.0092\n",
      "[8] : train loss 0.008724, val loss drop 0.0092 to 0.0070\n",
      "[10] : train loss 0.004714, val loss drop 0.0070 to 0.0068\n",
      "[11] : train loss 0.004655, val loss drop 0.0068 to 0.0052\n",
      "[12] : train loss 0.004357, val loss drop 0.0052 to 0.0044\n",
      "[17] : train loss 0.008228, val loss drop 0.0044 to 0.0031\n",
      "[20] : train loss 0.002819, val loss drop 0.0031 to 0.0029\n",
      "[27] : train loss 0.002938, val loss drop 0.0029 to 0.0024\n",
      "[32] : train loss 0.003343, val loss drop 0.0024 to 0.0022\n",
      "[33] : train loss 0.001892, val loss drop 0.0022 to 0.0014\n",
      "[58] : train loss 0.004058, val loss drop 0.0014 to 0.0012\n",
      "[70] : train loss 0.002090, val loss drop 0.0012 to 0.0008\n",
      "[87] : train loss 0.001325, val loss drop 0.0008 to 0.0007\n",
      "fold 2\n",
      "[1] : train loss 13.197899, val loss drop 10000000.0000 to 0.5241\n",
      "[2] : train loss 0.155865, val loss drop 0.5241 to 0.2053\n",
      "[3] : train loss 0.046737, val loss drop 0.2053 to 0.0451\n",
      "[4] : train loss 0.028777, val loss drop 0.0451 to 0.0208\n",
      "[5] : train loss 0.040958, val loss drop 0.0208 to 0.0169\n",
      "[6] : train loss 0.027528, val loss drop 0.0169 to 0.0157\n",
      "[7] : train loss 0.026077, val loss drop 0.0157 to 0.0083\n",
      "[8] : train loss 0.010474, val loss drop 0.0083 to 0.0063\n",
      "[9] : train loss 0.011196, val loss drop 0.0063 to 0.0046\n",
      "[18] : train loss 0.017362, val loss drop 0.0046 to 0.0031\n",
      "[22] : train loss 0.013204, val loss drop 0.0031 to 0.0024\n",
      "[38] : train loss 0.011638, val loss drop 0.0024 to 0.0016\n",
      "[43] : train loss 0.008724, val loss drop 0.0016 to 0.0014\n",
      "[59] : train loss 0.002634, val loss drop 0.0014 to 0.0011\n",
      "[72] : train loss 0.013752, val loss drop 0.0011 to 0.0007\n",
      "fold 3\n",
      "[1] : train loss 138.860047, val loss drop 10000000.0000 to 0.2041\n",
      "[2] : train loss 0.109171, val loss drop 0.2041 to 0.0697\n",
      "[4] : train loss 0.032590, val loss drop 0.0697 to 0.0630\n",
      "[5] : train loss 0.018955, val loss drop 0.0630 to 0.0209\n",
      "[6] : train loss 0.012940, val loss drop 0.0209 to 0.0133\n",
      "[7] : train loss 0.009020, val loss drop 0.0133 to 0.0101\n",
      "[8] : train loss 0.008657, val loss drop 0.0101 to 0.0096\n",
      "[10] : train loss 0.009124, val loss drop 0.0096 to 0.0075\n",
      "[13] : train loss 0.005088, val loss drop 0.0075 to 0.0075\n",
      "[18] : train loss 0.009966, val loss drop 0.0075 to 0.0061\n",
      "[19] : train loss 0.005355, val loss drop 0.0061 to 0.0058\n",
      "[20] : train loss 0.006219, val loss drop 0.0058 to 0.0049\n",
      "[21] : train loss 0.006562, val loss drop 0.0049 to 0.0038\n",
      "[28] : train loss 0.006848, val loss drop 0.0038 to 0.0033\n",
      "[29] : train loss 0.004299, val loss drop 0.0033 to 0.0031\n",
      "[42] : train loss 0.004841, val loss drop 0.0031 to 0.0026\n",
      "[52] : train loss 0.001724, val loss drop 0.0026 to 0.0022\n",
      "[78] : train loss 0.003206, val loss drop 0.0022 to 0.0016\n",
      "[85] : train loss 0.002154, val loss drop 0.0016 to 0.0016\n",
      "[89] : train loss 0.004686, val loss drop 0.0016 to 0.0014\n",
      "[93] : train loss 0.003959, val loss drop 0.0014 to 0.0008\n",
      "[99] : train loss 0.003857, val loss drop 0.0008 to 0.0008\n",
      "fold 4\n",
      "[1] : train loss 57.269105, val loss drop 10000000.0000 to 2.5587\n",
      "[2] : train loss 0.675081, val loss drop 2.5587 to 1.7600\n",
      "[3] : train loss 0.276893, val loss drop 1.7600 to 0.1142\n",
      "[4] : train loss 0.102119, val loss drop 0.1142 to 0.0733\n",
      "[5] : train loss 0.058564, val loss drop 0.0733 to 0.0375\n",
      "[6] : train loss 0.026186, val loss drop 0.0375 to 0.0149\n",
      "[9] : train loss 0.047667, val loss drop 0.0149 to 0.0097\n",
      "[11] : train loss 0.089367, val loss drop 0.0097 to 0.0081\n",
      "[15] : train loss 0.041431, val loss drop 0.0081 to 0.0080\n",
      "[27] : train loss 0.029608, val loss drop 0.0080 to 0.0055\n",
      "[30] : train loss 0.014360, val loss drop 0.0055 to 0.0039\n",
      "[45] : train loss 0.018017, val loss drop 0.0039 to 0.0033\n",
      "[49] : train loss 0.012764, val loss drop 0.0033 to 0.0017\n",
      "[62] : train loss 0.005504, val loss drop 0.0017 to 0.0015\n",
      "[98] : train loss 0.020566, val loss drop 0.0015 to 0.0009\n",
      "fold 5\n",
      "[1] : train loss 69.875375, val loss drop 10000000.0000 to 4.0779\n",
      "[2] : train loss 0.682590, val loss drop 4.0779 to 0.3328\n",
      "[3] : train loss 0.189285, val loss drop 0.3328 to 0.2032\n",
      "[4] : train loss 0.066438, val loss drop 0.2032 to 0.0590\n",
      "[5] : train loss 0.040478, val loss drop 0.0590 to 0.0213\n",
      "[6] : train loss 0.021925, val loss drop 0.0213 to 0.0146\n",
      "[8] : train loss 0.010778, val loss drop 0.0146 to 0.0099\n",
      "[15] : train loss 0.007833, val loss drop 0.0099 to 0.0087\n",
      "[16] : train loss 0.006246, val loss drop 0.0087 to 0.0071\n",
      "[18] : train loss 0.005798, val loss drop 0.0071 to 0.0071\n",
      "[21] : train loss 0.012635, val loss drop 0.0071 to 0.0047\n",
      "[26] : train loss 0.004697, val loss drop 0.0047 to 0.0046\n",
      "[29] : train loss 0.006077, val loss drop 0.0046 to 0.0044\n",
      "[41] : train loss 0.011815, val loss drop 0.0044 to 0.0037\n",
      "[42] : train loss 0.002832, val loss drop 0.0037 to 0.0033\n",
      "[48] : train loss 0.005998, val loss drop 0.0033 to 0.0025\n",
      "[60] : train loss 0.004288, val loss drop 0.0025 to 0.0021\n",
      "fold 6\n",
      "[1] : train loss 84.953569, val loss drop 10000000.0000 to 2.9608\n",
      "[2] : train loss 0.825820, val loss drop 2.9608 to 1.5575\n",
      "[3] : train loss 0.226027, val loss drop 1.5575 to 0.4451\n",
      "[4] : train loss 0.085064, val loss drop 0.4451 to 0.1365\n",
      "[5] : train loss 0.044951, val loss drop 0.1365 to 0.0377\n",
      "[6] : train loss 0.030464, val loss drop 0.0377 to 0.0198\n",
      "[7] : train loss 0.020697, val loss drop 0.0198 to 0.0117\n",
      "[8] : train loss 0.026587, val loss drop 0.0117 to 0.0105\n",
      "[14] : train loss 0.032004, val loss drop 0.0105 to 0.0087\n",
      "[23] : train loss 0.022532, val loss drop 0.0087 to 0.0041\n",
      "[24] : train loss 0.011124, val loss drop 0.0041 to 0.0041\n",
      "[26] : train loss 0.007709, val loss drop 0.0041 to 0.0035\n",
      "[33] : train loss 0.007458, val loss drop 0.0035 to 0.0034\n",
      "[34] : train loss 0.007960, val loss drop 0.0034 to 0.0022\n",
      "[51] : train loss 0.010322, val loss drop 0.0022 to 0.0021\n",
      "[59] : train loss 0.004234, val loss drop 0.0021 to 0.0016\n",
      "[73] : train loss 0.005846, val loss drop 0.0016 to 0.0016\n",
      "[74] : train loss 0.005593, val loss drop 0.0016 to 0.0013\n",
      "[81] : train loss 0.004674, val loss drop 0.0013 to 0.0010\n",
      "[100] : train loss 0.002183, val loss drop 0.0010 to 0.0006\n",
      "fold 7\n",
      "[1] : train loss 20.155690, val loss drop 10000000.0000 to 4.8055\n",
      "[2] : train loss 0.364242, val loss drop 4.8055 to 0.8449\n",
      "[3] : train loss 0.130019, val loss drop 0.8449 to 0.1341\n",
      "[4] : train loss 0.108941, val loss drop 0.1341 to 0.0544\n",
      "[5] : train loss 0.072405, val loss drop 0.0544 to 0.0398\n",
      "[9] : train loss 0.035772, val loss drop 0.0398 to 0.0256\n",
      "[10] : train loss 0.024469, val loss drop 0.0256 to 0.0164\n",
      "[19] : train loss 0.055230, val loss drop 0.0164 to 0.0078\n",
      "[23] : train loss 0.058502, val loss drop 0.0078 to 0.0075\n",
      "[54] : train loss 0.009505, val loss drop 0.0075 to 0.0048\n",
      "[69] : train loss 0.032274, val loss drop 0.0048 to 0.0027\n",
      "[91] : train loss 0.012959, val loss drop 0.0027 to 0.0021\n",
      "[98] : train loss 0.036141, val loss drop 0.0021 to 0.0021\n",
      "fold 8\n",
      "[1] : train loss 42.408585, val loss drop 10000000.0000 to 0.9067\n",
      "[2] : train loss 0.089600, val loss drop 0.9067 to 0.0634\n",
      "[3] : train loss 0.035794, val loss drop 0.0634 to 0.0435\n",
      "[4] : train loss 0.021773, val loss drop 0.0435 to 0.0144\n",
      "[5] : train loss 0.014905, val loss drop 0.0144 to 0.0083\n",
      "[6] : train loss 0.009200, val loss drop 0.0083 to 0.0076\n",
      "[7] : train loss 0.006780, val loss drop 0.0076 to 0.0063\n",
      "[14] : train loss 0.008143, val loss drop 0.0063 to 0.0055\n",
      "[17] : train loss 0.010834, val loss drop 0.0055 to 0.0032\n",
      "[27] : train loss 0.005415, val loss drop 0.0032 to 0.0022\n",
      "[30] : train loss 0.014182, val loss drop 0.0022 to 0.0021\n",
      "[37] : train loss 0.004529, val loss drop 0.0021 to 0.0020\n",
      "[38] : train loss 0.002577, val loss drop 0.0020 to 0.0018\n",
      "[42] : train loss 0.007349, val loss drop 0.0018 to 0.0015\n",
      "[54] : train loss 0.004766, val loss drop 0.0015 to 0.0015\n",
      "[55] : train loss 0.001616, val loss drop 0.0015 to 0.0013\n",
      "[71] : train loss 0.005825, val loss drop 0.0013 to 0.0010\n",
      "[90] : train loss 0.002350, val loss drop 0.0010 to 0.0009\n",
      "[96] : train loss 0.004292, val loss drop 0.0009 to 0.0009\n",
      "fold 9\n",
      "[1] : train loss 16.017460, val loss drop 10000000.0000 to 4.6177\n",
      "[2] : train loss 0.480824, val loss drop 4.6177 to 0.2095\n",
      "[3] : train loss 0.170589, val loss drop 0.2095 to 0.0953\n",
      "[4] : train loss 0.066026, val loss drop 0.0953 to 0.0515\n",
      "[5] : train loss 0.031428, val loss drop 0.0515 to 0.0281\n",
      "[6] : train loss 0.017082, val loss drop 0.0281 to 0.0196\n",
      "[7] : train loss 0.012827, val loss drop 0.0196 to 0.0079\n",
      "[9] : train loss 0.013116, val loss drop 0.0079 to 0.0060\n",
      "[11] : train loss 0.008394, val loss drop 0.0060 to 0.0046\n",
      "[14] : train loss 0.018666, val loss drop 0.0046 to 0.0035\n",
      "[16] : train loss 0.010655, val loss drop 0.0035 to 0.0033\n",
      "[17] : train loss 0.004377, val loss drop 0.0033 to 0.0030\n",
      "[20] : train loss 0.008118, val loss drop 0.0030 to 0.0027\n",
      "[25] : train loss 0.004637, val loss drop 0.0027 to 0.0022\n",
      "[46] : train loss 0.002414, val loss drop 0.0022 to 0.0018\n",
      "[51] : train loss 0.001964, val loss drop 0.0018 to 0.0015\n",
      "[58] : train loss 0.006515, val loss drop 0.0015 to 0.0011\n",
      "[68] : train loss 0.005673, val loss drop 0.0011 to 0.0010\n",
      "[75] : train loss 0.005434, val loss drop 0.0010 to 0.0008\n",
      "[90] : train loss 0.005169, val loss drop 0.0008 to 0.0006\n",
      "[97] : train loss 0.006408, val loss drop 0.0006 to 0.0005\n",
      "fold 10\n",
      "[1] : train loss 40.468015, val loss drop 10000000.0000 to 0.4226\n",
      "[3] : train loss 0.096759, val loss drop 0.4226 to 0.0568\n",
      "[4] : train loss 0.037228, val loss drop 0.0568 to 0.0364\n",
      "[5] : train loss 0.026398, val loss drop 0.0364 to 0.0254\n",
      "[6] : train loss 0.017280, val loss drop 0.0254 to 0.0167\n",
      "[7] : train loss 0.017643, val loss drop 0.0167 to 0.0075\n",
      "[10] : train loss 0.010896, val loss drop 0.0075 to 0.0073\n",
      "[11] : train loss 0.008220, val loss drop 0.0073 to 0.0056\n",
      "[15] : train loss 0.008887, val loss drop 0.0056 to 0.0048\n",
      "[16] : train loss 0.005631, val loss drop 0.0048 to 0.0041\n",
      "[20] : train loss 0.004945, val loss drop 0.0041 to 0.0033\n",
      "[29] : train loss 0.007569, val loss drop 0.0033 to 0.0022\n",
      "[70] : train loss 0.008926, val loss drop 0.0022 to 0.0021\n",
      "[74] : train loss 0.010961, val loss drop 0.0021 to 0.0019\n",
      "[82] : train loss 0.003290, val loss drop 0.0019 to 0.0017\n",
      "[85] : train loss 0.002691, val loss drop 0.0017 to 0.0013\n",
      "[89] : train loss 0.002279, val loss drop 0.0013 to 0.0009\n",
      "M train...\n",
      "fold 1\n",
      "[1] : train loss 33.855114, val loss drop 10000000.0000 to 22.8323\n",
      "[3] : train loss 1.948743, val loss drop 22.8323 to 4.7173\n",
      "[4] : train loss 1.015447, val loss drop 4.7173 to 0.6907\n",
      "[8] : train loss 0.851860, val loss drop 0.6907 to 0.3567\n",
      "[12] : train loss 0.344629, val loss drop 0.3567 to 0.1994\n",
      "[16] : train loss 0.138123, val loss drop 0.1994 to 0.1594\n",
      "[21] : train loss 0.239488, val loss drop 0.1594 to 0.1080\n",
      "[22] : train loss 0.163829, val loss drop 0.1080 to 0.0868\n",
      "[29] : train loss 0.179161, val loss drop 0.0868 to 0.0524\n",
      "[43] : train loss 0.094053, val loss drop 0.0524 to 0.0211\n",
      "[62] : train loss 0.119927, val loss drop 0.0211 to 0.0160\n",
      "[86] : train loss 0.092901, val loss drop 0.0160 to 0.0138\n",
      "fold 2\n",
      "[1] : train loss 34.315628, val loss drop 10000000.0000 to 56.4886\n",
      "[2] : train loss 3.814668, val loss drop 56.4886 to 48.7139\n",
      "[3] : train loss 1.611531, val loss drop 48.7139 to 15.8145\n",
      "[4] : train loss 0.863271, val loss drop 15.8145 to 8.1421\n",
      "[5] : train loss 0.485432, val loss drop 8.1421 to 1.5948\n",
      "[6] : train loss 0.422879, val loss drop 1.5948 to 0.4756\n",
      "[7] : train loss 0.410905, val loss drop 0.4756 to 0.3476\n",
      "[8] : train loss 0.465311, val loss drop 0.3476 to 0.2983\n",
      "[9] : train loss 0.303158, val loss drop 0.2983 to 0.1359\n",
      "[14] : train loss 0.253526, val loss drop 0.1359 to 0.1107\n",
      "[17] : train loss 0.184499, val loss drop 0.1107 to 0.0932\n",
      "[27] : train loss 0.148955, val loss drop 0.0932 to 0.0878\n",
      "[30] : train loss 0.230681, val loss drop 0.0878 to 0.0468\n",
      "[56] : train loss 0.046741, val loss drop 0.0468 to 0.0438\n",
      "[68] : train loss 0.159212, val loss drop 0.0438 to 0.0416\n",
      "[75] : train loss 0.187783, val loss drop 0.0416 to 0.0377\n",
      "[77] : train loss 0.163174, val loss drop 0.0377 to 0.0257\n",
      "[89] : train loss 0.175705, val loss drop 0.0257 to 0.0139\n",
      "fold 3\n",
      "[1] : train loss 28.374099, val loss drop 10000000.0000 to 143.4116\n",
      "[2] : train loss 3.855542, val loss drop 143.4116 to 50.9965\n",
      "[3] : train loss 1.477725, val loss drop 50.9965 to 8.7698\n",
      "[4] : train loss 0.907129, val loss drop 8.7698 to 8.0186\n",
      "[5] : train loss 0.883879, val loss drop 8.0186 to 2.5497\n",
      "[6] : train loss 0.451764, val loss drop 2.5497 to 0.6811\n",
      "[7] : train loss 0.476536, val loss drop 0.6811 to 0.1946\n",
      "[14] : train loss 0.467099, val loss drop 0.1946 to 0.1246\n",
      "[26] : train loss 0.129453, val loss drop 0.1246 to 0.0428\n",
      "[32] : train loss 0.112369, val loss drop 0.0428 to 0.0350\n",
      "[50] : train loss 0.342822, val loss drop 0.0350 to 0.0349\n",
      "[89] : train loss 0.121130, val loss drop 0.0349 to 0.0330\n",
      "[92] : train loss 0.108004, val loss drop 0.0330 to 0.0289\n",
      "[95] : train loss 0.064422, val loss drop 0.0289 to 0.0222\n",
      "fold 4\n",
      "[1] : train loss 25.680198, val loss drop 10000000.0000 to 193.8215\n",
      "[2] : train loss 3.339538, val loss drop 193.8215 to 16.7292\n",
      "[3] : train loss 1.553791, val loss drop 16.7292 to 10.8808\n",
      "[4] : train loss 0.858147, val loss drop 10.8808 to 2.8626\n",
      "[5] : train loss 0.597361, val loss drop 2.8626 to 0.8063\n",
      "[6] : train loss 0.439639, val loss drop 0.8063 to 0.7817\n",
      "[8] : train loss 0.649560, val loss drop 0.7817 to 0.2001\n",
      "[10] : train loss 0.417618, val loss drop 0.2001 to 0.1425\n",
      "[11] : train loss 0.400948, val loss drop 0.1425 to 0.1362\n",
      "[12] : train loss 0.312728, val loss drop 0.1362 to 0.1018\n",
      "[17] : train loss 0.307529, val loss drop 0.1018 to 0.0671\n",
      "[19] : train loss 0.155165, val loss drop 0.0671 to 0.0352\n",
      "[61] : train loss 0.205653, val loss drop 0.0352 to 0.0291\n",
      "[73] : train loss 0.183856, val loss drop 0.0291 to 0.0161\n",
      "[92] : train loss 0.202795, val loss drop 0.0161 to 0.0161\n",
      "fold 5\n",
      "[1] : train loss 29.034510, val loss drop 10000000.0000 to 203.4889\n",
      "[2] : train loss 3.914707, val loss drop 203.4889 to 41.4560\n",
      "[3] : train loss 1.490804, val loss drop 41.4560 to 4.0172\n",
      "[4] : train loss 0.692142, val loss drop 4.0172 to 1.1025\n",
      "[5] : train loss 0.622958, val loss drop 1.1025 to 0.6782\n",
      "[6] : train loss 0.436938, val loss drop 0.6782 to 0.1673\n",
      "[10] : train loss 0.278725, val loss drop 0.1673 to 0.1099\n",
      "[11] : train loss 0.146265, val loss drop 0.1099 to 0.0877\n",
      "[18] : train loss 0.246298, val loss drop 0.0877 to 0.0842\n",
      "[20] : train loss 0.146358, val loss drop 0.0842 to 0.0664\n",
      "[34] : train loss 0.348559, val loss drop 0.0664 to 0.0271\n",
      "[38] : train loss 0.196752, val loss drop 0.0271 to 0.0220\n",
      "[69] : train loss 0.092780, val loss drop 0.0220 to 0.0151\n",
      "fold 6\n",
      "[1] : train loss 27.665614, val loss drop 10000000.0000 to 165.9782\n",
      "[2] : train loss 3.439524, val loss drop 165.9782 to 25.1637\n",
      "[3] : train loss 1.261418, val loss drop 25.1637 to 3.4653\n",
      "[4] : train loss 0.619937, val loss drop 3.4653 to 0.3170\n",
      "[9] : train loss 0.332278, val loss drop 0.3170 to 0.2042\n",
      "[10] : train loss 0.268162, val loss drop 0.2042 to 0.1059\n",
      "[22] : train loss 0.151157, val loss drop 0.1059 to 0.0950\n",
      "[25] : train loss 0.225131, val loss drop 0.0950 to 0.0308\n",
      "[31] : train loss 0.134095, val loss drop 0.0308 to 0.0168\n",
      "[69] : train loss 0.132431, val loss drop 0.0168 to 0.0139\n",
      "[74] : train loss 0.102590, val loss drop 0.0139 to 0.0138\n",
      "fold 7\n",
      "[1] : train loss 30.492162, val loss drop 10000000.0000 to 73.1516\n",
      "[2] : train loss 4.079728, val loss drop 73.1516 to 45.1749\n",
      "[3] : train loss 1.859916, val loss drop 45.1749 to 3.5672\n",
      "[4] : train loss 0.973652, val loss drop 3.5672 to 0.6910\n",
      "[6] : train loss 0.302726, val loss drop 0.6910 to 0.6056\n",
      "[7] : train loss 0.322917, val loss drop 0.6056 to 0.1635\n",
      "[10] : train loss 0.250200, val loss drop 0.1635 to 0.1049\n",
      "[11] : train loss 0.197346, val loss drop 0.1049 to 0.1025\n",
      "[12] : train loss 0.183147, val loss drop 0.1025 to 0.0815\n",
      "[22] : train loss 0.202080, val loss drop 0.0815 to 0.0574\n",
      "[27] : train loss 0.255698, val loss drop 0.0574 to 0.0442\n",
      "[30] : train loss 0.212076, val loss drop 0.0442 to 0.0394\n",
      "[39] : train loss 0.214881, val loss drop 0.0394 to 0.0219\n",
      "[53] : train loss 0.223197, val loss drop 0.0219 to 0.0131\n",
      "fold 8\n",
      "[1] : train loss 31.346086, val loss drop 10000000.0000 to 178.8802\n",
      "[2] : train loss 5.068149, val loss drop 178.8802 to 50.7518\n",
      "[3] : train loss 1.920665, val loss drop 50.7518 to 11.2342\n",
      "[4] : train loss 0.965851, val loss drop 11.2342 to 0.8392\n",
      "[6] : train loss 0.506979, val loss drop 0.8392 to 0.6483\n",
      "[8] : train loss 0.430911, val loss drop 0.6483 to 0.1977\n",
      "[14] : train loss 0.573009, val loss drop 0.1977 to 0.1077\n",
      "[19] : train loss 0.460202, val loss drop 0.1077 to 0.1075\n",
      "[20] : train loss 0.339215, val loss drop 0.1075 to 0.0726\n",
      "[21] : train loss 0.367755, val loss drop 0.0726 to 0.0507\n",
      "[23] : train loss 0.110501, val loss drop 0.0507 to 0.0388\n",
      "[47] : train loss 0.185587, val loss drop 0.0388 to 0.0303\n",
      "[58] : train loss 0.102699, val loss drop 0.0303 to 0.0251\n",
      "[62] : train loss 0.107776, val loss drop 0.0251 to 0.0156\n",
      "fold 9\n",
      "[1] : train loss 24.277554, val loss drop 10000000.0000 to 205.4909\n",
      "[2] : train loss 2.854400, val loss drop 205.4909 to 22.0017\n",
      "[3] : train loss 1.278938, val loss drop 22.0017 to 3.5922\n",
      "[4] : train loss 0.803765, val loss drop 3.5922 to 1.1320\n",
      "[5] : train loss 0.546958, val loss drop 1.1320 to 0.7186\n",
      "[6] : train loss 0.433992, val loss drop 0.7186 to 0.4254\n",
      "[7] : train loss 0.249444, val loss drop 0.4254 to 0.2170\n",
      "[14] : train loss 0.194576, val loss drop 0.2170 to 0.0700\n",
      "[17] : train loss 0.159495, val loss drop 0.0700 to 0.0644\n",
      "[31] : train loss 0.103481, val loss drop 0.0644 to 0.0434\n",
      "[40] : train loss 0.144964, val loss drop 0.0434 to 0.0305\n",
      "[52] : train loss 0.091163, val loss drop 0.0305 to 0.0202\n",
      "[84] : train loss 0.131053, val loss drop 0.0202 to 0.0201\n",
      "fold 10\n",
      "[1] : train loss 29.929707, val loss drop 10000000.0000 to 107.4296\n",
      "[2] : train loss 4.108431, val loss drop 107.4296 to 36.8484\n",
      "[3] : train loss 1.746181, val loss drop 36.8484 to 12.1809\n",
      "[4] : train loss 0.858303, val loss drop 12.1809 to 1.4917\n",
      "[5] : train loss 0.637686, val loss drop 1.4917 to 0.6602\n",
      "[6] : train loss 0.735972, val loss drop 0.6602 to 0.4139\n",
      "[10] : train loss 0.293521, val loss drop 0.4139 to 0.1678\n",
      "[11] : train loss 0.177087, val loss drop 0.1678 to 0.1152\n",
      "[21] : train loss 0.353547, val loss drop 0.1152 to 0.0356\n",
      "[34] : train loss 0.161721, val loss drop 0.0356 to 0.0202\n",
      "[52] : train loss 0.173372, val loss drop 0.0202 to 0.0193\n",
      "[73] : train loss 0.169819, val loss drop 0.0193 to 0.0124\n",
      "[82] : train loss 0.166408, val loss drop 0.0124 to 0.0091\n",
      "[89] : train loss 0.088768, val loss drop 0.0091 to 0.0072\n"
     ]
    }
   ],
   "source": [
    "# train XY\n",
    "loss_xy = kfold_train('XY',train_f, train_t)\n",
    "\n",
    "add_feature = train_t[['X','Y']].values.reshape((2800, 1, 1, 2))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 3, axis = 1)\n",
    "trainX = np.concatenate((train_f, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_v = kfold_train('V',trainX, train_t)\n",
    "\n",
    "add_feature = train_t[['V']].values.reshape((2800, 1, 1, 1))\n",
    "add_feature = np.repeat(add_feature, 375, axis = 2)\n",
    "add_feature = np.repeat(add_feature, 3, axis = 1)\n",
    "trainX = np.concatenate((trainX, add_feature), axis = -1)\n",
    "\n",
    "# train V using XY\n",
    "loss_m = kfold_train('M',trainX, train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_model = {'xy':loss_xy, 'v':loss_v, 'm':loss_m}\n",
    "with open(os.path.join(save_path, 'loss_info.json'), 'w') as f:\n",
    "    for k in loss_per_model:\n",
    "        loss_per_model[k] = np.mean(loss_per_model[k])\n",
    "    f.write(json.dumps(loss_per_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xy': 0.0005755803212119304,\n",
       " 'v': 0.0010134256617246457,\n",
       " 'm': 0.01509001212695392}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_per_model # leaky relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fold(model,nfold, save_path, name, test_data):\n",
    "    pred_array = []\n",
    "    for i in range(1, nfold+1):\n",
    "        model.load_state_dict(torch.load(os.path.join(save_path, 'model_{}_fold{}.pt'.format(name, i))))\n",
    "        model = model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predict = model(test_data.cuda())\n",
    "        pred_array.append(predict.detach().cpu().numpy())\n",
    "    result = np.mean(pred_array, axis = 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict XY\n",
    "submission = pd.read_csv(os.path.join(root_dir, 'sample_submission.csv'))\n",
    "name = 'XY'\n",
    "n_features = test_f.size()[-1]\n",
    "# define model\n",
    "model = custom_cnn(n_features, len(name))\n",
    "result = predict_fold(model, nfold, save_path ,name, test_f)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 3, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict V\n",
    "name = 'V'\n",
    "n_features = test_f_add.size()[-1]\n",
    "\n",
    "# define model\n",
    "model = custom_cnn(n_features, len(name))\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = test_f_add.shape[0]\n",
    "add_feature_t = result.reshape((n_samples, 1, 1, len(name)))\n",
    "add_feature_t = np.repeat(add_feature_t, 375, axis = 2)\n",
    "add_feature_t = np.repeat(add_feature_t, 3, axis = 1)\n",
    "add_feature_t = torch.FloatTensor(add_feature_t)\n",
    "\n",
    "test_f_add = torch.cat([test_f_add, add_feature_t], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict M\n",
    "name = 'M'\n",
    "n_features = test_f_add.size()[-1]\n",
    "\n",
    "# define model\n",
    "model = custom_cnn(n_features, len(name))\n",
    "\n",
    "result = predict_fold(model, nfold, save_path,name, test_f_add)\n",
    "submission[list(name)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2800</td>\n",
       "      <td>-267.617279</td>\n",
       "      <td>-17.669365</td>\n",
       "      <td>113.666306</td>\n",
       "      <td>0.474636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "      <td>307.816345</td>\n",
       "      <td>-289.291321</td>\n",
       "      <td>90.171768</td>\n",
       "      <td>0.477309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2802</td>\n",
       "      <td>-222.694733</td>\n",
       "      <td>127.881386</td>\n",
       "      <td>29.972157</td>\n",
       "      <td>0.385684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2803</td>\n",
       "      <td>153.119186</td>\n",
       "      <td>266.849854</td>\n",
       "      <td>28.218323</td>\n",
       "      <td>0.398237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2804</td>\n",
       "      <td>-170.162430</td>\n",
       "      <td>188.144623</td>\n",
       "      <td>135.061035</td>\n",
       "      <td>0.472602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id           X           Y           M         V\n",
       "0  2800 -267.617279  -17.669365  113.666306  0.474636\n",
       "1  2801  307.816345 -289.291321   90.171768  0.477309\n",
       "2  2802 -222.694733  127.881386   29.972157  0.385684\n",
       "3  2803  153.119186  266.849854   28.218323  0.398237\n",
       "4  2804 -170.162430  188.144623  135.061035  0.472602"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(os.path.join(save_path, '{}.csv'.format(save_path.split('/')[-1])), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
